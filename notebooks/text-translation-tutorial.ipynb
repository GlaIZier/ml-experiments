{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1be0f2f1-7f61-4d19-a9f7-6e27ac9c0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "use_builtins = True\n",
    "\n",
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "      \n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "\n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "\n",
    "        if rank != len(names):\n",
    "            raise ValueError(f'Rank mismatch:\\n'\n",
    "                           f'    found {rank}: {shape.numpy()}\\n'\n",
    "                           f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                             f\"    found: {new_dim}\\n\"\n",
    "                             f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c35f5-ed78-4e95-9fd3-cd692deea894",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7067c97-ed48-4194-9609-b164788b1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 1s 0us/step\n",
      "2654208/2638744 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True,\n",
    "    cache_dir='data-ignored/translation')\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a98e3a-104c-46a8-a580-299344a08297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n",
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp in pairs]\n",
    "    targ = [targ for targ, inp in pairs]\n",
    "\n",
    "    return targ, inp\n",
    "\n",
    "targ, inp = load_data(path_to_file)\n",
    "print(inp[-1])\n",
    "print(targ[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bdea91a-b857-48fc-bb5b-b3c6906f0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 14:52:02.478638: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Todos pueden cometer errores.'], shape=(1,), dtype=string)\n",
      "\n",
      "tf.Tensor([b'Anyone can make mistakes.'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:1])\n",
    "    print()\n",
    "    print(example_target_batch[:1])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d113f4-6a1d-4554-adc1-baabb63732ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']\n",
      "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']\n",
      "tf.Tensor(\n",
      "[[   2   66  414 1635  672    4    3    0    0    0]\n",
      " [   2   89    9 1384   35  279    4    3    0    0]\n",
      " [   2    9   48  108    4    3    0    0    0    0]], shape=(3, 10), dtype=int64)\n",
      "['[START]' 'todos' 'pueden' 'cometer' 'errores' '.' '[END]' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '']\n"
     ]
    }
   ],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # Split accecented characters.\n",
    "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "    text = tf.strings.lower(text)\n",
    "    # Keep space, a to z, and select punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "    # Add spaces around punctuation.\n",
    "    text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "    # Strip whitespace.\n",
    "    text = tf.strings.strip(text)\n",
    "\n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text\n",
    "\n",
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "print(input_text_processor.get_vocabulary()[:10])\n",
    "\n",
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "print(output_text_processor.get_vocabulary()[:10])\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "\n",
    "\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "print(example_tokens[:3, :10])\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d57630-12c8-4fdd-91bc-f3f5f8bb4eaa",
   "metadata": {},
   "source": [
    "### Encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2bc57a-58a6-409b-9a14-66c93b75e11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 22)\n",
      "Encoder output, shape (batch, s, units): (64, 22, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # The GRU RNN layer processes those vectors sequentially.\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # Return the sequence and state\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding for each token.\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "        # 3. The GRU processes the embedding sequence.\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape: (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "        # 4. Returns the new sequence and its state.\n",
    "        return output, state\n",
    "\n",
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ece3939-07cc-4ecc-9fdc-7bdcb56b49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 22)\n",
      "Attention result shape: (batch_size, query_seq_length, units):           (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 22)\n"
     ]
    }
   ],
   "source": [
    "# Attention\n",
    "# dec -> query\n",
    "# enc -> value\n",
    "# value_units ~ enc units or units\n",
    "# query_units ~ dec units\n",
    "# s -> enc input sequence\n",
    "# t -> dec input sequence\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # For Eqn. (4), the  Bahdanau attention\n",
    "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention()\n",
    "    \n",
    "    def call(self, query, value, mask):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(query, ('batch', 't', 'query_units'))\n",
    "        shape_checker(value, ('batch', 's', 'value_units'))\n",
    "        shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "        # From Eqn. (4), `W1@ht`.\n",
    "        w1_query = self.W1(query)\n",
    "        shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "        # From Eqn. (4), `W2@hs`.\n",
    "        w2_key = self.W2(value)\n",
    "        shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs = [w1_query, value, w2_key],\n",
    "            mask=[query_mask, value_mask],\n",
    "            return_attention_scores = True,\n",
    "        )\n",
    "        shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_layer = BahdanauAttention(units)\n",
    "print((example_tokens != 0).shape)\n",
    "\n",
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length (t), units (attention units)):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length (t), value_seq_length (s)): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161014c6-ca9e-45d0-92c7-d6e7e1aa80ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n",
      "[['pistol']\n",
      " ['gonna']\n",
      " ['coming']\n",
      " ['bench']\n",
      " ['every']]\n",
      "[['europe']\n",
      " ['pockets']\n",
      " ['stabbed']\n",
      " ['collection']\n",
      " ['mustnt']]\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "    new_tokens: Any\n",
    "    enc_output: Any\n",
    "    mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "    logits: Any\n",
    "    attention_weights: Any\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # For Step 1. The embedding layer convets token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "\n",
    "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        # For step 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                        use_bias=False)\n",
    "\n",
    "        # For step 5. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "        \n",
    "    def call(self,\n",
    "         inputs: DecoderInput,\n",
    "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "            shape_checker = ShapeChecker()\n",
    "            shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "            shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "            shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "            if state is not None:\n",
    "                shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "            # Step 1. Lookup the embeddings\n",
    "            vectors = self.embedding(inputs.new_tokens)\n",
    "            shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "            # Step 2. Process one step with the RNN\n",
    "            rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "            shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "            # Step 3. Use the RNN output as the query for the attention over the\n",
    "            # encoder output.\n",
    "            context_vector, attention_weights = self.attention(\n",
    "              query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "            shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "            shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "            # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "            #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "            context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "            # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "            attention_vector = self.Wc(context_and_rnn_output)\n",
    "            shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "            # Step 5. Generate logit predictions:\n",
    "            logits = self.fc(attention_vector)\n",
    "            shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "            return DecoderOutput(logits, attention_weights), state\n",
    "\n",
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "\n",
    "# Usage example\n",
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "\n",
    "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
    "\n",
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')\n",
    "\n",
    "# Sample a token according to the logits and decode the token as the first word of the output. First prediction\n",
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "print(first_word[:5])\n",
    "\n",
    "# Second prediction\n",
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "                 example_enc_output,\n",
    "                 mask=(example_tokens != 0)),\n",
    "    state=dec_state)\n",
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "print(first_word[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e051c6-db9a-4c43-8462-b2c9d90c14b8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6347fbca-2195-4b02-8f34-41edf2f7f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.6058345>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5738454>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5131044>}\n",
      "0:00:09.292453\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        self.name = 'masked_loss'\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(y_true, ('batch', 't'))\n",
    "        shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "        # Calculate the loss for each item in the batch.\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "        # Mask off the losses on padding.\n",
    "        mask = tf.cast(y_true != 0, tf.float32)\n",
    "        shape_checker(mask, ('batch', 't'))\n",
    "        loss *= mask\n",
    "\n",
    "        # Return the total.\n",
    "        return tf.reduce_sum(loss)\n",
    "    \n",
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                          embedding_dim, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        self.shape_checker = ShapeChecker()\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "    \n",
    "    def _preprocess(self, input_text, target_text):\n",
    "        self.shape_checker(input_text, ('batch',))\n",
    "        self.shape_checker(target_text, ('batch',))\n",
    "\n",
    "        # Convert the text to token IDs\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        target_tokens = self.output_text_processor(target_text)\n",
    "        self.shape_checker(input_tokens, ('batch', 's'))\n",
    "        self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "        # Convert IDs to masks.\n",
    "        input_mask = input_tokens != 0\n",
    "        self.shape_checker(input_mask, ('batch', 's'))\n",
    "\n",
    "        target_mask = target_tokens != 0\n",
    "        self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "    \n",
    "    def _train_step(self, inputs):\n",
    "        input_text, target_text = inputs  \n",
    "\n",
    "        (input_tokens, input_mask,\n",
    "        target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "        max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode the input\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "            self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "            self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "            # Initialize the decoder's state to the encoder's final state.\n",
    "            # This only works if the encoder and decoder have the same number of\n",
    "            # units.\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            for t in tf.range(max_target_length-1):\n",
    "              # Pass in two tokens from the target sequence:\n",
    "              # 1. The current input to the decoder.\n",
    "              # 2. The target for the decoder's next prediction.\n",
    "                new_tokens = target_tokens[:, t:t+2]\n",
    "                step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                         enc_output, dec_state)\n",
    "                loss = loss + step_loss\n",
    "\n",
    "            # Average the loss over all non padding tokens.\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "        # Apply an optimization step\n",
    "        variables = self.trainable_variables \n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {'batch_loss': average_loss}\n",
    "    \n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "        # Run the decoder one step.\n",
    "        decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                                   enc_output=enc_output,\n",
    "                                   mask=input_mask)\n",
    "\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "        self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "        self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "        self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "        # `self.loss` returns the total for non-padded tokens\n",
    "        y = target_token\n",
    "        y_pred = dec_result.logits\n",
    "        step_loss = self.loss(y, y_pred)\n",
    "\n",
    "        return step_loss, dec_state \n",
    "    \n",
    "    # tf function is faster\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "    def _tf_train_step(self, inputs):\n",
    "        return self._train_step(inputs)\n",
    "    \n",
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "for n in range(3):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print(datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8680e6c0-88e9-416d-a19c-d7b40641df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9765253>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9597201>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.989764>}\n",
      "0:00:05.184365\n"
     ]
    }
   ],
   "source": [
    "translator.use_tf_function = True\n",
    "start_time = datetime.datetime.now()\n",
    "for n in range(3):\n",
    "    print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print(datetime.datetime.now() - start_time)\n",
    "\n",
    "# reset model\n",
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f7afcd8-a188-44a6-93e2-efa9deca7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1859/1859 [==============================] - 2381s 1s/step - batch_loss: 2.0793\n",
      "Epoch 2/3\n",
      "1859/1859 [==============================] - 2583s 1s/step - batch_loss: 1.0463\n",
      "Epoch 3/3\n",
      "1859/1859 [==============================] - 2349s 1s/step - batch_loss: 0.8151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x158e7edc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.logs = []\n",
    "\n",
    "    def on_train_batch_end(self, n, logs):\n",
    "        self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')\n",
    "\n",
    "train_translator.fit(dataset, epochs=3,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f108a9-9314-4c78-acad-159635e308ad",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddbfd582-528c-4fee-974a-a387adc28ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_80609/3155067552.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "its very cold here .\n",
      "this is my life .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True))\n",
    "\n",
    "        # The output should never generate padding, unknown, or start.\n",
    "        index_from_string = tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))\n",
    "        \n",
    "    def tokens_to_text(self, result_tokens):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(result_tokens, ('batch', 't'))\n",
    "        result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "        shape_checker(result_text_tokens, ('batch', 't'))\n",
    "\n",
    "        result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                           axis=1, separator=' ')\n",
    "        shape_checker(result_text, ('batch'))\n",
    "\n",
    "        result_text = tf.strings.strip(result_text)\n",
    "        shape_checker(result_text, ('batch',))\n",
    "        return result_text\n",
    "    \n",
    "    def sample(self, logits, temperature):\n",
    "        shape_checker = ShapeChecker()\n",
    "        # 't' is usually 1 here.\n",
    "        shape_checker(logits, ('batch', 't', 'vocab'))\n",
    "        shape_checker(self.token_mask, ('vocab',))\n",
    "\n",
    "        token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "        shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
    "\n",
    "        # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "        logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            new_tokens = tf.argmax(logits, axis=-1)\n",
    "        else: \n",
    "            logits = tf.squeeze(logits, axis=1)\n",
    "            new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                            num_samples=1)\n",
    "\n",
    "        shape_checker(new_tokens, ('batch', 't'))\n",
    "        return new_tokens\n",
    "    \n",
    "    def translate_unrolled(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "        batch_size = tf.shape(input_text)[0]\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "        dec_state = enc_state\n",
    "        new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                                     enc_output=enc_output,\n",
    "                                     mask=(input_tokens!=0))\n",
    "\n",
    "            dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "            attention.append(dec_result.attention_weights)\n",
    "\n",
    "            new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "            # If a sequence produces an `end_token`, set it `done`\n",
    "            done = done | (new_tokens == self.end_token)\n",
    "            # Once a sequence is done it only produces 0-padding.\n",
    "            new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "            # Collect the generated tokens\n",
    "            result_tokens.append(new_tokens)\n",
    "\n",
    "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "                break\n",
    "\n",
    "        # Convert the list of generates token ids to a list of strings.\n",
    "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "        result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "        if return_attention:\n",
    "            attention_stack = tf.concat(attention, axis=1)\n",
    "            return {'text': result_text, 'attention': attention_stack}\n",
    "        else:\n",
    "            return {'text': result_text}\n",
    "\n",
    "        \n",
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")\n",
    "\n",
    "input_text = tf.constant([\n",
    "    'hace mucho frio aqui.', # \"It's really cold here.\"\n",
    "    'Esta es mi vida.', # \"This is my life.\"\"\n",
    "])\n",
    "\n",
    "result = translator.translate_unrolled(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdf4af55-76ca-437e-ba3a-3d4ca1ca3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000001  0.99999994 1.         1.         1.         1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_80609/1351631907.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_80609/1351631907.py:14: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    sentence = tf_lower_and_split_punct(sentence).numpy().decode().split()\n",
    "    predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    attention = attention[:len(predicted_sentence), :len(sentence)]\n",
    "\n",
    "    ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    ax.set_xlabel('Input text')\n",
    "    ax.set_ylabel('Output text')\n",
    "    plt.suptitle('Attention weights')\n",
    "\n",
    "a = result['attention'][0]\n",
    "\n",
    "print(np.sum(a, axis=-1))\n",
    "_ = plt.bar(range(len(a[0, :])), a[0, :])\n",
    "\n",
    "i=0\n",
    "plot_attention(result['attention'][i], input_text[i], result['text'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
