{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838093c3-cb8a-4a10-aa5e-b3fb28d7617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:26:37,249 : INFO : No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "2021-12-18 14:26:37,251 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2021-12-18 14:26:37,255 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2021-12-18 14:26:37,256 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:26:37.258703: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-18 14:26:37.391021: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "# train_dataset = ds['train'].batch(args.batch_size)\n",
    "train_dataset = ds['train']\n",
    "val_dataset = ds['test'].take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862269e1-0bed-47c4-9c12-8af19d609d93",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6248a603-30a8-44d0-91eb-ea09971feb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=10)\n",
    "def get_encoder(vocab_size=args.vocab_size):\n",
    "    encoder = TextVectorization(max_tokens=vocab_size)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d57c1de9-170f-4c14-8911-5017a0af63df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fff 2\n"
     ]
    }
   ],
   "source": [
    "print('fff', max([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9855a40-9719-4449-b070-f7785fc43644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                16064     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,129\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:26:58.443117: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1361 of 10000\n",
      "2021-12-18 14:27:08.438771: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2704 of 10000\n",
      "2021-12-18 14:27:18.438015: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 4027 of 10000\n",
      "2021-12-18 14:27:28.434088: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5397 of 10000\n",
      "2021-12-18 14:27:38.447168: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6733 of 10000\n",
      "2021-12-18 14:27:48.436308: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 8051 of 10000\n",
      "2021-12-18 14:27:58.435892: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 9400 of 10000\n",
      "2021-12-18 14:28:02.879055: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719/782 [==========================>...] - ETA: 9s - loss: 0.5230 - accuracy: 0.7512 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:30:07.129295: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1390 of 10000\n",
      "2021-12-18 14:30:17.122027: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2832 of 10000\n",
      "2021-12-18 14:30:27.125266: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 4198 of 10000\n",
      "2021-12-18 14:30:37.120423: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5567 of 10000\n",
      "2021-12-18 14:30:47.121443: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6924 of 10000\n",
      "2021-12-18 14:30:57.132319: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 8320 of 10000\n",
      "2021-12-18 14:31:07.125631: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 9678 of 10000\n",
      "2021-12-18 14:31:09.453714: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 280s 262ms/step - loss: 0.5223 - accuracy: 0.7521 - val_loss: 0.5011 - val_accuracy: 0.7642 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:31:27.507590: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4748 - accuracy: 0.7826 - val_loss: 0.4729 - val_accuracy: 0.7777 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4636 - accuracy: 0.7889 - val_loss: 0.4700 - val_accuracy: 0.7822 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4538 - accuracy: 0.7909 - val_loss: 0.4824 - val_accuracy: 0.7718 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4378 - accuracy: 0.7980 - val_loss: 0.4631 - val_accuracy: 0.7815 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4248 - accuracy: 0.8046 - val_loss: 0.4745 - val_accuracy: 0.7776 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "729/782 [==========================>...] - ETA: 0s - loss: 0.4093 - accuracy: 0.8127\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4097 - accuracy: 0.8124 - val_loss: 0.4772 - val_accuracy: 0.7766 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "734/782 [===========================>..] - ETA: 0s - loss: 0.3771 - accuracy: 0.8306Restoring model weights from the end of the best epoch: 5.\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.3776 - accuracy: 0.8303 - val_loss: 0.4641 - val_accuracy: 0.7852 - lr: 1.0000e-04\n",
      "Epoch 00008: early stopping\n",
      "Val_accuracy: 0.7851999998092651\n",
      "Val_loss: 0.46305057406425476\n",
      "Accuracy: 0.8302800059318542\n"
     ]
    }
   ],
   "source": [
    "### Baseline. Bag of words. Preprocessing in dataset creation step.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    # declaring outputs as an input cause you have to declare tf.variable outside of tf function \n",
    "    def build_bag_of_words(tokens, label, outputs=tf.Variable(tf.zeros(args.small_vocab_size))):\n",
    "        # without it, tf saves the last state of the tensor\n",
    "        outputs.assign(tf.zeros_like(outputs))\n",
    "        for i in range(len(tokens)):\n",
    "            output_idx = tokens[i]\n",
    "            if output_idx >= tf.constant(args.small_vocab_size, dtype=tf.int64):\n",
    "                output_idx = tf.constant(1, dtype=tf.int64)\n",
    "            outputs[output_idx].assign(outputs[output_idx] + 1)\n",
    "        return outputs, label\n",
    "\n",
    "    ds_train = train_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    ds_val = val_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    # for d in ds_train.take(10):\n",
    "    #     print(d)\n",
    "        \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(250,)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "    \n",
    "baseline_bag_of_words()\n",
    "\n",
    "# val_accuracy: 0.785; val_loss 0.456; accuracy: 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767c4b4a-3459-41fc-8932-44eb7dedeb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace Tensor(\"bag_of_words_3/Shape:0\", shape=(2,), dtype=int32)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " bag_of_words_3 (BagOfWords)  (32, 250)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (32, 64)                  16064     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (32, 1)                   65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,129\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:59:12,266 : WARNING : Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace Tensor(\"sequential_4/bag_of_words_3/Shape:0\", shape=(2,), dtype=int32)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-18 14:59:12,462 : WARNING : Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.string, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace Tensor(\"sequential_4/bag_of_words_3/Shape:0\", shape=(2,), dtype=int32)\n",
      "exec [32 552]\n",
      "  1/782 [..............................] - ETA: 6:18 - loss: 0.6931 - accuracy: 0.5312exec [32 707]\n",
      "exec [32 861]\n",
      "exec [32 699]\n",
      "exec [32 626]\n",
      "exec [32 477]\n",
      "exec [32 650]\n",
      "exec [32 424]\n",
      "exec [32 790]\n",
      "exec [32 801]\n",
      "exec [32 645]\n",
      "exec [32 860]\n",
      "exec [32 926]\n",
      "exec [32 495]\n",
      "exec [32 880]\n",
      "exec [32 613]\n",
      "exec [32 582]\n",
      "exec [32 509]\n",
      " 18/782 [..............................] - ETA: 2s - loss: 0.6932 - accuracy: 0.5052  exec [32 765]\n",
      "exec [32 763]\n",
      "exec [32 550]\n",
      "exec [32 956]\n",
      "exec [32 774]\n",
      "exec [32 426]\n",
      "exec [32 682]\n",
      "exec [32 832]\n",
      "exec [32 768]\n",
      "exec [32 671]\n",
      "exec [32 988]\n",
      "exec [32 883]\n",
      "exec [32 774]\n",
      "exec [32 898]\n",
      "exec [32 950]\n",
      "exec [32 713]\n",
      "exec [32 805]\n",
      " 35/782 [>.............................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4991exec [32 688]\n",
      "exec [32 620]\n",
      "exec [32 986]\n",
      "exec [32 840]\n",
      "exec [32 807]\n",
      "exec [32 603]\n",
      "exec [32 384]\n",
      "exec [32 760]\n",
      "exec [32 888]\n",
      "exec [32 1146]\n",
      "exec [32 533]\n",
      "exec [32 857]\n",
      "exec [32 773]\n",
      "exec [32 1005]\n",
      "exec [32 668]\n",
      " 50/782 [>.............................] - ETA: 2s - loss: 0.6932 - accuracy: 0.5031exec [32 959]\n",
      "exec [32 985]\n",
      "exec [32 625]\n",
      "exec [32 944]\n",
      "exec [32 939]\n",
      "exec [32 988]\n",
      "exec [32 980]\n",
      "exec [32 689]\n",
      "exec [32 709]\n",
      "exec [32 674]\n",
      "exec [32 760]\n",
      "exec [32 916]\n",
      "exec [32 660]\n",
      " 63/782 [=>............................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4945exec [32 718]\n",
      "exec [32 965]\n",
      "exec [32 995]\n",
      "exec [32 824]\n",
      "exec [32 911]\n",
      "exec [32 825]\n",
      "exec [32 782]\n",
      "exec [32 657]\n",
      "exec [32 590]\n",
      "exec [32 918]\n",
      "exec [32 799]\n",
      "exec [32 972]\n",
      "exec [32 640]\n",
      "exec [32 413]\n",
      "exec [32 565]\n",
      "exec [32 956]\n",
      "exec [32 975]\n",
      " 80/782 [==>...........................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4992exec [32 816]\n",
      "exec [32 764]\n",
      "exec [32 1010]\n",
      "exec [32 828]\n",
      "exec [32 748]\n",
      "exec [32 622]\n",
      "exec [32 610]\n",
      "exec [32 479]\n",
      "exec [32 439]\n",
      "exec [32 1001]\n",
      "exec [32 536]\n",
      "exec [32 733]\n",
      "exec [32 982]\n",
      "exec [32 1183]\n",
      "exec [32 924]\n",
      "exec [32 708]\n",
      "exec [32 384]\n",
      " 97/782 [==>...........................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4952exec [32 2469]\n",
      "exec [32 523]\n",
      "exec [32 781]\n",
      "exec [32 691]\n",
      "exec [32 988]\n",
      "exec [32 616]\n",
      "exec [32 760]\n",
      "exec [32 897]\n",
      "exec [32 845]\n",
      "exec [32 428]\n",
      "exec [32 975]\n",
      "exec [32 826]\n",
      "exec [32 993]\n",
      "exec [32 563]\n",
      "exec [32 670]\n",
      "exec [32 731]\n",
      "113/782 [===>..........................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4917exec [32 595]\n",
      "exec [32 1838]\n",
      "exec [32 615]\n",
      "exec [32 1002]\n",
      "exec [32 601]\n",
      "exec [32 1007]\n",
      "exec [32 685]\n",
      "exec [32 817]\n",
      "exec [32 1526]\n",
      "exec [32 724]\n",
      "exec [32 473]\n",
      "124/782 [===>..........................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4927exec [32 860]\n",
      "exec [32 880]\n",
      "exec [32 832]\n",
      "exec [32 600]\n",
      "exec [32 807]\n",
      "exec [32 893]\n",
      "exec [32 546]\n",
      "exec [32 1000]\n",
      "exec [32 835]\n",
      "exec [32 966]\n",
      "exec [32 916]\n",
      "exec [32 979]\n",
      "exec [32 953]\n",
      "exec [32 701]\n",
      "exec [32 609]\n",
      "139/782 [====>.........................] - ETA: 2s - loss: 0.6931 - accuracy: 0.4998exec [32 773]\n",
      "exec [32 652]\n",
      "exec [32 587]\n",
      "exec [32 739]\n",
      "exec [32 1315]\n",
      "exec [32 981]\n",
      "exec [32 476]\n",
      "exec [32 485]\n",
      "exec [32 908]\n",
      "exec [32 909]\n",
      "exec [32 637]\n",
      "exec [32 514]\n",
      "exec [32 813]\n",
      "exec [32 814]\n",
      "exec [32 1000]\n",
      "exec [32 907]\n",
      "exec [32 570]\n",
      "156/782 [====>.........................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4984exec [32 533]\n",
      "exec [32 908]\n",
      "exec [32 686]\n",
      "exec [32 922]\n",
      "exec [32 658]\n",
      "exec [32 435]\n",
      "exec [32 962]\n",
      "exec [32 546]\n",
      "exec [32 458]\n",
      "exec [32 712]\n",
      "exec [32 819]\n",
      "exec [32 559]\n",
      "exec [32 994]\n",
      "exec [32 454]\n",
      "exec [32 734]\n",
      "exec [32 993]\n",
      "exec [32 998]\n",
      "173/782 [=====>........................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4967exec [32 910]\n",
      "exec [32 671]\n",
      "exec [32 672]\n",
      "exec [32 833]\n",
      "exec [32 578]\n",
      "exec [32 672]\n",
      "exec [32 724]\n",
      "exec [32 625]\n",
      "exec [32 768]\n",
      "exec [32 917]\n",
      "exec [32 769]\n",
      "exec [32 469]\n",
      "exec [32 997]\n",
      "exec [32 893]\n",
      "exec [32 692]\n",
      "exec [32 792]\n",
      "exec [32 985]\n",
      "190/782 [======>.......................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4946exec [32 594]\n",
      "exec [32 890]\n",
      "exec [32 476]\n",
      "exec [32 668]\n",
      "exec [32 817]\n",
      "exec [32 963]\n",
      "exec [32 992]\n",
      "exec [32 799]\n",
      "exec [32 451]\n",
      "exec [32 675]\n",
      "200/782 [======>.......................] - ETA: 2s - loss: 0.6932 - accuracy: 0.4950exec [32 699]\n",
      "exec [32 734]\n",
      "exec [32 607]\n",
      "exec [32 884]\n",
      "exec [32 1002]\n",
      "exec [32 617]\n",
      "exec [32 981]\n",
      "exec [32 833]\n",
      "exec [32 559]\n",
      "exec [32 668]\n",
      "exec [32 501]\n",
      "exec [32 786]\n",
      "exec [32 754]\n",
      "exec [32 934]\n",
      "exec [32 781]\n",
      "exec [32 999]\n",
      "exec [32 777]\n",
      "217/782 [=======>......................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4957exec [32 590]\n",
      "exec [32 728]\n",
      "exec [32 998]\n",
      "exec [32 700]\n",
      "exec [32 960]\n",
      "exec [32 658]\n",
      "exec [32 989]\n",
      "exec [32 551]\n",
      "exec [32 1051]\n",
      "exec [32 409]\n",
      "exec [32 796]\n",
      "exec [32 959]\n",
      "exec [32 528]\n",
      "exec [32 854]\n",
      "exec [32 946]\n",
      "exec [32 779]\n",
      "233/782 [=======>......................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4946exec [32 799]\n",
      "exec [32 679]\n",
      "exec [32 408]\n",
      "exec [32 513]\n",
      "exec [32 989]\n",
      "exec [32 724]\n",
      "exec [32 841]\n",
      "exec [32 981]\n",
      "exec [32 995]\n",
      "exec [32 592]\n",
      "exec [32 446]\n",
      "exec [32 946]\n",
      "exec [32 889]\n",
      "exec [32 1000]\n",
      "exec [32 525]\n",
      "exec [32 619]\n",
      "249/782 [========>.....................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4941exec [32 793]\n",
      "exec [32 758]\n",
      "exec [32 687]\n",
      "exec [32 988]\n",
      "exec [32 583]\n",
      "exec [32 415]\n",
      "exec [32 625]\n",
      "exec [32 978]\n",
      "exec [32 751]\n",
      "exec [32 996]\n",
      "exec [32 661]\n",
      "exec [32 974]\n",
      "exec [32 891]\n",
      "exec [32 516]\n",
      "exec [32 631]\n",
      "exec [32 669]\n",
      "exec [32 976]\n",
      "266/782 [=========>....................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4947exec [32 795]\n",
      "exec [32 945]\n",
      "exec [32 926]\n",
      "exec [32 999]\n",
      "exec [32 751]\n",
      "exec [32 700]\n",
      "exec [32 620]\n",
      "exec [32 573]\n",
      "exec [32 741]\n",
      "275/782 [=========>....................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4957exec [32 878]\n",
      "exec [32 963]\n",
      "exec [32 576]\n",
      "exec [32 974]\n",
      "exec [32 689]\n",
      "exec [32 791]\n",
      "exec [32 749]\n",
      "exec [32 916]\n",
      "exec [32 576]\n",
      "exec [32 811]\n",
      "exec [32 975]\n",
      "exec [32 964]\n",
      "exec [32 1000]\n",
      "exec [32 984]\n",
      "exec [32 532]\n",
      "exec [32 947]\n",
      "exec [32 917]\n",
      "exec [32 1083]\n",
      "293/782 [==========>...................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4957exec [32 950]\n",
      "exec [32 924]\n",
      "exec [32 978]\n",
      "exec [32 653]\n",
      "exec [32 621]\n",
      "exec [32 667]\n",
      "exec [32 857]\n",
      "exec [32 755]\n",
      "exec [32 1073]\n",
      "exec [32 814]\n",
      "exec [32 987]\n",
      "exec [32 714]\n",
      "exec [32 867]\n",
      "exec [32 796]\n",
      "exec [32 857]\n",
      "exec [32 611]\n",
      "exec [32 930]\n",
      "310/782 [==========>...................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4947exec [32 909]\n",
      "exec [32 938]\n",
      "exec [32 676]\n",
      "exec [32 695]\n",
      "exec [32 986]\n",
      "exec [32 756]\n",
      "exec [32 427]\n",
      "exec [32 684]\n",
      "exec [32 986]\n",
      "exec [32 1001]\n",
      "exec [32 989]\n",
      "exec [32 715]\n",
      "exec [32 677]\n",
      "exec [32 974]\n",
      "exec [32 608]\n",
      "exec [32 820]\n",
      "exec [32 790]\n",
      "327/782 [===========>..................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4936exec [32 1076]\n",
      "exec [32 787]\n",
      "exec [32 652]\n",
      "exec [32 1001]\n",
      "exec [32 904]\n",
      "exec [32 660]\n",
      "exec [32 569]\n",
      "exec [32 697]\n",
      "exec [32 824]\n",
      "exec [32 490]\n",
      "exec [32 690]\n",
      "exec [32 972]\n",
      "exec [32 703]\n",
      "exec [32 586]\n",
      "exec [32 668]\n",
      "exec [32 794]\n",
      "exec [32 502]\n",
      "344/782 [============>.................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4932exec [32 663]\n",
      "exec [32 340]\n",
      "exec [32 617]\n",
      "exec [32 859]\n",
      "exec [32 997]\n",
      "exec [32 912]\n",
      "exec [32 915]\n",
      "exec [32 724]\n",
      "exec [32 628]\n",
      "exec [32 804]\n",
      "exec [32 639]\n",
      "exec [32 876]\n",
      "exec [32 648]\n",
      "exec [32 789]\n",
      "exec [32 536]\n",
      "359/782 [============>.................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4927exec [32 603]\n",
      "exec [32 738]\n",
      "exec [32 568]\n",
      "exec [32 474]\n",
      "exec [32 979]\n",
      "exec [32 796]\n",
      "exec [32 985]\n",
      "exec [32 1009]\n",
      "exec [32 663]\n",
      "exec [32 561]\n",
      "exec [32 905]\n",
      "exec [32 697]\n",
      "exec [32 561]\n",
      "exec [32 976]\n",
      "exec [32 917]\n",
      "exec [32 665]\n",
      "375/782 [=============>................] - ETA: 1s - loss: 0.6932 - accuracy: 0.4932exec [32 720]\n",
      "exec [32 865]\n",
      "exec [32 1001]\n",
      "exec [32 511]\n",
      "exec [32 864]\n",
      "exec [32 981]\n",
      "exec [32 687]\n",
      "exec [32 933]\n",
      "exec [32 756]\n",
      "exec [32 1000]\n",
      "exec [32 667]\n",
      "exec [32 946]\n",
      "exec [32 859]\n",
      "exec [32 493]\n",
      "exec [32 785]\n",
      "exec [32 591]\n",
      "391/782 [==============>...............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4924exec [32 1192]\n",
      "exec [32 655]\n",
      "exec [32 721]\n",
      "exec [32 711]\n",
      "exec [32 708]\n",
      "exec [32 1277]\n",
      "exec [32 622]\n",
      "exec [32 626]\n",
      "exec [32 627]\n",
      "exec [32 945]\n",
      "exec [32 552]\n",
      "exec [32 703]\n",
      "exec [32 975]\n",
      "exec [32 931]\n",
      "405/782 [==============>...............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4926exec [32 816]\n",
      "exec [32 561]\n",
      "exec [32 623]\n",
      "exec [32 961]\n",
      "exec [32 891]\n",
      "exec [32 977]\n",
      "exec [32 663]\n",
      "exec [32 958]\n",
      "exec [32 665]\n",
      "exec [32 495]\n",
      "exec [32 1369]\n",
      "exec [32 662]\n",
      "417/782 [==============>...............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4928exec [32 835]\n",
      "exec [32 552]\n",
      "exec [32 886]\n",
      "exec [32 966]\n",
      "exec [32 651]\n",
      "exec [32 984]\n",
      "exec [32 981]\n",
      "exec [32 836]\n",
      "exec [32 807]\n",
      "exec [32 863]\n",
      "exec [32 610]\n",
      "exec [32 810]\n",
      "exec [32 723]\n",
      "exec [32 831]\n",
      "exec [32 702]\n",
      "432/782 [===============>..............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4925exec [32 815]\n",
      "exec [32 431]\n",
      "exec [32 882]\n",
      "exec [32 954]\n",
      "exec [32 708]\n",
      "exec [32 566]\n",
      "exec [32 936]\n",
      "exec [32 954]\n",
      "exec [32 998]\n",
      "exec [32 987]\n",
      "exec [32 979]\n",
      "exec [32 658]\n",
      "exec [32 661]\n",
      "exec [32 723]\n",
      "exec [32 389]\n",
      "exec [32 739]\n",
      "exec [32 538]\n",
      "449/782 [================>.............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4923exec [32 728]\n",
      "exec [32 719]\n",
      "exec [32 750]\n",
      "exec [32 1011]\n",
      "exec [32 793]\n",
      "exec [32 608]\n",
      "exec [32 667]\n",
      "exec [32 813]\n",
      "exec [32 813]\n",
      "exec [32 740]\n",
      "exec [32 953]\n",
      "exec [32 961]\n",
      "exec [32 652]\n",
      "exec [32 380]\n",
      "exec [32 825]\n",
      "exec [32 964]\n",
      "465/782 [================>.............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4909exec [32 498]\n",
      "exec [32 599]\n",
      "exec [32 809]\n",
      "exec [32 788]\n",
      "exec [32 829]\n",
      "exec [32 966]\n",
      "exec [32 593]\n",
      "exec [32 631]\n",
      "exec [32 984]\n",
      "exec [32 389]\n",
      "exec [32 652]\n",
      "exec [32 697]\n",
      "exec [32 916]\n",
      "exec [32 699]\n",
      "exec [32 966]\n",
      "exec [32 639]\n",
      "exec [32 668]\n",
      "exec [32 554]\n",
      "483/782 [=================>............] - ETA: 1s - loss: 0.6932 - accuracy: 0.4921exec [32 612]\n",
      "exec [32 1001]\n",
      "exec [32 453]\n",
      "exec [32 776]\n",
      "exec [32 672]\n",
      "exec [32 1520]\n",
      "exec [32 883]\n",
      "exec [32 704]\n",
      "exec [32 580]\n",
      "exec [32 552]\n",
      "exec [32 704]\n",
      "exec [32 985]\n",
      "495/782 [=================>............] - ETA: 0s - loss: 0.6932 - accuracy: 0.4928exec [32 448]\n",
      "exec [32 905]\n",
      "exec [32 1065]\n",
      "exec [32 930]\n",
      "exec [32 494]\n",
      "exec [32 885]\n",
      "exec [32 552]\n",
      "exec [32 636]\n",
      "exec [32 834]\n",
      "exec [32 938]\n",
      "exec [32 1186]\n",
      "exec [32 682]\n",
      "exec [32 606]\n",
      "exec [32 993]\n",
      "exec [32 1056]\n",
      "exec [32 991]\n",
      "exec [32 786]\n",
      "exec [32 544]\n",
      "513/782 [==================>...........] - ETA: 0s - loss: 0.6932 - accuracy: 0.4929exec [32 606]\n",
      "exec [32 705]\n",
      "exec [32 696]\n",
      "exec [32 991]\n",
      "exec [32 709]\n",
      "exec [32 418]\n",
      "exec [32 757]\n",
      "exec [32 615]\n",
      "exec [32 595]\n",
      "exec [32 987]\n",
      "exec [32 825]\n",
      "exec [32 854]\n",
      "exec [32 761]\n",
      "exec [32 899]\n",
      "exec [32 755]\n",
      "exec [32 735]\n",
      "exec [32 733]\n",
      "exec [32 583]\n",
      "531/782 [===================>..........] - ETA: 0s - loss: 0.6932 - accuracy: 0.4932exec [32 487]\n",
      "exec [32 1001]\n",
      "exec [32 506]\n",
      "exec [32 971]\n",
      "exec [32 573]\n",
      "exec [32 978]\n",
      "exec [32 804]\n",
      "exec [32 626]\n",
      "exec [32 745]\n",
      "exec [32 985]\n",
      "exec [32 700]\n",
      "exec [32 475]\n",
      "exec [32 797]\n",
      "exec [32 869]\n",
      "exec [32 504]\n",
      "exec [32 882]\n",
      "exec [32 949]\n",
      "exec [32 964]\n",
      "549/782 [====================>.........] - ETA: 0s - loss: 0.6932 - accuracy: 0.4931exec [32 765]\n",
      "exec [32 959]\n",
      "exec [32 801]\n",
      "exec [32 1006]\n",
      "exec [32 970]\n",
      "exec [32 821]\n",
      "exec [32 584]\n",
      "exec [32 978]\n",
      "exec [32 621]\n",
      "exec [32 1147]\n",
      "exec [32 971]\n",
      "exec [32 1473]\n",
      "exec [32 631]\n",
      "exec [32 845]\n",
      "exec [32 673]\n",
      "exec [32 993]\n",
      "exec [32 845]\n",
      "566/782 [====================>.........] - ETA: 0s - loss: 0.6932 - accuracy: 0.4934exec [32 836]\n",
      "exec [32 844]\n",
      "exec [32 1012]\n",
      "exec [32 851]\n",
      "exec [32 605]\n",
      "exec [32 436]\n",
      "exec [32 885]\n",
      "exec [32 801]\n",
      "exec [32 946]\n",
      "exec [32 831]\n",
      "exec [32 802]\n",
      "exec [32 522]\n",
      "exec [32 724]\n",
      "579/782 [=====================>........] - ETA: 0s - loss: 0.6932 - accuracy: 0.4941exec [32 476]\n",
      "exec [32 1293]\n",
      "exec [32 801]\n",
      "exec [32 995]\n",
      "exec [32 1360]\n",
      "exec [32 808]\n",
      "exec [32 996]\n",
      "exec [32 977]\n",
      "exec [32 693]\n",
      "exec [32 767]\n",
      "exec [32 999]\n",
      "exec [32 945]\n",
      "exec [32 727]\n",
      "exec [32 594]\n",
      "exec [32 903]\n",
      "exec [32 500]\n",
      "exec [32 907]\n",
      "exec [32 804]\n",
      "597/782 [=====================>........] - ETA: 0s - loss: 0.6932 - accuracy: 0.4942exec [32 1592]\n",
      "exec [32 799]\n",
      "exec [32 684]\n",
      "exec [32 780]\n",
      "exec [32 663]\n",
      "exec [32 862]\n",
      "exec [32 611]\n",
      "exec [32 892]\n",
      "exec [32 594]\n",
      "exec [32 507]\n",
      "exec [32 828]\n",
      "exec [32 609]\n",
      "exec [32 595]\n",
      "exec [32 793]\n",
      "exec [32 787]\n",
      "exec [32 755]\n",
      "exec [32 663]\n",
      "exec [32 838]\n",
      "exec [32 874]\n",
      "616/782 [======================>.......] - ETA: 0s - loss: 0.6932 - accuracy: 0.4943exec [32 692]\n",
      "exec [32 741]\n",
      "exec [32 585]\n",
      "exec [32 711]\n",
      "exec [32 632]\n",
      "exec [32 586]\n",
      "exec [32 547]\n",
      "exec [32 713]\n",
      "exec [32 832]\n",
      "exec [32 740]\n",
      "exec [32 516]\n",
      "exec [32 990]\n",
      "exec [32 898]\n",
      "exec [32 538]\n",
      "exec [32 996]\n",
      "exec [32 664]\n",
      "exec [32 993]\n",
      "exec [32 971]\n",
      "634/782 [=======================>......] - ETA: 0s - loss: 0.6932 - accuracy: 0.4939exec [32 993]\n",
      "exec [32 899]\n",
      "exec [32 789]\n",
      "exec [32 696]\n",
      "exec [32 584]\n",
      "exec [32 545]\n",
      "exec [32 400]\n",
      "exec [32 978]\n",
      "exec [32 720]\n",
      "exec [32 627]\n",
      "exec [32 485]\n",
      "exec [32 661]\n",
      "exec [32 989]\n",
      "exec [32 605]\n",
      "exec [32 733]\n",
      "exec [32 1826]\n",
      "exec [32 673]\n",
      "exec [32 726]\n",
      "652/782 [========================>.....] - ETA: 0s - loss: 0.6932 - accuracy: 0.4934exec [32 773]\n",
      "exec [32 854]\n",
      "exec [32 322]\n",
      "exec [32 800]\n",
      "exec [32 989]\n",
      "exec [32 987]\n",
      "exec [32 436]\n",
      "exec [32 850]\n",
      "exec [32 534]\n",
      "exec [32 1069]\n",
      "exec [32 795]\n",
      "663/782 [========================>.....] - ETA: 0s - loss: 0.6932 - accuracy: 0.4928exec [32 906]\n",
      "exec [32 686]\n",
      "exec [32 995]\n",
      "exec [32 693]\n",
      "exec [32 791]\n",
      "exec [32 832]\n",
      "exec [32 492]\n",
      "exec [32 931]\n",
      "exec [32 807]\n",
      "exec [32 752]\n",
      "exec [32 956]\n",
      "exec [32 883]\n",
      "exec [32 855]\n",
      "exec [32 628]\n",
      "exec [32 972]\n",
      "exec [32 959]\n",
      "exec [32 915]\n",
      "exec [32 822]\n",
      "681/782 [=========================>....] - ETA: 0s - loss: 0.6932 - accuracy: 0.4927exec [32 713]\n",
      "exec [32 572]\n",
      "exec [32 975]\n",
      "exec [32 792]\n",
      "exec [32 917]\n",
      "exec [32 987]\n",
      "exec [32 624]\n",
      "exec [32 961]\n",
      "exec [32 758]\n",
      "exec [32 547]\n",
      "exec [32 961]\n",
      "exec [32 933]\n",
      "exec [32 713]\n",
      "exec [32 770]\n",
      "exec [32 497]\n",
      "exec [32 628]\n",
      "exec [32 733]\n",
      "698/782 [=========================>....] - ETA: 0s - loss: 0.6932 - accuracy: 0.4923exec [32 593]\n",
      "exec [32 863]\n",
      "exec [32 836]\n",
      "exec [32 1000]\n",
      "exec [32 861]\n",
      "exec [32 992]\n",
      "exec [32 886]\n",
      "exec [32 564]\n",
      "exec [32 696]\n",
      "exec [32 612]\n",
      "exec [32 991]\n",
      "exec [32 817]\n",
      "exec [32 833]\n",
      "exec [32 701]\n",
      "exec [32 727]\n",
      "exec [32 671]\n",
      "exec [32 853]\n",
      "715/782 [==========================>...] - ETA: 0s - loss: 0.6932 - accuracy: 0.4924exec [32 783]\n",
      "exec [32 1010]\n",
      "exec [32 508]\n",
      "exec [32 501]\n",
      "exec [32 625]\n",
      "exec [32 645]\n",
      "exec [32 939]\n",
      "exec [32 596]\n",
      "exec [32 864]\n",
      "exec [32 923]\n",
      "exec [32 725]\n",
      "exec [32 755]\n",
      "exec [32 751]\n",
      "exec [32 881]\n",
      "exec [32 816]\n",
      "exec [32 536]\n",
      "731/782 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4923exec [32 954]\n",
      "exec [32 700]\n",
      "exec [32 978]\n",
      "exec [32 664]\n",
      "exec [32 1723]\n",
      "exec [32 956]\n",
      "exec [32 744]\n",
      "exec [32 664]\n",
      "exec [32 600]\n",
      "exec [32 560]\n",
      "exec [32 824]\n",
      "exec [32 622]\n",
      "exec [32 1000]\n",
      "exec [32 366]\n",
      "exec [32 808]\n",
      "746/782 [===========================>..] - ETA: 0s - loss: 0.6932 - accuracy: 0.4930exec [32 567]\n",
      "exec [32 955]\n",
      "exec [32 803]\n",
      "exec [32 1007]\n",
      "exec [32 891]\n",
      "exec [32 998]\n",
      "exec [32 444]\n",
      "exec [32 738]\n",
      "exec [32 627]\n",
      "exec [32 904]\n",
      "exec [32 1005]\n",
      "exec [32 658]\n",
      "exec [32 761]\n",
      "exec [32 447]\n",
      "exec [32 742]\n",
      "exec [32 583]\n",
      "exec [32 967]\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4928exec [32 858]\n",
      "exec [32 918]\n",
      "exec [32 1397]\n",
      "exec [32 900]\n",
      "exec [32 996]\n",
      "exec [32 467]\n",
      "exec [32 969]\n",
      "exec [32 686]\n",
      "exec [32 545]\n",
      "exec [32 961]\n",
      "exec [32 946]\n",
      "exec [32 616]\n",
      "exec [32 791]\n",
      "exec [32 678]\n",
      "exec [32 797]\n",
      "exec [32 586]\n",
      "exec [32 742]\n",
      "exec [32 690]\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.6932 - accuracy: 0.4924exec [8 744]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [8,1] vs. [32,1]\n\t [[node gradient_tape/binary_crossentropy/logistic_loss/mul/Mul\n (defined at /Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]] [Op:__inference_train_function_140635]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/binary_crossentropy/logistic_loss/mul/Mul:\nIn[0] gradient_tape/binary_crossentropy/logistic_loss/sub/Neg:\t\nIn[1] binary_crossentropy/Cast (defined at /Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/losses.py:1797)\n\nOperation defined at: (most recent call last)\n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_49794/2480421604.py\", line 49, in <module>\n>>>     baseline_bag_of_words1()\n>>> \n>>>   File \"/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_49794/2480421604.py\", line 47, in baseline_bag_of_words1\n>>>     model.fit(train_dataset.batch(32), epochs=args.epochs)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_49794/2480421604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mbaseline_bag_of_words1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_49794/2480421604.py\u001b[0m in \u001b[0;36mbaseline_bag_of_words1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mreduce_lr_on_plateau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mbaseline_bag_of_words1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [8,1] vs. [32,1]\n\t [[node gradient_tape/binary_crossentropy/logistic_loss/mul/Mul\n (defined at /Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]] [Op:__inference_train_function_140635]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/binary_crossentropy/logistic_loss/mul/Mul:\nIn[0] gradient_tape/binary_crossentropy/logistic_loss/sub/Neg:\t\nIn[1] binary_crossentropy/Cast (defined at /Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/losses.py:1797)\n\nOperation defined at: (most recent call last)\n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/Users/mkhokhlush/.pyenv/versions/3.8.6/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_49794/2480421604.py\", line 49, in <module>\n>>>     baseline_bag_of_words1()\n>>> \n>>>   File \"/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_49794/2480421604.py\", line 47, in baseline_bag_of_words1\n>>>     model.fit(train_dataset.batch(32), epochs=args.epochs)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words1():\n",
    "    \n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocab_size=args.small_vocab_size, batch_size=args.batch_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocab_size = vocab_size\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            super().build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            print('trace', tf.shape(inputs))\n",
    "            tf.print('exec', tf.shape(inputs))\n",
    "            if inputs.shape[-1] == None:\n",
    "                return tf.constant(np.zeros([32, 250]))\n",
    "                # return tf.shape(inputs)\n",
    "            outputs_np = np.zeros([self.batch_size, self.vocab_size])\n",
    "            if inputs.shape[-1] != None:\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    for ii in range(inputs.shape[-1]):\n",
    "                        ouput_idx = inputs[i][ii]\n",
    "                        outputs_np[i][ouput_idx] = outputs_np[i][ouput_idx] + 1\n",
    "            return tf.constant(outputs_np)\n",
    "\n",
    "    encoder = get_encoder(args.small_vocab_size)\n",
    "    bag_of_words = BagOfWords(args.small_vocab_size, args.batch_size)\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(None,), dtype=tf.string))\n",
    "    model.add(encoder)\n",
    "    model.add(bag_of_words)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset.batch(32), epochs=args.epochs)\n",
    "\n",
    "baseline_bag_of_words1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da0d4-6800-4ed1-8b25-7bb883ba0e0a",
   "metadata": {},
   "source": [
    "### Rnn with embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a7d78-9dd7-4879-b07d-277d741ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def rnn_with_embedding():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True))\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "if False:\n",
    "    rnn_with_embedding()\n",
    "\n",
    "# Epoch 3/50\n",
    "# 782/782 [======] - 314s 401ms/step - loss: 0.2752 - accuracy: 0.8867 - val_loss: 0.3107 - val_accuracy: 0.8667 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597494b5-f388-47ca-8723-c190a1c0934f",
   "metadata": {},
   "source": [
    "### Different embeddings, glove, bert, transformer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
