{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838093c3-cb8a-4a10-aa5e-b3fb28d7617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 11:59:52,731 : INFO : No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "2022-01-23 11:59:52,732 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2022-01-23 11:59:52,735 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2022-01-23 11:59:52,735 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 11:59:52.738913: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-23 11:59:52.879104: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "# train_dataset = ds['train'].batch(args.batch_size)\n",
    "train_dataset = ds['train']\n",
    "val_dataset = ds['test'].take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6248a603-30a8-44d0-91eb-ea09971feb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=10)\n",
    "def get_encoder(vocab_size=args.vocab_size):\n",
    "    encoder = TextVectorization(max_tokens=vocab_size)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d4af6-5c96-4861-9824-a3626388e89e",
   "metadata": {},
   "source": [
    "### Baseline. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9855a40-9719-4449-b070-f7785fc43644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                16064     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,129\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:15:14.780774: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1271 of 10000\n",
      "2021-12-20 16:15:24.780641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2539 of 10000\n",
      "2021-12-20 16:15:34.780499: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 3782 of 10000\n",
      "2021-12-20 16:15:44.782971: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5062 of 10000\n",
      "2021-12-20 16:15:54.783161: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6395 of 10000\n",
      "2021-12-20 16:16:04.782261: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 7649 of 10000\n",
      "2021-12-20 16:16:14.780100: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 8950 of 10000\n",
      "2021-12-20 16:16:23.157376: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.5572 - accuracy: 0.7463 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:18:31.684886: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1342 of 10000\n",
      "2021-12-20 16:18:41.672330: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2685 of 10000\n",
      "2021-12-20 16:18:51.678592: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 3988 of 10000\n",
      "2021-12-20 16:19:01.669707: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5270 of 10000\n",
      "2021-12-20 16:19:11.676965: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6543 of 10000\n",
      "2021-12-20 16:19:21.671241: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 7828 of 10000\n",
      "2021-12-20 16:19:31.676103: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 9125 of 10000\n",
      "2021-12-20 16:19:38.532035: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 294s 275ms/step - loss: 0.5569 - accuracy: 0.7464 - val_loss: 0.4963 - val_accuracy: 0.7651 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:19:57.737089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4850 - accuracy: 0.7795 - val_loss: 0.5144 - val_accuracy: 0.7634 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "742/782 [===========================>..] - ETA: 0s - loss: 0.4760 - accuracy: 0.7835\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4764 - accuracy: 0.7831 - val_loss: 0.4982 - val_accuracy: 0.7708 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4452 - accuracy: 0.7979 - val_loss: 0.4630 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4418 - accuracy: 0.7992 - val_loss: 0.4616 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4400 - accuracy: 0.7999 - val_loss: 0.4604 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4382 - accuracy: 0.8022 - val_loss: 0.4602 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4367 - accuracy: 0.8021 - val_loss: 0.4603 - val_accuracy: 0.7850 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.8026 - val_loss: 0.4599 - val_accuracy: 0.7844 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4334 - accuracy: 0.8034 - val_loss: 0.4652 - val_accuracy: 0.7817 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.8045\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4315 - accuracy: 0.8045 - val_loss: 0.4602 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4270 - accuracy: 0.8074 - val_loss: 0.4594 - val_accuracy: 0.7843 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4268 - accuracy: 0.8082 - val_loss: 0.4592 - val_accuracy: 0.7851 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4265 - accuracy: 0.8072 - val_loss: 0.4598 - val_accuracy: 0.7834 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8070 ETA: 0s - loss: 0.4261 - accuracy: 0.\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4263 - accuracy: 0.8074 - val_loss: 0.4592 - val_accuracy: 0.7848 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8076Restoring model weights from the end of the best epoch: 13.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4257 - accuracy: 0.8077 - val_loss: 0.4592 - val_accuracy: 0.7846 - lr: 1.0000e-06\n",
      "Epoch 00016: early stopping\n",
      "Val_accuracy: 0.7851200103759766\n",
      "Val_loss: 0.45919713377952576\n",
      "Accuracy: 0.8082000017166138\n"
     ]
    }
   ],
   "source": [
    "### Baseline. Bag of words. Preprocessing in dataset creation step.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    # declaring outputs as an input cause you have to declare tf.variable outside of tf function \n",
    "    def build_bag_of_words(tokens, label, outputs=tf.Variable(tf.zeros(args.small_vocab_size))):\n",
    "        # without it, tf saves the last state of the tensor\n",
    "        outputs.assign(tf.zeros_like(outputs))\n",
    "        for i in range(len(tokens)):\n",
    "            output_idx = tokens[i]\n",
    "            if output_idx >= tf.constant(args.small_vocab_size, dtype=tf.int64):\n",
    "                output_idx = tf.constant(1, dtype=tf.int64)\n",
    "            outputs[output_idx].assign(outputs[output_idx] + 1)\n",
    "        return outputs, label\n",
    "\n",
    "    ds_train = train_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    ds_val = val_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    # for d in ds_train.take(10):\n",
    "    #     print(d)\n",
    "        \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(250,)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    baseline_bag_of_words()\n",
    "\n",
    "# val_accuracy: 0.785; val_loss 0.456; accuracy: 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37034b81-b518-48fe-8246-2f0b3bcbf1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'all' b'my' b'cats' b'in' b'a' b'row' b'' b'' b'' b'' b'']\n",
      " [b'when' b'my' b'cat' b'sits' b'down' b'she' b'looks' b'like' b'a'\n",
      "  b'furby' b'toy']\n",
      " [b'the' b'cat' b'from' b'the' b'outer' b'space' b'' b'' b'' b'' b'']\n",
      " [b'sunshine' b'loves' b'to' b'sit' b'like' b'this' b'for' b'some'\n",
      "  b'reason' b'' b'']], shape=(4, 11), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'sunshine' b'for' b'outer' b'looks' b'down' b'all' b'like' b'this'\n",
      " b'she' b'sit' b'toy' b'loves' b'row' b'my' b'the' b'sits' b'some' b'when'\n",
      " b'in' b'space' b'furby' b'' b'to' b'cats' b'from' b'reason' b'a' b'cat'], shape=(28,), dtype=string)\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 1s 2ms/step - loss: 0.8469\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8071\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def stackoverflow_answer():\n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabulary_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        def call(self, inputs):  \n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            for i in range(batch_size):\n",
    "                string = inputs[i]\n",
    "                string_length = tf.shape(tf.where(tf.math.not_equal(string, b'')))[0]\n",
    "                string = string[:string_length]\n",
    "                string_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "                for s in string:\n",
    "                    string_array = string_array.write(string_array.size(), tf.where(tf.equal(s, self.vocabulary_size), 1.0, 0.0))\n",
    "                outputs = outputs.write(i, tf.cast(tf.reduce_any(tf.cast(string_array.stack(), dtype=tf.bool), axis=0), dtype=tf.float32))\n",
    "            return outputs.stack()\n",
    "        \n",
    "    labels = [[1], [0], [1], [0]]\n",
    "\n",
    "    texts  = ['All my cats in a row',\n",
    "              'When my cat sits down, she looks like a Furby toy!',\n",
    "              'The cat from the outer space',\n",
    "              'Sunshine loves to sit like this for some reason.']\n",
    "\n",
    "    DEFAULT_STRIP_REGEX = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n",
    "    tensor_of_strings = tf.constant(texts)\n",
    "    tensor_of_strings = tf.strings.lower(tensor_of_strings)\n",
    "    tensor_of_strings = tf.strings.regex_replace(tensor_of_strings, DEFAULT_STRIP_REGEX, \"\")\n",
    "    split_strings = tf.strings.split(tensor_of_strings).to_tensor()\n",
    "    print(split_strings)\n",
    "    flattened_split_strings = tf.reshape(split_strings, (split_strings.shape[0] * split_strings.shape[1]))\n",
    "    unique_words, _ = tf.unique(flattened_split_strings)\n",
    "    unique_words = tf.random.shuffle(unique_words)\n",
    "    print(unique_words)\n",
    "\n",
    "    bag_of_words = BagOfWords(vocabulary_size = unique_words)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((split_strings, labels))\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(bag_of_words)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy())\n",
    "    model.fit(train_dataset.batch(2), epochs=2)\n",
    "\n",
    "if False:\n",
    "    stackoverflow_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aab844-52e3-4bd8-9610-7a31f8e7f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline. Bag of words. Preprocessing in a layer.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words_layer():\n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabulary_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocabulary_size = vocabulary_size\n",
    "        def call(self, inputs):  \n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            for i in range(batch_size):\n",
    "                int_string = inputs[i]\n",
    "                array_string = tf.TensorArray(dtype=tf.float32, size=self.vocabulary_size)\n",
    "                array_string.unstack(tf.zeros(self.vocabulary_size))\n",
    "                for int_word in int_string:\n",
    "                    idx = int_word\n",
    "                    idx = tf.cond(idx >= self.vocabulary_size, lambda: 1, lambda: tf.cast(idx, tf.int32))\n",
    "                    array_string = array_string.write(idx, array_string.read(idx) + 1.0)\n",
    "                outputs = outputs.write(i, array_string.stack())\n",
    "            return outputs.stack()\n",
    "        \n",
    "    encoder = get_encoder(args.small_vocab_size)\n",
    "    bag_of_words = BagOfWords(args.small_vocab_size)\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(bag_of_words)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # instead of build() on model\n",
    "    for d in train_dataset.batch(args.batch_size).take(1):\n",
    "        model(d[0])\n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(1)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    baseline_bag_of_words_layer()\n",
    "    \n",
    "# Val_accuracy: 0.7857599854469299\n",
    "# Val_loss: 0.45941051840782166\n",
    "# Accuracy: 0.7972400188446045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da0d4-6800-4ed1-8b25-7bb883ba0e0a",
   "metadata": {},
   "source": [
    "### Rnn with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a7d78-9dd7-4879-b07d-277d741ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def rnn_with_embedding():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True))\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "if False:\n",
    "    rnn_with_embedding()\n",
    "\n",
    "# Epoch 3/50\n",
    "# 782/782 [======] - 314s 401ms/step - loss: 0.2752 - accuracy: 0.8867 - val_loss: 0.3107 - val_accuracy: 0.8667 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6cb06-b87b-400d-9de8-9808812b921f",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "feeb0471-2ecc-4d7e-aa56-93f5ecff8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 2500)             1         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,130\n",
      "Trainable params: 160,129\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3950 - accuracy: 0.8393 - val_loss: 0.3443 - val_accuracy: 0.8626 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "  1/782 [..............................] - ETA: 1:24 - loss: 0.5382 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:24.176406: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2862 - accuracy: 0.8853 - val_loss: 0.3506 - val_accuracy: 0.8630 - lr: 0.0010\n",
      "Epoch 3/50\n",
      " 20/782 [..............................] - ETA: 2s - loss: 0.1955 - accuracy: 0.9172  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:27.218414: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9177\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2106 - accuracy: 0.9177 - val_loss: 0.3586 - val_accuracy: 0.8681 - lr: 0.0010\n",
      "Epoch 4/50\n",
      " 20/782 [..............................] - ETA: 2s - loss: 0.1026 - accuracy: 0.9719  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:30.250760: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775/782 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9622Restoring model weights from the end of the best epoch: 1.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1134 - accuracy: 0.9622 - val_loss: 0.3568 - val_accuracy: 0.8710 - lr: 1.0000e-04\n",
      "Epoch 00004: early stopping\n",
      "Val_accuracy: 0.870959997177124\n",
      "Val_loss: 0.34430137276649475\n",
      "Accuracy: 0.9621599912643433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:33.197737: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def tfidf():\n",
    "    \n",
    "    def get_tfidf_encoder():\n",
    "        encoder = TextVectorization(max_tokens=args.vocab_size, output_mode=\"tf_idf\")\n",
    "        encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "        return encoder\n",
    "        \n",
    "    encoder = get_tfidf_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # instead of build() on model\n",
    "    for d in train_dataset.batch(args.batch_size).take(1):\n",
    "        model(d[0])\n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(1)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    tfidf()\n",
    "    \n",
    "# Val_accuracy: 0.870959997177124\n",
    "# Val_loss: 0.34430137276649475\n",
    "# Accuracy: 0.9621599912643433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906359c-396b-4b05-8b5d-8eb5b45a7bd1",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330fa97-bae0-4f4e-b79d-f4b329fc79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "def word2vec():\n",
    "    \n",
    "    # read the models remotely from google without downloading them\n",
    "    os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n",
    "    \n",
    "    hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/Wiki-words-250/2\",\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    word2vec()\n",
    "    \n",
    "# loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88c204-8f8e-498e-a192-713edad53203",
   "metadata": {},
   "source": [
    "### Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f33d826-96a7-431f-9661-9d10a2b3d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting glove weights from /tmp/glove.6B.zip\n",
      "Found 400000 word vectors.\n",
      "Converted 2475 words (25 misses)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 100)         250200    \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 343,001\n",
      "Trainable params: 92,801\n",
      "Non-trainable params: 250,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 205s 258ms/step - loss: 0.6119 - accuracy: 0.6469 - val_loss: 0.4276 - val_accuracy: 0.8082 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:37:11.946015: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "782/782 [==============================] - 195s 249ms/step - loss: 0.3957 - accuracy: 0.8244 - val_loss: 0.3794 - val_accuracy: 0.8323 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:40:27.089018: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "782/782 [==============================] - 192s 246ms/step - loss: 0.3554 - accuracy: 0.8428 - val_loss: 0.3398 - val_accuracy: 0.8536 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:43:39.550935: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "782/782 [==============================] - 176s 225ms/step - loss: 0.3337 - accuracy: 0.8564 - val_loss: 0.3282 - val_accuracy: 0.8608 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:46:35.959688: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "782/782 [==============================] - 175s 224ms/step - loss: 0.3125 - accuracy: 0.8646 - val_loss: 0.3840 - val_accuracy: 0.8416 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:49:31.134563: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "782/782 [==============================] - 176s 225ms/step - loss: 0.2945 - accuracy: 0.8760 - val_loss: 0.3197 - val_accuracy: 0.8648 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:52:26.785338: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50\n",
      "782/782 [==============================] - 175s 224ms/step - loss: 0.2771 - accuracy: 0.8850 - val_loss: 0.3014 - val_accuracy: 0.8720 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:55:21.747019: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "782/782 [==============================] - 174s 222ms/step - loss: 0.2619 - accuracy: 0.8913 - val_loss: 0.2955 - val_accuracy: 0.8727 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 14:58:15.762200: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50\n",
      "782/782 [==============================] - 177s 226ms/step - loss: 0.2453 - accuracy: 0.8993 - val_loss: 0.3015 - val_accuracy: 0.8751 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 15:01:12.938594: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "781/782 [============================>.] - ETA: 0s - loss: 0.2288 - accuracy: 0.9097\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 177s 226ms/step - loss: 0.2288 - accuracy: 0.9097 - val_loss: 0.3043 - val_accuracy: 0.8704 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 15:04:09.539037: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9252Restoring model weights from the end of the best epoch: 8.\n",
      "782/782 [==============================] - 179s 228ms/step - loss: 0.1912 - accuracy: 0.9252 - val_loss: 0.3078 - val_accuracy: 0.8769 - lr: 1.0000e-04\n",
      "Epoch 00011: early stopping\n",
      "Val_accuracy: 0.8768799901008606\n",
      "Val_loss: 0.2955174744129181\n",
      "Accuracy: 0.9252399802207947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-23 15:07:08.055186: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def glove():\n",
    "    \n",
    "    def fetch_glove_vectors():\n",
    "        # Glove embedding weights can be downloaded from https://nlp.stanford.edu/projects/glove/\n",
    "        glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "        data_dir = \"/tmp\"\n",
    "        local_zip_file_path = os.path.join(data_dir, os.path.basename(glove_url))\n",
    "        if not os.path.isfile(local_zip_file_path):\n",
    "            print(f'Retreiving glove weights from {glove_url}')\n",
    "            urllib.request.urlretrieve(glove_url, local_zip_file_path)\n",
    "        with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
    "            print(f'Extracting glove weights from {local_zip_file_path}')\n",
    "            z.extractall(path=data_dir)\n",
    "        \n",
    "        return os.path.join(data_dir, \"glove.6B.{}d.txt\")\n",
    "    \n",
    "    def load_glove_vectors(path):\n",
    "        voc = encoder.get_vocabulary()\n",
    "        word_index = dict(zip(voc, range(len(voc))))\n",
    "        num_tokens = len(voc) + 2\n",
    "        embedding_dim = 100\n",
    "        \n",
    "        embeddings_index = {}\n",
    "        with open(path.format(embedding_dim)) as f:\n",
    "            for line in f:\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                embeddings_index[word] = coefs\n",
    "        print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "        \n",
    "\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "        # Prepare embedding matrix\n",
    "        embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in embedding index will be all-zeros.\n",
    "                # This includes the representation for \"padding\" and \"OOV\"\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            else:\n",
    "                misses += 1\n",
    "        print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "        return num_tokens, embedding_dim, embedding_matrix\n",
    "    \n",
    "    encoder = get_encoder()\n",
    "    num_tokens, embedding_dim, embedding_matrix = load_glove_vectors(fetch_glove_vectors())\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "        num_tokens,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False,\n",
    "    )\n",
    "        \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # instead of build() on model\n",
    "    for d in train_dataset.batch(args.batch_size).take(1):\n",
    "        model(d[0])\n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if False: \n",
    "    glove()\n",
    "    \n",
    "# Val_accuracy: 0.8768799901008606\n",
    "# Val_loss: 0.2955174744129181\n",
    "# Accuracy: 0.9252399802207947"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c7849-d834-4b42-9dc8-f1d497257ff3",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597494b5-f388-47ca-8723-c190a1c0934f",
   "metadata": {},
   "source": [
    "### Different embeddings, glove, bert, transformer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
