{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838093c3-cb8a-4a10-aa5e-b3fb28d7617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 17:25:52,522 : INFO : No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "2021-11-23 17:25:52,523 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2021-11-23 17:25:52,526 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2021-11-23 17:25:52,527 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-23 17:25:52.529149: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-23 17:25:52.646828: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "train_dataset = ds['train'].batch(args.batch_size)\n",
    "val_dataset = ds['test'].batch(args.batch_size).take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].batch(args.batch_size).skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862269e1-0bed-47c4-9c12-8af19d609d93",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6248a603-30a8-44d0-91eb-ea09971feb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=10)\n",
    "def get_encoder(vocab_size=args.vocab_size):\n",
    "    encoder = TextVectorization(max_tokens=vocab_size)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94380a60-e52c-44e2-9ca4-417d9c9723e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def rnn_with_embedding():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True))\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "if False:\n",
    "    rnn_with_embedding()\n",
    "\n",
    "# Epoch 3/50\n",
    "# 782/782 [======] - 314s 401ms/step - loss: 0.2752 - accuracy: 0.8867 - val_loss: 0.3107 - val_accuracy: 0.8667 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da0d4-6800-4ed1-8b25-7bb883ba0e0a",
   "metadata": {},
   "source": [
    "### Different embeddings, glove, bert, transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb3342-75a4-4f8d-ab2f-141832877ee5",
   "metadata": {},
   "source": [
    "### Baseline. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e68db382-667f-4991-931d-1c69949521bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " bag_of_words_18 (BagOfWords  (32, 250)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense (Dense)               (32, 64)                  16064     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (32, 1)                   65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,129\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2\n",
      "tf.Tensor(\n",
      "[[0.8364591 ]\n",
      " [0.6553567 ]\n",
      " [0.598919  ]\n",
      " [0.5465018 ]\n",
      " [0.6891667 ]\n",
      " [0.40440455]\n",
      " [0.99999976]\n",
      " [0.6298546 ]\n",
      " [0.65501285]\n",
      " [0.6193238 ]\n",
      " [0.7953652 ]\n",
      " [0.60837686]\n",
      " [0.99999964]\n",
      " [0.8277716 ]\n",
      " [0.6405902 ]\n",
      " [0.7118129 ]\n",
      " [0.25411254]\n",
      " [0.9479047 ]\n",
      " [0.8722769 ]\n",
      " [0.62926614]\n",
      " [0.7137131 ]\n",
      " [0.9153209 ]\n",
      " [0.76539886]\n",
      " [0.8510958 ]\n",
      " [0.66559565]\n",
      " [0.67159003]\n",
      " [0.6635642 ]\n",
      " [0.6381273 ]\n",
      " [0.45885164]\n",
      " [0.52400553]\n",
      " [0.35704458]\n",
      " [0.9994633 ]], shape=(32, 1), dtype=float32)\n",
      "2\n",
      "tf.Tensor(\n",
      "[[0.5264807 ]\n",
      " [0.3643481 ]\n",
      " [0.7393769 ]\n",
      " [0.28871322]\n",
      " [0.9400429 ]\n",
      " [0.69824344]\n",
      " [0.8703655 ]\n",
      " [0.73495626]\n",
      " [0.6090819 ]\n",
      " [0.5884873 ]\n",
      " [0.29233563]\n",
      " [0.9160941 ]\n",
      " [0.81594217]\n",
      " [0.837389  ]\n",
      " [0.6193121 ]\n",
      " [0.9747751 ]\n",
      " [0.7702022 ]\n",
      " [0.60388434]\n",
      " [0.80130005]\n",
      " [0.57457566]\n",
      " [0.78690886]\n",
      " [0.87971187]\n",
      " [0.802886  ]\n",
      " [0.64035213]\n",
      " [0.24158445]\n",
      " [1.        ]\n",
      " [0.7998152 ]\n",
      " [0.5799107 ]\n",
      " [0.8123247 ]\n",
      " [0.8433491 ]\n",
      " [0.7416082 ]\n",
      " [0.38847575]], shape=(32, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words():\n",
    "    \n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocab_size=args.small_vocab_size, batch_size=args.batch_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocab_size = vocab_size\n",
    "            self.batch_size = batch_size\n",
    "            self.initial_value = tf.zeros([batch_size, vocab_size])\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            super().build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            self.build(inputs.shape)\n",
    "            print(len(inputs.shape))\n",
    "            outputs_np = np.zeros([self.batch_size, self.vocab_size])\n",
    "            if inputs.shape[-1] != None:\n",
    "                for i in range(inputs.shape[0]):\n",
    "                    for ii in range(inputs.shape[-1]):\n",
    "                        ouput_idx = inputs[i][ii]\n",
    "                        outputs_np[i][ouput_idx] = outputs_np[i][ouput_idx] + 1\n",
    "            return tf.Variable(outputs_np)\n",
    "\n",
    "    encoder = get_encoder(args.small_vocab_size)\n",
    "    bag_of_words = BagOfWords(args.small_vocab_size)\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(bag_of_words)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    # for d in ds['train'].take(1):\n",
    "    #     print(model(d[0]))\n",
    "        \n",
    "    for d in train_dataset.take(2):\n",
    "        print(model(d[0]))\n",
    "    # print(encoder.get_vocabulary())\n",
    "#     model.add(keras.layers.Embedding(\n",
    "#         input_dim=len(encoder.get_vocabulary()),\n",
    "#         output_dim=64,\n",
    "#         # Use masking to handle the variable sequence lengths\n",
    "#         mask_zero=True))\n",
    "#     model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "#     model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#     model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "    \n",
    "#     monitor='val_loss'\n",
    "#     early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, mode='auto', restore_best_weights=True, verbose=1)\n",
    "#     reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "#     model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "baseline_bag_of_words()\n",
    "\n",
    "# Epoch 3/50\n",
    "# 782/782 [======] - 314s 401ms/step - loss: 0.2752 - accuracy: 0.8867 - val_loss: 0.3107 - val_accuracy: 0.8667 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a3def1-4f9a-4874-9779-77d4219aa113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# for i in tf.zeros(3):\n",
    "#     i += 1\n",
    "#     print(i)\n",
    "    \n",
    "t = tf.Variable(tf.zeros(3))\n",
    "for i in range(3):\n",
    "    print(t[i])\n",
    "    t[i].assign(t[i] + 1)\n",
    "# tt = tf.unstack(t)\n",
    "# for i in tt:\n",
    "#     print(t[i])\n",
    "# t[1].assign(12)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597494b5-f388-47ca-8723-c190a1c0934f",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
