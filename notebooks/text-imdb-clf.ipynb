{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "838093c3-cb8a-4a10-aa5e-b3fb28d7617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:00:51,717 : INFO : No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "2022-01-17 15:00:51,739 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2022-01-17 15:00:51,771 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2022-01-17 15:00:51,772 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:00:52.083377: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "# train_dataset = ds['train'].batch(args.batch_size)\n",
    "train_dataset = ds['train']\n",
    "val_dataset = ds['test'].take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6248a603-30a8-44d0-91eb-ea09971feb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=10)\n",
    "def get_encoder(vocab_size=args.vocab_size):\n",
    "    encoder = TextVectorization(max_tokens=vocab_size)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d4af6-5c96-4861-9824-a3626388e89e",
   "metadata": {},
   "source": [
    "### Baseline. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9855a40-9719-4449-b070-f7785fc43644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                16064     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,129\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:15:14.780774: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1271 of 10000\n",
      "2021-12-20 16:15:24.780641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2539 of 10000\n",
      "2021-12-20 16:15:34.780499: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 3782 of 10000\n",
      "2021-12-20 16:15:44.782971: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5062 of 10000\n",
      "2021-12-20 16:15:54.783161: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6395 of 10000\n",
      "2021-12-20 16:16:04.782261: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 7649 of 10000\n",
      "2021-12-20 16:16:14.780100: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 8950 of 10000\n",
      "2021-12-20 16:16:23.157376: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.5572 - accuracy: 0.7463 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:18:31.684886: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1342 of 10000\n",
      "2021-12-20 16:18:41.672330: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2685 of 10000\n",
      "2021-12-20 16:18:51.678592: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 3988 of 10000\n",
      "2021-12-20 16:19:01.669707: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5270 of 10000\n",
      "2021-12-20 16:19:11.676965: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6543 of 10000\n",
      "2021-12-20 16:19:21.671241: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 7828 of 10000\n",
      "2021-12-20 16:19:31.676103: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 9125 of 10000\n",
      "2021-12-20 16:19:38.532035: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 294s 275ms/step - loss: 0.5569 - accuracy: 0.7464 - val_loss: 0.4963 - val_accuracy: 0.7651 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:19:57.737089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4850 - accuracy: 0.7795 - val_loss: 0.5144 - val_accuracy: 0.7634 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "742/782 [===========================>..] - ETA: 0s - loss: 0.4760 - accuracy: 0.7835\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4764 - accuracy: 0.7831 - val_loss: 0.4982 - val_accuracy: 0.7708 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4452 - accuracy: 0.7979 - val_loss: 0.4630 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4418 - accuracy: 0.7992 - val_loss: 0.4616 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4400 - accuracy: 0.7999 - val_loss: 0.4604 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4382 - accuracy: 0.8022 - val_loss: 0.4602 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4367 - accuracy: 0.8021 - val_loss: 0.4603 - val_accuracy: 0.7850 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.8026 - val_loss: 0.4599 - val_accuracy: 0.7844 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4334 - accuracy: 0.8034 - val_loss: 0.4652 - val_accuracy: 0.7817 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.8045\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4315 - accuracy: 0.8045 - val_loss: 0.4602 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4270 - accuracy: 0.8074 - val_loss: 0.4594 - val_accuracy: 0.7843 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4268 - accuracy: 0.8082 - val_loss: 0.4592 - val_accuracy: 0.7851 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4265 - accuracy: 0.8072 - val_loss: 0.4598 - val_accuracy: 0.7834 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8070 ETA: 0s - loss: 0.4261 - accuracy: 0.\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4263 - accuracy: 0.8074 - val_loss: 0.4592 - val_accuracy: 0.7848 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8076Restoring model weights from the end of the best epoch: 13.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4257 - accuracy: 0.8077 - val_loss: 0.4592 - val_accuracy: 0.7846 - lr: 1.0000e-06\n",
      "Epoch 00016: early stopping\n",
      "Val_accuracy: 0.7851200103759766\n",
      "Val_loss: 0.45919713377952576\n",
      "Accuracy: 0.8082000017166138\n"
     ]
    }
   ],
   "source": [
    "### Baseline. Bag of words. Preprocessing in dataset creation step.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    # declaring outputs as an input cause you have to declare tf.variable outside of tf function \n",
    "    def build_bag_of_words(tokens, label, outputs=tf.Variable(tf.zeros(args.small_vocab_size))):\n",
    "        # without it, tf saves the last state of the tensor\n",
    "        outputs.assign(tf.zeros_like(outputs))\n",
    "        for i in range(len(tokens)):\n",
    "            output_idx = tokens[i]\n",
    "            if output_idx >= tf.constant(args.small_vocab_size, dtype=tf.int64):\n",
    "                output_idx = tf.constant(1, dtype=tf.int64)\n",
    "            outputs[output_idx].assign(outputs[output_idx] + 1)\n",
    "        return outputs, label\n",
    "\n",
    "    ds_train = train_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    ds_val = val_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    # for d in ds_train.take(10):\n",
    "    #     print(d)\n",
    "        \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(250,)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    baseline_bag_of_words()\n",
    "\n",
    "# val_accuracy: 0.785; val_loss 0.456; accuracy: 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37034b81-b518-48fe-8246-2f0b3bcbf1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'all' b'my' b'cats' b'in' b'a' b'row' b'' b'' b'' b'' b'']\n",
      " [b'when' b'my' b'cat' b'sits' b'down' b'she' b'looks' b'like' b'a'\n",
      "  b'furby' b'toy']\n",
      " [b'the' b'cat' b'from' b'the' b'outer' b'space' b'' b'' b'' b'' b'']\n",
      " [b'sunshine' b'loves' b'to' b'sit' b'like' b'this' b'for' b'some'\n",
      "  b'reason' b'' b'']], shape=(4, 11), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'sunshine' b'for' b'outer' b'looks' b'down' b'all' b'like' b'this'\n",
      " b'she' b'sit' b'toy' b'loves' b'row' b'my' b'the' b'sits' b'some' b'when'\n",
      " b'in' b'space' b'furby' b'' b'to' b'cats' b'from' b'reason' b'a' b'cat'], shape=(28,), dtype=string)\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 1s 2ms/step - loss: 0.8469\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8071\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def stackoverflow_answer():\n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabulary_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        def call(self, inputs):  \n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            for i in range(batch_size):\n",
    "                string = inputs[i]\n",
    "                string_length = tf.shape(tf.where(tf.math.not_equal(string, b'')))[0]\n",
    "                string = string[:string_length]\n",
    "                string_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "                for s in string:\n",
    "                    string_array = string_array.write(string_array.size(), tf.where(tf.equal(s, self.vocabulary_size), 1.0, 0.0))\n",
    "                outputs = outputs.write(i, tf.cast(tf.reduce_any(tf.cast(string_array.stack(), dtype=tf.bool), axis=0), dtype=tf.float32))\n",
    "            return outputs.stack()\n",
    "        \n",
    "    labels = [[1], [0], [1], [0]]\n",
    "\n",
    "    texts  = ['All my cats in a row',\n",
    "              'When my cat sits down, she looks like a Furby toy!',\n",
    "              'The cat from the outer space',\n",
    "              'Sunshine loves to sit like this for some reason.']\n",
    "\n",
    "    DEFAULT_STRIP_REGEX = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n",
    "    tensor_of_strings = tf.constant(texts)\n",
    "    tensor_of_strings = tf.strings.lower(tensor_of_strings)\n",
    "    tensor_of_strings = tf.strings.regex_replace(tensor_of_strings, DEFAULT_STRIP_REGEX, \"\")\n",
    "    split_strings = tf.strings.split(tensor_of_strings).to_tensor()\n",
    "    print(split_strings)\n",
    "    flattened_split_strings = tf.reshape(split_strings, (split_strings.shape[0] * split_strings.shape[1]))\n",
    "    unique_words, _ = tf.unique(flattened_split_strings)\n",
    "    unique_words = tf.random.shuffle(unique_words)\n",
    "    print(unique_words)\n",
    "\n",
    "    bag_of_words = BagOfWords(vocabulary_size = unique_words)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((split_strings, labels))\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(bag_of_words)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy())\n",
    "    model.fit(train_dataset.batch(2), epochs=2)\n",
    "\n",
    "if False:\n",
    "    stackoverflow_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aab844-52e3-4bd8-9610-7a31f8e7f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Baseline. Bag of words. Preprocessing in a layer.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words_layer():\n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabulary_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocabulary_size = vocabulary_size\n",
    "        def call(self, inputs):  \n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            for i in range(batch_size):\n",
    "                int_string = inputs[i]\n",
    "                array_string = tf.TensorArray(dtype=tf.float32, size=self.vocabulary_size)\n",
    "                array_string.unstack(tf.zeros(self.vocabulary_size))\n",
    "                for int_word in int_string:\n",
    "                    idx = int_word\n",
    "                    idx = tf.cond(idx >= self.vocabulary_size, lambda: 1, lambda: tf.cast(idx, tf.int32))\n",
    "                    array_string = array_string.write(idx, array_string.read(idx) + 1.0)\n",
    "                outputs = outputs.write(i, array_string.stack())\n",
    "            return outputs.stack()\n",
    "        \n",
    "    encoder = get_encoder(args.small_vocab_size)\n",
    "    bag_of_words = BagOfWords(args.small_vocab_size)\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(bag_of_words)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # instead of build() on model\n",
    "    for d in train_dataset.batch(args.batch_size).take(1):\n",
    "        model(d[0])\n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(1)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    baseline_bag_of_words_layer()\n",
    "    \n",
    "# Val_accuracy: 0.7857599854469299\n",
    "# Val_loss: 0.45941051840782166\n",
    "# Accuracy: 0.7972400188446045"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da0d4-6800-4ed1-8b25-7bb883ba0e0a",
   "metadata": {},
   "source": [
    "### Rnn with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a7d78-9dd7-4879-b07d-277d741ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def rnn_with_embedding():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True))\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "if False:\n",
    "    rnn_with_embedding()\n",
    "\n",
    "# Epoch 3/50\n",
    "# 782/782 [======] - 314s 401ms/step - loss: 0.2752 - accuracy: 0.8867 - val_loss: 0.3107 - val_accuracy: 0.8667 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6cb06-b87b-400d-9de8-9808812b921f",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "feeb0471-2ecc-4d7e-aa56-93f5ecff8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_6 (TextV  (None, 2500)             1         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 64)                160064    \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,130\n",
      "Trainable params: 160,129\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3950 - accuracy: 0.8393 - val_loss: 0.3443 - val_accuracy: 0.8626 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "  1/782 [..............................] - ETA: 1:24 - loss: 0.5382 - accuracy: 0.7812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:24.176406: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2862 - accuracy: 0.8853 - val_loss: 0.3506 - val_accuracy: 0.8630 - lr: 0.0010\n",
      "Epoch 3/50\n",
      " 20/782 [..............................] - ETA: 2s - loss: 0.1955 - accuracy: 0.9172  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:27.218414: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 0.2106 - accuracy: 0.9177\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2106 - accuracy: 0.9177 - val_loss: 0.3586 - val_accuracy: 0.8681 - lr: 0.0010\n",
      "Epoch 4/50\n",
      " 20/782 [..............................] - ETA: 2s - loss: 0.1026 - accuracy: 0.9719  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:30.250760: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775/782 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9622Restoring model weights from the end of the best epoch: 1.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1134 - accuracy: 0.9622 - val_loss: 0.3568 - val_accuracy: 0.8710 - lr: 1.0000e-04\n",
      "Epoch 00004: early stopping\n",
      "Val_accuracy: 0.870959997177124\n",
      "Val_loss: 0.34430137276649475\n",
      "Accuracy: 0.9621599912643433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-12 18:19:33.197737: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "def tfidf():\n",
    "    \n",
    "    def get_tfidf_encoder():\n",
    "        encoder = TextVectorization(max_tokens=args.vocab_size, output_mode=\"tf_idf\")\n",
    "        encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "        return encoder\n",
    "        \n",
    "    encoder = get_tfidf_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # instead of build() on model\n",
    "    for d in train_dataset.batch(args.batch_size).take(1):\n",
    "        model(d[0])\n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(1)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    tfidf()\n",
    "    \n",
    "# Val_accuracy: 0.870959997177124\n",
    "# Val_loss: 0.34430137276649475\n",
    "# Accuracy: 0.9621599912643433"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906359c-396b-4b05-8b5d-8eb5b45a7bd1",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c330fa97-bae0-4f4e-b79d-f4b329fc79f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer_1 (KerasLayer)  (None, 250)               252343750 \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 64)                16064     \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,359,879\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 252,343,750\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.5694 - accuracy: 0.7081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:02.430234: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 6s 6ms/step - loss: 0.5679 - accuracy: 0.7097 - val_loss: 0.5133 - val_accuracy: 0.7561 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.5043 - accuracy: 0.7575"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:07.186301: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.5041 - accuracy: 0.7578 - val_loss: 0.5057 - val_accuracy: 0.7540 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4859 - accuracy: 0.7682 - val_loss: 0.6211 - val_accuracy: 0.6957 - lr: 0.0010\n",
      "Epoch 4/50\n",
      " 22/782 [..............................] - ETA: 1s - loss: 0.4777 - accuracy: 0.7699  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:10.746094: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 0.4738 - accuracy: 0.7763"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:13.682784: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4739 - accuracy: 0.7763 - val_loss: 0.4801 - val_accuracy: 0.7734 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4655 - accuracy: 0.7831 - val_loss: 0.5689 - val_accuracy: 0.7252 - lr: 0.0010\n",
      "Epoch 6/50\n",
      " 24/782 [..............................] - ETA: 1s - loss: 0.4535 - accuracy: 0.7812  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:17.070486: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759/782 [============================>.] - ETA: 0s - loss: 0.4586 - accuracy: 0.7854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:19.693249: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4607 - accuracy: 0.7836 - val_loss: 0.4752 - val_accuracy: 0.7739 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.4548 - accuracy: 0.7876"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:22.738540: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4546 - accuracy: 0.7880 - val_loss: 0.4694 - val_accuracy: 0.7735 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4509 - accuracy: 0.7898 - val_loss: 0.5223 - val_accuracy: 0.7417 - lr: 0.0010\n",
      "Epoch 9/50\n",
      " 24/782 [..............................] - ETA: 1s - loss: 0.4748 - accuracy: 0.7826  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:25.815949: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774/782 [============================>.] - ETA: 0s - loss: 0.4488 - accuracy: 0.7919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:28.396220: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4485 - accuracy: 0.7920 - val_loss: 0.4541 - val_accuracy: 0.7839 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4441 - accuracy: 0.7944 - val_loss: 0.4564 - val_accuracy: 0.7822 - lr: 0.0010\n",
      "Epoch 11/50\n",
      " 24/782 [..............................] - ETA: 1s - loss: 0.4098 - accuracy: 0.8138  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:31.593680: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760/782 [============================>.] - ETA: 0s - loss: 0.4440 - accuracy: 0.7958\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4431 - accuracy: 0.7959 - val_loss: 0.4683 - val_accuracy: 0.7736 - lr: 0.0010\n",
      "Epoch 12/50\n",
      " 25/782 [..............................] - ETA: 1s - loss: 0.4417 - accuracy: 0.7962  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:34.176108: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.4320 - accuracy: 0.8023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:36.777039: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4319 - accuracy: 0.8024 - val_loss: 0.4443 - val_accuracy: 0.7909 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "775/782 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.8030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:39.841457: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4308 - accuracy: 0.8026 - val_loss: 0.4440 - val_accuracy: 0.7922 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "765/782 [============================>.] - ETA: 0s - loss: 0.4299 - accuracy: 0.8039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:42.925019: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4302 - accuracy: 0.8035 - val_loss: 0.4429 - val_accuracy: 0.7935 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "764/782 [============================>.] - ETA: 0s - loss: 0.4303 - accuracy: 0.8023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:46.012356: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4299 - accuracy: 0.8024 - val_loss: 0.4428 - val_accuracy: 0.7925 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.4294 - accuracy: 0.8027"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:49.075938: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4297 - accuracy: 0.8026 - val_loss: 0.4427 - val_accuracy: 0.7936 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4294 - accuracy: 0.8038 - val_loss: 0.4443 - val_accuracy: 0.7921 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      " 24/782 [..............................] - ETA: 1s - loss: 0.4139 - accuracy: 0.8138  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:52.252189: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "778/782 [============================>.] - ETA: 0s - loss: 0.4289 - accuracy: 0.8039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:54.875597: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4287 - accuracy: 0.8040 - val_loss: 0.4418 - val_accuracy: 0.7934 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4289 - accuracy: 0.8050 - val_loss: 0.4431 - val_accuracy: 0.7930 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      " 25/782 [..............................] - ETA: 1s - loss: 0.4368 - accuracy: 0.7975  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:46:57.901524: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 0.4285 - accuracy: 0.8042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:00.485249: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4283 - accuracy: 0.8043 - val_loss: 0.4417 - val_accuracy: 0.7928 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4264 - accuracy: 0.8058 - val_loss: 0.4421 - val_accuracy: 0.7925 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      " 25/782 [..............................] - ETA: 1s - loss: 0.4525 - accuracy: 0.8037  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:03.547961: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/782 [============================>.] - ETA: 0s - loss: 0.4260 - accuracy: 0.8059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:06.091249: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4265 - accuracy: 0.8056 - val_loss: 0.4414 - val_accuracy: 0.7928 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "776/782 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:09.148011: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4263 - accuracy: 0.8063 - val_loss: 0.4414 - val_accuracy: 0.7931 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "777/782 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8061\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4262 - accuracy: 0.8058 - val_loss: 0.4425 - val_accuracy: 0.7921 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      " 24/782 [..............................] - ETA: 1s - loss: 0.4225 - accuracy: 0.8151  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:12.201004: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769/782 [============================>.] - ETA: 0s - loss: 0.4267 - accuracy: 0.8057"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:14.753705: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4263 - accuracy: 0.8059 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-06\n",
      "Epoch 26/50\n",
      "763/782 [============================>.] - ETA: 0s - loss: 0.4259 - accuracy: 0.8059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:17.917005: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4260 - accuracy: 0.8060 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-06\n",
      "Epoch 27/50\n",
      "767/782 [============================>.] - ETA: 0s - loss: 0.4249 - accuracy: 0.8070"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:21.097696: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-07\n",
      "Epoch 28/50\n",
      "768/782 [============================>.] - ETA: 0s - loss: 0.4254 - accuracy: 0.8068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:24.175000: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4260 - accuracy: 0.8066 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-07\n",
      "Epoch 29/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-08\n",
      "Epoch 30/50\n",
      " 24/782 [..............................] - ETA: 1s - loss: 0.4150 - accuracy: 0.8086  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:27.299396: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/782 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:29.837255: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-08\n",
      "Epoch 31/50\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-09\n",
      "Epoch 32/50\n",
      " 26/782 [..............................] - ETA: 1s - loss: 0.4648 - accuracy: 0.7849  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:32.945009: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764/782 [============================>.] - ETA: 0s - loss: 0.4268 - accuracy: 0.8062\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-09\n",
      "Epoch 33/50\n",
      " 25/782 [..............................] - ETA: 1s - loss: 0.4150 - accuracy: 0.8263  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:35.548774: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767/782 [============================>.] - ETA: 0s - loss: 0.4263 - accuracy: 0.8060Restoring model weights from the end of the best epoch: 30.\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932 - lr: 1.0000e-10\n",
      "Epoch 00033: early stopping\n",
      "Val_accuracy: 0.7936000227928162\n",
      "Val_loss: 0.44133269786834717\n",
      "Accuracy: 0.8065599799156189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 15:47:38.099661: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "def word2vec():\n",
    "    \n",
    "    # read the models remotely from google without downloading them\n",
    "    os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n",
    "    \n",
    "    hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/Wiki-words-250/2\",\n",
    "                           input_shape=[], dtype=tf.string)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    ds_train = train_dataset.shuffle(200000).batch(args.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_val = val_dataset.batch(args.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if False: \n",
    "    word2vec()\n",
    "    \n",
    "# loss: 0.4260 - accuracy: 0.8065 - val_loss: 0.4413 - val_accuracy: 0.7932"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597494b5-f388-47ca-8723-c190a1c0934f",
   "metadata": {},
   "source": [
    "### Different embeddings, glove, bert, transformer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
