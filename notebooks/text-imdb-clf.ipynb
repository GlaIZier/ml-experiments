{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838093c3-cb8a-4a10-aa5e-b3fb28d7617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 17:43:40,704 : INFO : No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "2021-12-28 17:43:40,707 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2021-12-28 17:43:40,711 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2021-12-28 17:43:40,711 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 17:43:41.151842: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "# train_dataset = ds['train'].batch(args.batch_size)\n",
    "train_dataset = ds['train']\n",
    "val_dataset = ds['test'].take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6248a603-30a8-44d0-91eb-ea09971feb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "@functools.lru_cache(maxsize=10)\n",
    "def get_encoder(vocab_size=args.vocab_size):\n",
    "    encoder = TextVectorization(max_tokens=vocab_size)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d4af6-5c96-4861-9824-a3626388e89e",
   "metadata": {},
   "source": [
    "### Baseline. Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9855a40-9719-4449-b070-f7785fc43644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                16064     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,129\n",
      "Trainable params: 16,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:15:14.780774: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1271 of 10000\n",
      "2021-12-20 16:15:24.780641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2539 of 10000\n",
      "2021-12-20 16:15:34.780499: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 3782 of 10000\n",
      "2021-12-20 16:15:44.782971: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5062 of 10000\n",
      "2021-12-20 16:15:54.783161: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6395 of 10000\n",
      "2021-12-20 16:16:04.782261: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 7649 of 10000\n",
      "2021-12-20 16:16:14.780100: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 8950 of 10000\n",
      "2021-12-20 16:16:23.157376: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/782 [============================>.] - ETA: 0s - loss: 0.5572 - accuracy: 0.7463 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:18:31.684886: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 1342 of 10000\n",
      "2021-12-20 16:18:41.672330: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 2685 of 10000\n",
      "2021-12-20 16:18:51.678592: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 3988 of 10000\n",
      "2021-12-20 16:19:01.669707: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 5270 of 10000\n",
      "2021-12-20 16:19:11.676965: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 6543 of 10000\n",
      "2021-12-20 16:19:21.671241: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 7828 of 10000\n",
      "2021-12-20 16:19:31.676103: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:380] Filling up shuffle buffer (this may take a while): 9125 of 10000\n",
      "2021-12-20 16:19:38.532035: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:405] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 294s 275ms/step - loss: 0.5569 - accuracy: 0.7464 - val_loss: 0.4963 - val_accuracy: 0.7651 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 16:19:57.737089: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4850 - accuracy: 0.7795 - val_loss: 0.5144 - val_accuracy: 0.7634 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "742/782 [===========================>..] - ETA: 0s - loss: 0.4760 - accuracy: 0.7835\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4764 - accuracy: 0.7831 - val_loss: 0.4982 - val_accuracy: 0.7708 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4452 - accuracy: 0.7979 - val_loss: 0.4630 - val_accuracy: 0.7837 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4418 - accuracy: 0.7992 - val_loss: 0.4616 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4400 - accuracy: 0.7999 - val_loss: 0.4604 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4382 - accuracy: 0.8022 - val_loss: 0.4602 - val_accuracy: 0.7851 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4367 - accuracy: 0.8021 - val_loss: 0.4603 - val_accuracy: 0.7850 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.8026 - val_loss: 0.4599 - val_accuracy: 0.7844 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4334 - accuracy: 0.8034 - val_loss: 0.4652 - val_accuracy: 0.7817 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "766/782 [============================>.] - ETA: 0s - loss: 0.4319 - accuracy: 0.8045\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4315 - accuracy: 0.8045 - val_loss: 0.4602 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4270 - accuracy: 0.8074 - val_loss: 0.4594 - val_accuracy: 0.7843 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4268 - accuracy: 0.8082 - val_loss: 0.4592 - val_accuracy: 0.7851 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4265 - accuracy: 0.8072 - val_loss: 0.4598 - val_accuracy: 0.7834 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "771/782 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.8070 ETA: 0s - loss: 0.4261 - accuracy: 0.\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "782/782 [==============================] - 1s 1ms/step - loss: 0.4263 - accuracy: 0.8074 - val_loss: 0.4592 - val_accuracy: 0.7848 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "773/782 [============================>.] - ETA: 0s - loss: 0.4255 - accuracy: 0.8076Restoring model weights from the end of the best epoch: 13.\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4257 - accuracy: 0.8077 - val_loss: 0.4592 - val_accuracy: 0.7846 - lr: 1.0000e-06\n",
      "Epoch 00016: early stopping\n",
      "Val_accuracy: 0.7851200103759766\n",
      "Val_loss: 0.45919713377952576\n",
      "Accuracy: 0.8082000017166138\n"
     ]
    }
   ],
   "source": [
    "### Baseline. Bag of words. Preprocessing in dataset creation step.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    # declaring outputs as an input cause you have to declare tf.variable outside of tf function \n",
    "    def build_bag_of_words(tokens, label, outputs=tf.Variable(tf.zeros(args.small_vocab_size))):\n",
    "        # without it, tf saves the last state of the tensor\n",
    "        outputs.assign(tf.zeros_like(outputs))\n",
    "        for i in range(len(tokens)):\n",
    "            output_idx = tokens[i]\n",
    "            if output_idx >= tf.constant(args.small_vocab_size, dtype=tf.int64):\n",
    "                output_idx = tf.constant(1, dtype=tf.int64)\n",
    "            outputs[output_idx].assign(outputs[output_idx] + 1)\n",
    "        return outputs, label\n",
    "\n",
    "    ds_train = train_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    ds_val = val_dataset.map(lambda sent, l: (get_encoder()(sent), l)).map(build_bag_of_words).cache().shuffle(10000).batch(args.batch_size)\n",
    "    # for d in ds_train.take(10):\n",
    "    #     print(d)\n",
    "        \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(250,)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    baseline_bag_of_words()\n",
    "\n",
    "# val_accuracy: 0.785; val_loss 0.456; accuracy: 0.803"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37034b81-b518-48fe-8246-2f0b3bcbf1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'all' b'my' b'cats' b'in' b'a' b'row' b'' b'' b'' b'' b'']\n",
      " [b'when' b'my' b'cat' b'sits' b'down' b'she' b'looks' b'like' b'a'\n",
      "  b'furby' b'toy']\n",
      " [b'the' b'cat' b'from' b'the' b'outer' b'space' b'' b'' b'' b'' b'']\n",
      " [b'sunshine' b'loves' b'to' b'sit' b'like' b'this' b'for' b'some'\n",
      "  b'reason' b'' b'']], shape=(4, 11), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'sunshine' b'for' b'outer' b'looks' b'down' b'all' b'like' b'this'\n",
      " b'she' b'sit' b'toy' b'loves' b'row' b'my' b'the' b'sits' b'some' b'when'\n",
      " b'in' b'space' b'furby' b'' b'to' b'cats' b'from' b'reason' b'a' b'cat'], shape=(28,), dtype=string)\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 1s 2ms/step - loss: 0.8469\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8071\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def test1():\n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabulary_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        def call(self, inputs):  \n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            for i in range(batch_size):\n",
    "                string = inputs[i]\n",
    "                string_length = tf.shape(tf.where(tf.math.not_equal(string, b'')))[0]\n",
    "                string = string[:string_length]\n",
    "                string_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "                for s in string:\n",
    "                    string_array = string_array.write(string_array.size(), tf.where(tf.equal(s, self.vocabulary_size), 1.0, 0.0))\n",
    "                outputs = outputs.write(i, tf.cast(tf.reduce_any(tf.cast(string_array.stack(), dtype=tf.bool), axis=0), dtype=tf.float32))\n",
    "            return outputs.stack()\n",
    "        \n",
    "    labels = [[1], [0], [1], [0]]\n",
    "\n",
    "    texts  = ['All my cats in a row',\n",
    "              'When my cat sits down, she looks like a Furby toy!',\n",
    "              'The cat from the outer space',\n",
    "              'Sunshine loves to sit like this for some reason.']\n",
    "\n",
    "    DEFAULT_STRIP_REGEX = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n",
    "    tensor_of_strings = tf.constant(texts)\n",
    "    tensor_of_strings = tf.strings.lower(tensor_of_strings)\n",
    "    tensor_of_strings = tf.strings.regex_replace(tensor_of_strings, DEFAULT_STRIP_REGEX, \"\")\n",
    "    split_strings = tf.strings.split(tensor_of_strings).to_tensor()\n",
    "    print(split_strings)\n",
    "    flattened_split_strings = tf.reshape(split_strings, (split_strings.shape[0] * split_strings.shape[1]))\n",
    "    unique_words, _ = tf.unique(flattened_split_strings)\n",
    "    unique_words = tf.random.shuffle(unique_words)\n",
    "    print(unique_words)\n",
    "\n",
    "    bag_of_words = BagOfWords(vocabulary_size = unique_words)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((split_strings, labels))\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(bag_of_words)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss = tf.keras.losses.BinaryCrossentropy())\n",
    "    model.fit(train_dataset.batch(2), epochs=2)\n",
    "\n",
    "test1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd0d3be6-61a1-4e49-8929-38e2d7fd44e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot convert a partially known TensorShape <unknown> to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_73307/409747735.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# t = t.write(0, 0.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, index, name)\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m     \"\"\"\n\u001b[0;32m-> 1152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_implementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtf_should_use\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_use_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    785\u001b[0m             index)\n\u001b[1;32m    786\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_after_read\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/ops/tensor_array_ops.py\u001b[0m in \u001b[0;36m_maybe_zero\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m       val = self._tensor_array[ix] = array_ops.zeros(\n\u001b[0m\u001b[1;32m    854\u001b[0m           shape=self._element_shape, dtype=self._dtype)\n\u001b[1;32m    855\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_tensor_shape_tensor_conversion_function\u001b[0;34m(s, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    364\u001b[0m         f\"Cannot convert a partially known TensorShape {s} to a Tensor.\")\n\u001b[1;32m    365\u001b[0m   \u001b[0ms_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert a partially known TensorShape <unknown> to a Tensor."
     ]
    }
   ],
   "source": [
    "t = tf.TensorArray(dtype=tf.float32, size=2)\n",
    "# t = t.write(0, 0.0)\n",
    "t.read(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15aab844-52e3-4bd8-9610-7a31f8e7f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 11  14  34 ...   0   0   0]\n",
      " [ 10  26  75 ...   0   0   0]\n",
      " [  1   1   2 ...   0   0   0]\n",
      " ...\n",
      " [ 11  18 110 ...   0   0   0]\n",
      " [  1   1   1 ...   0   0   0]\n",
      " [  1   1  10 ...   0   0   0]], shape=(32, 552), dtype=int64)\n",
      "trace  tf.Tensor(32, shape=(), dtype=int32)\n",
      "exec tf  32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"bag_of_words_23\" (type BagOfWords).\n\nAttempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x15c209580>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(32, 552), dtype=int64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_73307/2522439765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mbaseline_bag_of_words_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_73307/2522439765.py\u001b[0m in \u001b[0;36mbaseline_bag_of_words_layer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# tf.print(encoder(d[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mbag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_73307/2522439765.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;31m# array_string = array_string.write(idx, array_string.read(idx) + 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# string_array = string_array.write(string_array.size(), tf.where(tf.equal(s, self.vocabulary_size), 1.0, 0.0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"bag_of_words_23\" (type BagOfWords).\n\nAttempt to convert a value (<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x15c209580>) with an unsupported type (<class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) to a Tensor.\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(32, 552), dtype=int64)"
     ]
    }
   ],
   "source": [
    "### Baseline. Bag of words. Preprocessing in a layer.\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words_layer():\n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocabulary_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocabulary_size = vocabulary_size\n",
    "\n",
    "        def call(self, inputs):  \n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            print('trace ', batch_size)\n",
    "            tf.print('exec tf ', batch_size)\n",
    "            outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "            for i in range(batch_size):\n",
    "                int_string = inputs[i]\n",
    "                # string_length = tf.shape(tf.where(tf.math.not_equal(string, b'')))[0]\n",
    "                # string = string[:string_length]\n",
    "                # string_array = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True) \n",
    "                array_string = tf.TensorArray(dtype=tf.float32, size=self.vocabulary_size)\n",
    "                # array_string = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True) \n",
    "                for int_word in int_string:\n",
    "                    idx = int_word\n",
    "                    if idx >= tf.constant(self.vocabulary_size, dtype=tf.int64):\n",
    "                        idx = tf.constant(1, dtype=tf.int64)\n",
    "                    # tf.print(array_string)\n",
    "                    array_string = array_string.write(idx, 1) \n",
    "                    # array_string = array_string.write(idx, array_string.read(idx) + 1) \n",
    "                    # string_array = string_array.write(string_array.size(), tf.where(tf.equal(s, self.vocabulary_size), 1.0, 0.0))\n",
    "                outputs = outputs.write(i, array_string)\n",
    "            return outputs.stack()\n",
    "        \n",
    "    encoder = get_encoder(args.small_vocab_size)\n",
    "    bag_of_words = BagOfWords(args.small_vocab_size)\n",
    "    for d in train_dataset.batch(args.batch_size).take(1):\n",
    "        print(encoder(d[0]))\n",
    "        # tf.print(encoder(d[0]))\n",
    "        bag_of_words(encoder(d[0]))\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    # model.add(keras.layers.InputLayer(input_shape=(None,), dtype=tf.string))\n",
    "    model.add(encoder)\n",
    "    model.add(bag_of_words)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    history = model.fit(train_dataset.batch(args.batch_size), validation_data=val_dataset.batch(args.batch_size), epochs=args.epochs)\n",
    "    \n",
    "    print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "    print('Val_loss:', min(history.history['val_loss']))\n",
    "    print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "if True: \n",
    "    baseline_bag_of_words_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "936597a8-a8fc-43f7-ac12-739af800e5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Exception encountered when calling layer \"bag_of_words_46\" (type BagOfWords).\n\nin user code:\n\n    File \"/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_58431/748529715.py\", line 26, in call  *\n        self.outputs[i][output_idx].assign(self.outputs[i][output_idx] + 1)\n\n    AttributeError: 'Tensor' object has no attribute 'assign'\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, None), dtype=int64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_58431/748529715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mbaseline_bag_of_words1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_58431/748529715.py\u001b[0m in \u001b[0;36mbaseline_bag_of_words1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/ml-experiments/.venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer \"bag_of_words_46\" (type BagOfWords).\n\nin user code:\n\n    File \"/var/folders/g9/6qklj4h53bv0c1rjnffg7bmw0000gp/T/ipykernel_58431/748529715.py\", line 26, in call  *\n        self.outputs[i][output_idx].assign(self.outputs[i][output_idx] + 1)\n\n    AttributeError: 'Tensor' object has no attribute 'assign'\n\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, None), dtype=int64)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def baseline_bag_of_words2():\n",
    "    \n",
    "    class BagOfWords(tf.keras.layers.Layer):\n",
    "        def __init__(self, vocab_size=args.small_vocab_size, batch_size=args.batch_size):\n",
    "            super(BagOfWords, self).__init__()\n",
    "            self.vocab_size = vocab_size\n",
    "            self.batch_size = batch_size\n",
    "            self.outputs = tf.Variable(tf.zeros([batch_size, vocab_size]))\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            super().build(input_shape)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            if tf.shape(inputs)[-1] == None:\n",
    "                return tf.zeros([self.batch_size, self.vocab_size])\n",
    "            self.outputs.assign(tf.zeros([self.batch_size, self.vocab_size]))\n",
    "            for i in range(tf.shape(inputs)[0]):\n",
    "                for ii in range(tf.shape(inputs)[-1]):\n",
    "                    output_idx = inputs[i][ii]\n",
    "                    if output_idx >= tf.constant(self.vocab_size, dtype=tf.int64):\n",
    "                        output_idx = tf.constant(1, dtype=tf.int64)\n",
    "                    self.outputs[i][output_idx].assign(self.outputs[i][output_idx] + 1)                        \n",
    "            return outputs\n",
    "\n",
    "    encoder = get_encoder(args.small_vocab_size)\n",
    "    bag_of_words = BagOfWords(args.small_vocab_size, args.batch_size)\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=(None,), dtype=tf.string))\n",
    "    model.add(encoder)\n",
    "    model.add(bag_of_words)\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=10, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=3, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset.batch(32), epochs=args.epochs)\n",
    "\n",
    "baseline_bag_of_words2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8da0d4-6800-4ed1-8b25-7bb883ba0e0a",
   "metadata": {},
   "source": [
    "### Rnn with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9a7d78-9dd7-4879-b07d-277d741ef048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "def rnn_with_embedding():\n",
    "    encoder = get_encoder()\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    model.add(encoder)\n",
    "    model.add(keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True))\n",
    "    model.add(keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    monitor='val_loss'\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "if False:\n",
    "    rnn_with_embedding()\n",
    "\n",
    "# Epoch 3/50\n",
    "# 782/782 [======] - 314s 401ms/step - loss: 0.2752 - accuracy: 0.8867 - val_loss: 0.3107 - val_accuracy: 0.8667 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597494b5-f388-47ca-8723-c190a1c0934f",
   "metadata": {},
   "source": [
    "### Different embeddings, glove, bert, transformer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
