{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24030bef-84b3-4bb0-9386-2e10e3794d2b",
   "metadata": {},
   "source": [
    "# Bag of words implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb8c66-6545-4e74-ac5a-883cd31c690b",
   "metadata": {},
   "source": [
    "Try to implement a simple sentiment classifier from scratch and check how difficult the process is. \n",
    "\n",
    "inspuired by\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "https://developers.google.com/machine-learning/guides/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c536807-d1c3-46bb-96a1-a0d19b29c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:31,593 : INFO : No config specified, defaulting to config: imdb_reviews/plain_text\n",
      "2023-12-13 18:41:31,595 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2023-12-13 18:41:31,596 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2023-12-13 18:41:31,648 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.13.0\n",
      "Eager mode:  True\n",
      "GPU is NOT AVAILABLE\n",
      "25000\n",
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:31.668504: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Todo control logging with debug\n",
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "print(test_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "# train_dataset = ds['train'].batch(args.batch_size)\n",
    "train_dataset = ds['train']\n",
    "val_dataset = ds['test'].take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291fb79-b188-4b19-826f-87c97060bfec",
   "metadata": {},
   "source": [
    "## 1. Bag of words from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a29783a-77e4-42c1-9869-190d1645e908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_processed layer: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all']\n",
      "one_processed model: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:34.901141: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:41:34.927698: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class OneTextPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OneTextPreprocessing, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        # print(f\"Type inputs: {type(inputs)}\")\n",
    "        # print(f\"Inputs shape: {inputs.shape}\")\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            byte_string = string_tensor.numpy()\n",
    "            # print(f\"byte_string: {byte_string}\")\n",
    "            processed = ''.join(c for c in byte_string.decode('UTF-8').lower() if c.isalnum() or c == \" \")\n",
    "            # print(f\"type(processed): {type(processed)}\")\n",
    "            outputs = outputs.write(i, processed)\n",
    "        stacked_output = outputs.stack()\n",
    "        # print(f\"type(stacked_output): {type(stacked_output)}\")\n",
    "        # print(f\"tf.shape(stacked_output): {tf.shape(stacked_output)}\")\n",
    "        # outputs shape: B, Ragged texts\n",
    "        return stacked_output\n",
    "\n",
    "one_text_preprocessing = OneTextPreprocessing()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    one_processed = one_text_preprocessing(d[0])\n",
    "    print(f\"one_processed layer: {one_processed}\")\n",
    "\n",
    "one_model = keras.models.Sequential()\n",
    "one_model.add(one_text_preprocessing)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    one_processed = one_model(d[0])\n",
    "    print(f\"one_processed model: {one_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9ce39f-ab02-4c0c-8368-b2951f0d34e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_processed layer: <tf.RaggedTensor [[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie.', b\"Don't\",\n",
      "  b'be', b'lured', b'in', b'by', b'Christopher', b'Walken', b'or',\n",
      "  b'Michael', b'Ironside.', b'Both', b'are', b'great', b'actors,', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history.', b'Even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b\"movie's\", b'ridiculous', b'storyline.', b'This',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'US', b'propaganda',\n",
      "  b'piece.', b'The', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'Columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions.', b'Maria', b'Conchita', b'Alonso',\n",
      "  b'appeared', b'phony,', b'and', b'her', b'pseudo-love', b'affair',\n",
      "  b'with', b'Walken', b'was', b'nothing', b'but', b'a', b'pathetic',\n",
      "  b'emotional', b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid',\n",
      "  b'of', b'any', b'real', b'meaning.', b'I', b'am', b'disappointed',\n",
      "  b'that', b'there', b'are', b'movies', b'like', b'this,', b'ruining',\n",
      "  b\"actor's\", b'like', b'Christopher', b\"Walken's\", b'good', b'name.', b'I',\n",
      "  b'could', b'barely', b'sit', b'through', b'it.']                          ,\n",
      " [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films,', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including,', b'really', b'tired,',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot.', b'However', b'on',\n",
      "  b'this', b'occasion', b'I', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish.', b'The', b'plot', b'development', b'was',\n",
      "  b'constant.', b'Constantly', b'slow', b'and', b'boring.', b'Things',\n",
      "  b'seemed', b'to', b'happen,', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why.', b'I',\n",
      "  b'admit,', b'I', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film,', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else.', b'I', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all.']                                               ]>\n",
      "two_processed model: <tf.RaggedTensor [[b'this', b'was', b'an', b'absolutely', b'terrible', b'movie', b'dont',\n",
      "  b'be', b'lured', b'in', b'by', b'christopher', b'walken', b'or',\n",
      "  b'michael', b'ironside', b'both', b'are', b'great', b'actors', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history', b'even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b'movies', b'ridiculous', b'storyline', b'this',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'us', b'propaganda',\n",
      "  b'piece', b'the', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions', b'maria', b'conchita', b'alonso',\n",
      "  b'appeared', b'phony', b'and', b'her', b'pseudolove', b'affair', b'with',\n",
      "  b'walken', b'was', b'nothing', b'but', b'a', b'pathetic', b'emotional',\n",
      "  b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid', b'of', b'any',\n",
      "  b'real', b'meaning', b'i', b'am', b'disappointed', b'that', b'there',\n",
      "  b'are', b'movies', b'like', b'this', b'ruining', b'actors', b'like',\n",
      "  b'christopher', b'walkens', b'good', b'name', b'i', b'could', b'barely',\n",
      "  b'sit', b'through', b'it']                                                ,\n",
      " [b'i', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including', b'really', b'tired',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot', b'however', b'on',\n",
      "  b'this', b'occasion', b'i', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish', b'the', b'plot', b'development', b'was',\n",
      "  b'constant', b'constantly', b'slow', b'and', b'boring', b'things',\n",
      "  b'seemed', b'to', b'happen', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why', b'i',\n",
      "  b'admit', b'i', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else', b'i', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all']                                                ]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:38.564331: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:41:38.624440: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class TwoTokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(TwoTokenizer, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        # print(f\"Type inputs: {type(inputs)}\")\n",
    "        # print(f\"Inputs shape: {inputs.shape}\")\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        tokens_flat_tensor_array = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        str_len_tensor_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        start_len = 0\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            byte_string = string_tensor.numpy()\n",
    "            # print(f\"byte_string: {byte_string}\")\n",
    "            string = byte_string.decode('UTF-8')\n",
    "            tokens = string.split()\n",
    "            for iw in tf.range(len(tokens)):\n",
    "                tokens_flat_tensor_array = tokens_flat_tensor_array.write(start_len + iw, tokens[iw])\n",
    "            str_len_tensor_array = str_len_tensor_array.write(i, len(tokens))\n",
    "            start_len += len(tokens)\n",
    "        \n",
    "        ragged_tensor = tf.RaggedTensor.from_row_lengths(\n",
    "                values=tokens_flat_tensor_array.stack(),\n",
    "                row_lengths=str_len_tensor_array.stack())\n",
    "        # print(f\"type(ragged_tensor): {type(ragged_tensor)}\")\n",
    "        # print(f\"tf.shape(ragged_tensor): {tf.shape(ragged_tensor)}\")\n",
    "        # outputs shape: Ragged tensor: B, Tokens\n",
    "        return ragged_tensor\n",
    "        \n",
    "two_tokenizer = TwoTokenizer()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    two_processed = two_tokenizer(d[0])\n",
    "    print(f\"two_processed layer: {two_processed}\")\n",
    "\n",
    "two_model = keras.models.Sequential()\n",
    "two_model.add(one_text_preprocessing)\n",
    "two_model.add(two_tokenizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    two_processed = two_model(d[0])\n",
    "    print(f\"two_processed model: {two_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f851e838-d5fa-4b3f-832d-11dee20c73e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:44.758196: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:41:45.727213: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:41:45.817624: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_processed model: [[43.  2.  1.  2.  1.  0.  1.  3.  5.  1.  2.  2.  0.  1.  0.  1.  3.  0.\n",
      "   3.  2.  0.  0.  2.  0.  0.  2.  0.  1.  0.  2.  0.  0.  1.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "   3.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.\n",
      "   0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  2.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38.  5.  4.  2.  5.  4.  1.  0.  3.  1.  6.  0.  0.  1.  0.  1.  3.  3.\n",
      "   0.  3.  0.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  1.  0.  0.  2.  0.\n",
      "   0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "three_processed model with batch normalization: [[ 0.9999199  -0.9997778  -0.9997779   0.         -0.99987507 -0.99987507\n",
      "   0.          0.99977785  0.9995005   0.         -0.99987507  0.99950033\n",
      "   0.          0.          0.          0.          0.         -0.99977785\n",
      "   0.99977785 -0.99800587  0.          0.          0.99950033 -0.99950033\n",
      "   0.          0.99950033 -0.99950033  0.998006   -0.998006    0.99950033\n",
      "   0.         -0.998006    0.998006    0.         -0.99950033  0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.99950033  0.          0.          0.         -0.998006    0.998006\n",
      "  -0.998006    0.          0.998006    0.998006    0.          0.\n",
      "   0.99977785  0.          0.          0.         -0.998006    0.\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.99950033  0.          0.\n",
      "   0.          0.998006    0.          0.          0.99950033  0.99950033\n",
      "   0.          0.          0.          0.          0.998006    0.\n",
      "   0.         -0.998006    0.         -0.998006    0.          0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.99950033  0.         -0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.          0.         -0.998006   -0.998006    0.          0.\n",
      "   0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.998006    0.998006    0.\n",
      "   0.          0.          0.          0.         -0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.998006    0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.99950033  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.9999199   0.99977803  0.9997778   0.          0.99987507  0.99987507\n",
      "   0.         -0.99977785 -0.9995003   0.          0.99987507 -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.99977785\n",
      "  -0.99977785  0.99800634  0.          0.         -0.99950033  0.99950033\n",
      "   0.         -0.99950033  0.99950033 -0.998006    0.998006   -0.99950033\n",
      "   0.          0.998006   -0.998006    0.          0.99950033  0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "  -0.99950033  0.          0.          0.          0.998006   -0.998006\n",
      "   0.998006    0.         -0.998006   -0.998006    0.          0.\n",
      "  -0.99977785  0.          0.          0.          0.998006    0.\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.99950033  0.          0.\n",
      "   0.         -0.998006    0.          0.         -0.99950033 -0.99950033\n",
      "   0.          0.          0.          0.         -0.998006    0.\n",
      "   0.          0.998006    0.          0.998006    0.         -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.998006    0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.99950033  0.          0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.          0.          0.998006    0.998006    0.          0.\n",
      "  -0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.998006   -0.998006    0.\n",
      "   0.          0.          0.          0.          0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.998006   -0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.99950033  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "three_processed model with layer normalization: [[15.267825    0.55133015  0.19239126  0.55133015  0.19239126 -0.16654766\n",
      "   0.19239126  0.9102691   1.6281468   0.19239126  0.55133015  0.55133015\n",
      "  -0.16654766  0.19239126 -0.16654766  0.19239126  0.9102691  -0.16654766\n",
      "   0.9102691   0.55133015 -0.16654766 -0.16654766  0.55133015 -0.16654766\n",
      "  -0.16654766  0.55133015 -0.16654766  0.19239126 -0.16654766  0.55133015\n",
      "  -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "   0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766  0.19239126  0.19239126 -0.16654766 -0.16654766\n",
      "   0.9102691  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766  0.55133015 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766  0.55133015  0.55133015\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.55133015\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766  0.19239126  0.19239126 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766]\n",
      " [14.861998    1.8015503   1.4057791   0.61423683  1.8015503   1.4057791\n",
      "   0.21846567 -0.17730547  1.010008    0.21846567  2.1973214  -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547  0.21846567  1.010008    1.010008\n",
      "  -0.17730547  1.010008   -0.17730547 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547  0.61423683 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547  0.61423683 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567  0.21846567  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547  0.21846567  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547  0.21846567  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.61423683 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:45.958031: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import functools\n",
    "\n",
    "class ThreeVectorizer(tf.keras.layers.Layer):\n",
    "    def __init__(self, token_to_int: dict):\n",
    "        super(ThreeVectorizer, self).__init__()\n",
    "        self._token_to_int = token_to_int\n",
    "        self._vocab_size = len(self._token_to_int) + 1\n",
    "    \n",
    "    def call(self, inputs):  \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            tokens = inputs[i]\n",
    "            array_string = tf.TensorArray(dtype=tf.float32, size=self._vocab_size)\n",
    "            array_string.unstack(tf.zeros(self._vocab_size))\n",
    "            for token in tokens:\n",
    "                idx = self._token_to_int.get(token.numpy(), tf.constant(0))\n",
    "                array_string = array_string.write(idx, array_string.read(idx) + 1.0)\n",
    "            outputs = outputs.write(i, array_string.stack())\n",
    "        return outputs.stack()\n",
    "\n",
    "    @classmethod\n",
    "    @functools.lru_cache(maxsize=10)\n",
    "    def from_train_data(cls, train_ds, vocab_size=args.small_vocab_size, batch_size=args.batch_size, take=5):\n",
    "        _one_text_preprocessing = OneTextPreprocessing()\n",
    "        _two_tokenizer = TwoTokenizer()\n",
    "        _preproc_mdl = keras.models.Sequential()\n",
    "        _preproc_mdl.add(_one_text_preprocessing)\n",
    "        _preproc_mdl.add(_two_tokenizer)\n",
    "\n",
    "        _counter = Counter()\n",
    "        for d in train_ds.batch(batch_size).take(take):\n",
    "            _processed = _preproc_mdl(d[0])\n",
    "            for b in _processed:\n",
    "                _counter.update(b.numpy().tolist())\n",
    "\n",
    "        # +-1 cause we need space for unknown tokens with 0 index\n",
    "        _token_dict = {k: tf.cast(i + 1, tf.int32) for i, (k, _) in enumerate(_counter.most_common(vocab_size - 1))}\n",
    "        return cls(_token_dict)\n",
    "\n",
    "three_vectorizer = ThreeVectorizer.from_train_data(train_dataset)\n",
    "\n",
    "three_model = keras.models.Sequential()\n",
    "three_model.add(one_text_preprocessing)\n",
    "three_model.add(two_tokenizer)\n",
    "three_model.add(three_vectorizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    three_processed = three_model(d[0])\n",
    "    print(f\"three_processed model: {three_processed}\")\n",
    "\n",
    "three_model.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    three_processed = three_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"three_processed model with batch normalization: {three_processed}\")\n",
    "\n",
    "three_model.pop()\n",
    "three_model.add(tf.keras.layers.LayerNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    three_processed = three_model(d[0])\n",
    "    print(f\"three_processed model with layer normalization: {three_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed165be-e50d-40c6-b81e-ff5747883ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four_processed model with batch normalization: [[0.65778136]\n",
      " [0.32867935]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:41:49.901409: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "four_model = keras.models.Sequential()\n",
    "four_model.add(one_text_preprocessing)\n",
    "four_model.add(two_tokenizer)\n",
    "four_model.add(three_vectorizer)\n",
    "four_model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "four_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "four_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    four_processed = four_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"four_processed model with batch normalization: {four_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf7c32-d26c-48bf-afc5-003417e9ea31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "five_model = keras.models.Sequential()\n",
    "five_model.add(one_text_preprocessing)\n",
    "five_model.add(two_tokenizer)\n",
    "five_model.add(three_vectorizer)\n",
    "five_model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "five_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "five_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# instead of model.build() we can just call the model on some data\n",
    "for d in train_dataset.batch(args.batch_size).take(1):\n",
    "    five_model(d[0])\n",
    "five_model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3), loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'], run_eagerly=True)\n",
    "five_model.summary()\n",
    "\n",
    "ds_train = train_dataset.shuffle(args.batch_size * 10).batch(args.batch_size).prefetch(1)\n",
    "ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "monitor='val_loss'\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "\n",
    "history = five_model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "print('Val_loss:', min(history.history['val_loss']))\n",
    "print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "# 782/782 [==============================] - 2417s 3s/step - loss: 0.4725 - accuracy: 0.7740 - val_loss: 0.4740 - val_accuracy: 0.7734 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa8cba-84fd-4986-9da9-9d5782a8767a",
   "metadata": {},
   "source": [
    "### Problems occurred with implementing it\n",
    "- Using TensorArray and other special TF types to support different sizes of tensors\n",
    "- When trying to train (model.fit()):\n",
    "  AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "    \n",
    "    \n",
    "    Call arguments received by layer 'one_text_preprocessing' (type OneTextPreprocessing):\n",
    "      â€¢ inputs=tf.Tensor(shape=(None,), dtype=string)\n",
    "    on calling numpy() on a Tensor when model.fit().  During just __call__() on the layers and the model, this didn't happen\n",
    "\n",
    "  Solution from https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy: This can also happen in TF2.0 if your code is wrapped in a @tf.function or inside a Keras layer (my case). Both of those run in graph mode. There's a lot of secretly broken code out of there because behavior differs between eager and graph modes and people are not aware that they're switching contexts, so be careful!\n",
    "  The issue seems to be that for certain functions during the fitting model.fit() the @tf.function decorator prohibits the execution of functions like tensor.numpy() for performance reasons.\n",
    "  \n",
    "    The solution for me was to pass the flag run_eagerly=True to the model.compile() like this model.compile(..., run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1434e2-948e-4aef-b5d4-c41ed30653fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.998006  , -0.99800634],\n",
       "       [ 0.9980061 ,  0.99800587]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl = tf.keras.layers.BatchNormalization(axis=1)\n",
    "\n",
    "tl(tf.constant([[1, 3], [2, 4]]), training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe01b8-622f-46ea-b4f6-98ce1eaaa2ba",
   "metadata": {},
   "source": [
    "## 2. Bag of words from scratch in graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd430113-0666-4822-b02a-38d8e37cffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf is eager by default. Need to define tf.function to make it run in graph mode\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dadb695-edae-4a00-a495-af5dff22291b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "six_processed layer: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all'];\n",
      "six_processed model: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all'];\n",
      "six_processed model graph inside tf function: Tensor(\"sequential_5/six_text_preprocessing/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string);\n",
      "six_processed model graph outside of tf function: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all'];\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:01.352673: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:42:01.379798: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:42:01.444325: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class SixTextPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SixTextPreprocessing, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        # print(f\"Type inputs: {type(inputs)}\")\n",
    "        # print(f\"Inputs shape: {inputs.shape}\")\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            lower_string = tf.strings.lower(string_tensor)\n",
    "            # print(lower_string)\n",
    "            processed = tf.strings.regex_replace(lower_string, \"[^\\w\\s]\", \"\", replace_global=True, name=None)\n",
    "            # print(processed)\n",
    "            outputs = outputs.write(i, processed)\n",
    "        stacked_output = outputs.stack()\n",
    "        # print(f\"type(stacked_output): {type(stacked_output)}\")\n",
    "        # print(f\"tf.shape(stacked_output): {tf.shape(stacked_output)}\")\n",
    "        # outputs shape: B, Ragged texts\n",
    "        return stacked_output\n",
    "\n",
    "six_text_preprocessing = SixTextPreprocessing()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    six_processed = six_text_preprocessing(d[0])\n",
    "    print(f\"six_processed layer: {six_processed};\")\n",
    "\n",
    "six_model = keras.models.Sequential()\n",
    "six_model.add(six_text_preprocessing)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    six_processed = six_model(d[0])\n",
    "    print(f\"six_processed model: {six_processed};\")\n",
    "\n",
    "@tf.function\n",
    "def run_six_graph(inp):\n",
    "    six_processed_graph = six_model(inp)\n",
    "    tf.print(f\"six_processed model graph inside tf function: {six_processed_graph};\")\n",
    "    return six_processed_graph\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    six_processed_graph = run_six_graph(d[0])\n",
    "    print(f\"six_processed model graph outside of tf function: {six_processed_graph};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0584615d-2d59-48c5-bd47-fec7d34a3fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:04.057722: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:42:04.140352: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven_processed layer: <tf.RaggedTensor [[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie.', b\"Don't\",\n",
      "  b'be', b'lured', b'in', b'by', b'Christopher', b'Walken', b'or',\n",
      "  b'Michael', b'Ironside.', b'Both', b'are', b'great', b'actors,', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history.', b'Even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b\"movie's\", b'ridiculous', b'storyline.', b'This',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'US', b'propaganda',\n",
      "  b'piece.', b'The', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'Columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions.', b'Maria', b'Conchita', b'Alonso',\n",
      "  b'appeared', b'phony,', b'and', b'her', b'pseudo-love', b'affair',\n",
      "  b'with', b'Walken', b'was', b'nothing', b'but', b'a', b'pathetic',\n",
      "  b'emotional', b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid',\n",
      "  b'of', b'any', b'real', b'meaning.', b'I', b'am', b'disappointed',\n",
      "  b'that', b'there', b'are', b'movies', b'like', b'this,', b'ruining',\n",
      "  b\"actor's\", b'like', b'Christopher', b\"Walken's\", b'good', b'name.', b'I',\n",
      "  b'could', b'barely', b'sit', b'through', b'it.']                          ,\n",
      " [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films,', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including,', b'really', b'tired,',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot.', b'However', b'on',\n",
      "  b'this', b'occasion', b'I', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish.', b'The', b'plot', b'development', b'was',\n",
      "  b'constant.', b'Constantly', b'slow', b'and', b'boring.', b'Things',\n",
      "  b'seemed', b'to', b'happen,', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why.', b'I',\n",
      "  b'admit,', b'I', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film,', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else.', b'I', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all.']                                               ]>;\n",
      "seven_processed model: <tf.RaggedTensor [[b'this', b'was', b'an', b'absolutely', b'terrible', b'movie', b'dont',\n",
      "  b'be', b'lured', b'in', b'by', b'christopher', b'walken', b'or',\n",
      "  b'michael', b'ironside', b'both', b'are', b'great', b'actors', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history', b'even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b'movies', b'ridiculous', b'storyline', b'this',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'us', b'propaganda',\n",
      "  b'piece', b'the', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions', b'maria', b'conchita', b'alonso',\n",
      "  b'appeared', b'phony', b'and', b'her', b'pseudolove', b'affair', b'with',\n",
      "  b'walken', b'was', b'nothing', b'but', b'a', b'pathetic', b'emotional',\n",
      "  b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid', b'of', b'any',\n",
      "  b'real', b'meaning', b'i', b'am', b'disappointed', b'that', b'there',\n",
      "  b'are', b'movies', b'like', b'this', b'ruining', b'actors', b'like',\n",
      "  b'christopher', b'walkens', b'good', b'name', b'i', b'could', b'barely',\n",
      "  b'sit', b'through', b'it']                                                ,\n",
      " [b'i', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including', b'really', b'tired',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot', b'however', b'on',\n",
      "  b'this', b'occasion', b'i', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish', b'the', b'plot', b'development', b'was',\n",
      "  b'constant', b'constantly', b'slow', b'and', b'boring', b'things',\n",
      "  b'seemed', b'to', b'happen', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why', b'i',\n",
      "  b'admit', b'i', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else', b'i', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all']                                                ]>;\n",
      "seven_processed model graph inside tf function: tf.RaggedTensor(values=Tensor(\"sequential_6/seven_tokenizer/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string), row_splits=Tensor(\"sequential_6/seven_tokenizer/RaggedFromRowLengths/control_dependency:0\", shape=(None,), dtype=int32));\n",
      "seven_processed model graph outside of tf function: <tf.RaggedTensor [[b'this', b'was', b'an', b'absolutely', b'terrible', b'movie', b'dont',\n",
      "  b'be', b'lured', b'in', b'by', b'christopher', b'walken', b'or',\n",
      "  b'michael', b'ironside', b'both', b'are', b'great', b'actors', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history', b'even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b'movies', b'ridiculous', b'storyline', b'this',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'us', b'propaganda',\n",
      "  b'piece', b'the', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions', b'maria', b'conchita', b'alonso',\n",
      "  b'appeared', b'phony', b'and', b'her', b'pseudolove', b'affair', b'with',\n",
      "  b'walken', b'was', b'nothing', b'but', b'a', b'pathetic', b'emotional',\n",
      "  b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid', b'of', b'any',\n",
      "  b'real', b'meaning', b'i', b'am', b'disappointed', b'that', b'there',\n",
      "  b'are', b'movies', b'like', b'this', b'ruining', b'actors', b'like',\n",
      "  b'christopher', b'walkens', b'good', b'name', b'i', b'could', b'barely',\n",
      "  b'sit', b'through', b'it']                                                ,\n",
      " [b'i', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including', b'really', b'tired',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot', b'however', b'on',\n",
      "  b'this', b'occasion', b'i', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish', b'the', b'plot', b'development', b'was',\n",
      "  b'constant', b'constantly', b'slow', b'and', b'boring', b'things',\n",
      "  b'seemed', b'to', b'happen', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why', b'i',\n",
      "  b'admit', b'i', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else', b'i', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all']                                                ]>;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:04.298209: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class SevenTokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SevenTokenizer, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        tokens_flat_tensor_array = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        str_len_tensor_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        start_len = 0\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            tokens_ragged_tensor = tf.strings.split(string_tensor, \" \")\n",
    "            for iw in tf.range(tf.size(tokens_ragged_tensor)):\n",
    "                tokens_flat_tensor_array = tokens_flat_tensor_array.write(start_len + iw, tokens_ragged_tensor[iw])\n",
    "            str_len_tensor_array = str_len_tensor_array.write(i, tf.size(tokens_ragged_tensor))\n",
    "            start_len += tf.size(tokens_ragged_tensor)\n",
    "        \n",
    "        ragged_tensor = tf.RaggedTensor.from_row_lengths(\n",
    "                values=tokens_flat_tensor_array.stack(),\n",
    "                row_lengths=str_len_tensor_array.stack())\n",
    "        # print(f\"type(ragged_tensor): {type(ragged_tensor)}\")\n",
    "        # print(f\"tf.shape(ragged_tensor): {tf.shape(ragged_tensor)}\")\n",
    "        # outputs shape: Ragged tensor: B, Tokens\n",
    "        return ragged_tensor\n",
    "        \n",
    "seven_tokenizer = SevenTokenizer()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    seven_processed = seven_tokenizer(d[0])\n",
    "    print(f\"seven_processed layer: {seven_processed};\")\n",
    "\n",
    "seven_model = keras.models.Sequential()\n",
    "seven_model.add(six_text_preprocessing)\n",
    "seven_model.add(seven_tokenizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    seven_processed = seven_model(d[0])\n",
    "    print(f\"seven_processed model: {seven_processed};\")\n",
    "\n",
    "@tf.function\n",
    "def run_seven_graph(inp):\n",
    "    seven_processed_graph = seven_model(inp)\n",
    "    tf.print(f\"seven_processed model graph inside tf function: {seven_processed_graph};\")\n",
    "    return seven_processed_graph\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    seven_processed_graph = run_seven_graph(d[0])\n",
    "    print(f\"seven_processed model graph outside of tf function: {seven_processed_graph};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897fa267-ccaa-43fc-9a99-27aada4adf75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:13.606515: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(250, shape=(), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:15.131237: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight_processed model: [[43.  2.  1.  2.  1.  0.  1.  3.  5.  1.  2.  2.  0.  1.  0.  1.  3.  0.\n",
      "   3.  2.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  2.  0.  0.  1.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  3.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "   0.  0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38.  5.  4.  2.  5.  4.  1.  0.  3.  1.  6.  0.  0.  1.  0.  1.  3.  3.\n",
      "   0.  3.  0.  0.  0.  2.  0.  0.  2.  0.  0.  1.  0.  0.  1.  0.  0.  2.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]];\n",
      "eight_processed model with batch normalization: [[ 0.9999199  -0.9997778  -0.9997779   0.         -0.99987507 -0.99987507\n",
      "   0.          0.99977785  0.9995005   0.         -0.99987507  0.99950033\n",
      "   0.          0.          0.          0.          0.         -0.99977785\n",
      "   0.99977785 -0.99800587  0.          0.          0.99950033 -0.99950033\n",
      "   0.          0.99950033 -0.99950033  0.998006    0.         -0.998006\n",
      "   0.99950033  0.         -0.998006    0.998006    0.         -0.99950033\n",
      "   0.          0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.99950033  0.          0.          0.         -0.998006\n",
      "   0.998006   -0.998006    0.          0.998006    0.998006    0.\n",
      "   0.          0.99977785  0.          0.          0.         -0.998006\n",
      "   0.          0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.99950033  0.\n",
      "   0.          0.          0.998006    0.          0.          0.99950033\n",
      "   0.99950033  0.          0.          0.          0.          0.998006\n",
      "   0.          0.         -0.998006    0.         -0.998006    0.\n",
      "   0.99950033  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.998006   -0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.99950033  0.         -0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.          0.         -0.998006   -0.998006    0.\n",
      "   0.          0.998006    0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.998006    0.998006\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.9999199   0.99977803  0.9997778   0.          0.99987507  0.99987507\n",
      "   0.         -0.99977785 -0.9995003   0.          0.99987507 -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.99977785\n",
      "  -0.99977785  0.99800634  0.          0.         -0.99950033  0.99950033\n",
      "   0.         -0.99950033  0.99950033 -0.998006    0.          0.998006\n",
      "  -0.99950033  0.          0.998006   -0.998006    0.          0.99950033\n",
      "   0.         -0.998006    0.998006    0.          0.          0.\n",
      "   0.         -0.99950033  0.          0.          0.          0.998006\n",
      "  -0.998006    0.998006    0.         -0.998006   -0.998006    0.\n",
      "   0.         -0.99977785  0.          0.          0.          0.998006\n",
      "   0.         -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.99950033  0.\n",
      "   0.          0.         -0.998006    0.          0.         -0.99950033\n",
      "  -0.99950033  0.          0.          0.          0.         -0.998006\n",
      "   0.          0.          0.998006    0.          0.998006    0.\n",
      "  -0.99950033  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.998006    0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.99950033  0.          0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006    0.          0.          0.998006    0.998006    0.\n",
      "   0.         -0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.998006   -0.998006\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.998006    0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]];\n",
      "eight_processed model with layer normalization: [[15.267825    0.55133015  0.19239126  0.55133015  0.19239126 -0.16654766\n",
      "   0.19239126  0.9102691   1.6281468   0.19239126  0.55133015  0.55133015\n",
      "  -0.16654766  0.19239126 -0.16654766  0.19239126  0.9102691  -0.16654766\n",
      "   0.9102691   0.55133015 -0.16654766 -0.16654766  0.55133015 -0.16654766\n",
      "  -0.16654766  0.55133015 -0.16654766  0.19239126 -0.16654766 -0.16654766\n",
      "   0.55133015 -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126  0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766  0.19239126  0.19239126 -0.16654766\n",
      "  -0.16654766  0.9102691  -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.55133015 -0.16654766\n",
      "  -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766  0.55133015\n",
      "   0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766  0.55133015 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126  0.19239126 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766]\n",
      " [14.861998    1.8015503   1.4057791   0.61423683  1.8015503   1.4057791\n",
      "   0.21846567 -0.17730547  1.010008    0.21846567  2.1973214  -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547  0.21846567  1.010008    1.010008\n",
      "  -0.17730547  1.010008   -0.17730547 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547  0.61423683 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567  0.21846567  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547  0.21846567  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567  0.21846567\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547]];\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:15.401624: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:42:15.499074: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:42:15.588966: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight_processed model graph inside tf function: Tensor(\"sequential_8/eight_vectorizer/TensorArrayV2Stack/TensorListStack:0\", shape=(None, 250), dtype=float32);\n",
      "eight_processed model graph outside of tf function: [[43.  2.  1.  2.  1.  0.  1.  3.  5.  1.  2.  2.  0.  1.  0.  1.  3.  0.\n",
      "   3.  2.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  2.  0.  0.  1.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  3.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "   0.  0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38.  5.  4.  2.  5.  4.  1.  0.  3.  1.  6.  0.  0.  1.  0.  1.  3.  3.\n",
      "   0.  3.  0.  0.  0.  2.  0.  0.  2.  0.  0.  1.  0.  0.  1.  0.  0.  2.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]];\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import functools\n",
    "from tensorflow.lookup import KeyValueTensorInitializer, StaticHashTable\n",
    "\n",
    "class EightVectorizer(tf.keras.layers.Layer):\n",
    "    # +-1 cause we need space for unknown tokens with 0 index\n",
    "    def __init__(self, token_to_int: StaticHashTable):\n",
    "        super(EightVectorizer, self).__init__()\n",
    "        self._token_to_int = token_to_int\n",
    "        self._vocab_size = tf.cast(self._token_to_int.size() + 1, tf.int32)\n",
    "    \n",
    "    def call(self, inputs):  \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            tokens = inputs[i]\n",
    "            array_string = tf.TensorArray(dtype=tf.float32, size=self._vocab_size)\n",
    "            # array_string = tf.TensorArray(dtype=tf.float32, size=250)\n",
    "            array_string.unstack(tf.zeros(self._vocab_size))\n",
    "            # array_string.unstack(tf.zeros(250))\n",
    "            # for itoken in tf.range(tf.size(\n",
    "            indexes = self._token_to_int.lookup(tokens)\n",
    "            for idx in indexes:\n",
    "                array_string = array_string.write(idx, array_string.read(idx) + 1.0)\n",
    "            outputs = outputs.write(i, array_string.stack())\n",
    "        return outputs.stack()\n",
    "\n",
    "    @classmethod\n",
    "    @functools.lru_cache(maxsize=10)\n",
    "    def from_train_data(cls, train_ds, vocab_size=args.small_vocab_size, batch_size=args.batch_size, take=5):\n",
    "        _six_text_preprocessing = SixTextPreprocessing()\n",
    "        _seven_tokenizer = SevenTokenizer()\n",
    "        _preproc_mdl = keras.models.Sequential()\n",
    "        _preproc_mdl.add(_six_text_preprocessing)\n",
    "        _preproc_mdl.add(_seven_tokenizer)\n",
    "\n",
    "        _counter = Counter()\n",
    "        for d in train_ds.batch(batch_size).take(take):\n",
    "            _processed = _preproc_mdl(d[0])\n",
    "            for ragged_tensor in _processed:\n",
    "                _counter.update(ragged_tensor.numpy().tolist())\n",
    "        keys_tensor = tf.constant([t for t, _ in _counter.most_common(vocab_size - 1)])\n",
    "        # print(f\"keys_tensor: {keys_tensor}\")\n",
    "        vals_tensor = tf.constant([i for i in range(1, vocab_size)])\n",
    "        # print(f\"vals_tensor: {vals_tensor}\")\n",
    "        _token_table = StaticHashTable(KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=0)\n",
    "        return cls(_token_table)\n",
    "\n",
    "eight_vectorizer = EightVectorizer.from_train_data(train_dataset)\n",
    "print(eight_vectorizer._vocab_size)\n",
    "\n",
    "eight_model = keras.models.Sequential()\n",
    "eight_model.add(six_text_preprocessing)\n",
    "eight_model.add(seven_tokenizer)\n",
    "eight_model.add(eight_vectorizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed = eight_model(d[0])\n",
    "    print(f\"eight_processed model: {eight_processed};\")\n",
    "\n",
    "eight_model.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed = eight_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"eight_processed model with batch normalization: {eight_processed};\")\n",
    "\n",
    "eight_model.pop()\n",
    "eight_model.add(tf.keras.layers.LayerNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed = eight_model(d[0])\n",
    "    print(f\"eight_processed model with layer normalization: {eight_processed};\")\n",
    "\n",
    "eight_model.pop()\n",
    "@tf.function\n",
    "def run_eight_graph(inp):\n",
    "    eight_processed_graph = eight_model(inp)\n",
    "    tf.print(f\"eight_processed model graph inside tf function: {eight_processed_graph};\")\n",
    "    return eight_processed_graph\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed_graph = run_eight_graph(d[0])\n",
    "    print(f\"eight_processed model graph outside of tf function: {eight_processed_graph};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "400f49ca-5ef6-43a8-b256-878afae3cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four_processed model with batch normalization: [[0.6115303 ]\n",
      " [0.70190346]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:24.005226: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "nine_model = keras.models.Sequential()\n",
    "nine_model.add(six_text_preprocessing)\n",
    "nine_model.add(seven_tokenizer)\n",
    "nine_model.add(eight_vectorizer)\n",
    "nine_model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "nine_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "nine_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    nine_processed = nine_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"four_processed model with batch normalization: {nine_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a2ebf69-59a5-4310-9728-5cdaf7aca5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:42:31.718138: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-13 18:42:33,942 : WARNING : At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "2023-12-13 18:42:33,944 : WARNING : There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " six_text_preprocessing (Si  (None,)                   0         \n",
      " xTextPreprocessing)                                             \n",
      "                                                                 \n",
      " seven_tokenizer (SevenToke  (None, None)              0         \n",
      " nizer)                                                          \n",
      "                                                                 \n",
      " eight_vectorizer (EightVec  (None, 250)               0         \n",
      " torizer)                                                        \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 250)               1000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                16064     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17129 (66.91 KB)\n",
      "Trainable params: 16629 (64.96 KB)\n",
      "Non-trainable params: 500 (1.95 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "782/782 [==============================] - 105s 133ms/step - loss: 0.5283 - accuracy: 0.7380 - val_loss: 0.4894 - val_accuracy: 0.7634 - lr: 0.0010\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:44:18.523169: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 105s 134ms/step - loss: 0.4777 - accuracy: 0.7696 - val_loss: 0.4821 - val_accuracy: 0.7682 - lr: 0.0010\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:46:03.325311: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 106s 135ms/step - loss: 0.4559 - accuracy: 0.7831 - val_loss: 0.4746 - val_accuracy: 0.7727 - lr: 0.0010\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:47:48.835855: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 105s 134ms/step - loss: 0.4433 - accuracy: 0.7902 - val_loss: 0.4764 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "  1/782 [..............................] - ETA: 1:02 - loss: 0.3945 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:49:33.887136: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/782 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8042\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "782/782 [==============================] - 104s 134ms/step - loss: 0.4235 - accuracy: 0.8042 - val_loss: 0.4792 - val_accuracy: 0.7713 - lr: 0.0010\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:51:18.378543: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/782 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8258Restoring model weights from the end of the best epoch: 3.\n",
      "782/782 [==============================] - 105s 134ms/step - loss: 0.3895 - accuracy: 0.8258 - val_loss: 0.4748 - val_accuracy: 0.7764 - lr: 1.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Val_accuracy: 0.7764000296592712\n",
      "Val_loss: 0.4746430218219757\n",
      "Accuracy: 0.8258399963378906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:53:03.208903: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "ten_model = keras.models.Sequential()\n",
    "ten_model.add(six_text_preprocessing)\n",
    "ten_model.add(seven_tokenizer)\n",
    "ten_model.add(eight_vectorizer)\n",
    "ten_model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "ten_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "ten_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# instead of model.build() we can just call the model on some data\n",
    "for d in train_dataset.batch(args.batch_size).take(1):\n",
    "    ten_model(d[0])\n",
    "ten_model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3), loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'], run_eagerly=False)\n",
    "ten_model.summary()\n",
    "\n",
    "ds_train = train_dataset.shuffle(args.batch_size * 10).batch(args.batch_size).prefetch(1)\n",
    "ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "monitor='val_loss'\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "\n",
    "history = ten_model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "print('Val_loss:', min(history.history['val_loss']))\n",
    "print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "# 82/782 [==============================] - 105s 134ms/step - loss: 0.3895 - accuracy: 0.8258 - val_loss: 0.4748 - val_accuracy: 0.7764 - lr: 1.0000e-04\n",
    "# Epoch 6: early stopping\n",
    "# Val_accuracy: 0.7764000296592712\n",
    "# Val_loss: 0.4746430218219757\n",
    "# Accuracy: 0.8258399963378906"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1d4d2-07e2-40bc-bd5b-b628c47401ed",
   "metadata": {},
   "source": [
    "## Problems faced when implementing the  in graph mode\n",
    "1. Can use only tf methods and functions, no numpy or python func. If you lack some function in tf, it might be difficult to implement something.\n",
    "2. To run it graph mode, you need to wrap a model in a tf fucntion and run a tf.function. Can't just enable graph mode to be usef everywhere.\n",
    "3. Couldn't make it print a tensor value inside tf.function neither with print() nor with tf.print() it returns something like Tensor(\"sequential_4/six_text_preprocessing_4/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string) instead of a real value\n",
    "4. AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "5. When self._vocab_size = self._token_to_int.size() + 1 (Tensor int64) got an error AssertionError: Unreachable\n",
    "    \n",
    "    \n",
    "    Call arguments received by layer 'eight_vectorizer_16' (type EightVectorizer):\n",
    "      â€¢ inputs=tf.RaggedTensor(values=Tensor(\"sequential_39/seven_tokenizer_2/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string), row_splits=Tensor(\"sequential_39/seven_tokenizer_2/RaggedFromRowLengths/control_dependency:0\", shape=(None,), dtype=int32))\n",
    "   needed to cast:\n",
    "   self._vocab_size = tf.cast(self._token_to_int.size() + 1, tf.int32)Â \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbcdc8-0c5a-4a73-8143-05a27debf77e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Even if you develop a model for the eager mode, you need to use tf ds and functions (e.g. TensorArray) in order to make it work. \n",
    "\n",
    "In graph mode, you can use only tf code and wrap it with @tf.function, but the graph model (ten_model) is trained approx 20 times faster than the one that uses python code (five_model) in eager mode."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
