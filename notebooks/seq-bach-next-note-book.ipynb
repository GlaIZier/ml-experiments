{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7e0ad426-43d4-460d-8106-948121374cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d after flat_map: [[66 61 57 54]\n",
      " [66 61 57 54]\n",
      " [68 61 59 54]\n",
      " [68 61 59 54]\n",
      " [69 66 61 54]\n",
      " [69 66 61 56]\n",
      " [69 66 61 57]\n",
      " [69 66 61 59]\n",
      " [68 65 61 61]\n",
      " [68 65 61 61]\n",
      " [68 65 59 49]\n",
      " [68 65 59 49]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 59 50]\n",
      " [66 66 59 50]\n",
      " [68 66 59 50]\n",
      " [68 66 59 50]\n",
      " [69 66 61 49]\n",
      " [69 66 61 49]\n",
      " [69 66 61 47]\n",
      " [69 66 61 47]\n",
      " [68 65 61 49]\n",
      " [68 65 61 49]\n",
      " [68 65 59 49]\n",
      " [68 65 59 49]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [73 66 57 54]]\n",
      "d after preprocess: [31 26 22 19 31 26 22 19 33 26 24 19 33 26 24 19 34 31 26 19 34 31 26 21\n",
      " 34 31 26 22 34 31 26 24 33 30 26 26 33 30 26 26 33 30 24 14 33 30 24 14\n",
      " 31 31 22 15 31 31 22 15 31 31 22 15 31 31 22 15 31 31 24 15 31 31 24 15\n",
      " 33 31 24 15 33 31 24 15 34 31 26 14 34 31 26 14 34 31 26 12 34 31 26 12\n",
      " 33 30 26 14 33 30 26 14 33 30 24 14 33 30 24 14 31 26 22  7 31 26 22  7\n",
      " 31 26 22  7 31 26 22  7 38 31 22 19]\n",
      "Tensor(\"args_0:0\", shape=(None, None), dtype=int32)\n",
      "final dataset: (<tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n",
      "array([[31, 26, 22, ..., 38, 31, 22],\n",
      "       [31, 31, 24, ..., 33, 30, 26],\n",
      "       [38, 31, 22, ..., 36, 33, 29],\n",
      "       ...,\n",
      "       [29, 26, 22, ..., 27, 22, 18],\n",
      "       [32, 26, 23, ..., 30, 27, 22],\n",
      "       [27, 22, 18, ..., 27, 22, 19]], dtype=int32)>, <tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n",
      "array([[26, 22, 19, ..., 31, 22, 19],\n",
      "       [31, 24, 15, ..., 30, 26, 14],\n",
      "       [31, 22, 19, ..., 33, 29, 17],\n",
      "       ...,\n",
      "       [26, 22, 10, ..., 22, 18, 15],\n",
      "       [26, 23, 17, ..., 27, 22,  6],\n",
      "       [22, 18, 15, ..., 22, 19, 15]], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:49:40.654408: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d after flat_map: [[66 61 57 54]\n",
      " [66 61 57 54]\n",
      " [68 61 59 54]\n",
      " [68 61 59 54]\n",
      " [69 66 61 54]\n",
      " [69 66 61 56]\n",
      " [69 66 61 57]\n",
      " [69 66 61 59]\n",
      " [68 65 61 61]\n",
      " [68 65 61 61]\n",
      " [68 65 59 49]\n",
      " [68 65 59 49]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 59 50]\n",
      " [66 66 59 50]\n",
      " [68 66 59 50]\n",
      " [68 66 59 50]\n",
      " [69 66 61 49]\n",
      " [69 66 61 49]\n",
      " [69 66 61 47]\n",
      " [69 66 61 47]\n",
      " [68 65 61 49]\n",
      " [68 65 61 49]\n",
      " [68 65 59 49]\n",
      " [68 65 59 49]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [73 66 57 54]]\n",
      "d after preprocess: [31 26 22 19 31 26 22 19 33 26 24 19 33 26 24 19 34 31 26 19 34 31 26 21\n",
      " 34 31 26 22 34 31 26 24 33 30 26 26 33 30 26 26 33 30 24 14 33 30 24 14\n",
      " 31 31 22 15 31 31 22 15 31 31 22 15 31 31 22 15 31 31 24 15 31 31 24 15\n",
      " 33 31 24 15 33 31 24 15 34 31 26 14 34 31 26 14 34 31 26 12 34 31 26 12\n",
      " 33 30 26 14 33 30 26 14 33 30 24 14 33 30 24 14 31 26 22  7 31 26 22  7\n",
      " 31 26 22  7 31 26 22  7 38 31 22 19]\n",
      "Tensor(\"args_0:0\", shape=(None, None), dtype=int32)\n",
      "final dataset: (<tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n",
      "array([[31, 26, 22, ..., 38, 31, 22],\n",
      "       [31, 31, 24, ..., 33, 30, 26],\n",
      "       [38, 31, 22, ..., 36, 33, 29],\n",
      "       ...,\n",
      "       [29, 26, 22, ..., 27, 22, 18],\n",
      "       [32, 26, 23, ..., 30, 27, 22],\n",
      "       [27, 22, 18, ..., 27, 22, 19]], dtype=int32)>, <tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n",
      "array([[26, 22, 19, ..., 31, 22, 19],\n",
      "       [31, 24, 15, ..., 30, 26, 14],\n",
      "       [31, 22, 19, ..., 33, 29, 17],\n",
      "       ...,\n",
      "       [26, 22, 10, ..., 22, 18, 15],\n",
      "       [26, 23, 17, ..., 27, 22,  6],\n",
      "       [22, 18, 15, ..., 22, 19, 15]], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:49:41.837426: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d after flat_map: [[66 61 57 54]\n",
      " [66 61 57 54]\n",
      " [68 61 59 54]\n",
      " [68 61 59 54]\n",
      " [69 66 61 54]\n",
      " [69 66 61 56]\n",
      " [69 66 61 57]\n",
      " [69 66 61 59]\n",
      " [68 65 61 61]\n",
      " [68 65 61 61]\n",
      " [68 65 59 49]\n",
      " [68 65 59 49]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 57 50]\n",
      " [66 66 59 50]\n",
      " [66 66 59 50]\n",
      " [68 66 59 50]\n",
      " [68 66 59 50]\n",
      " [69 66 61 49]\n",
      " [69 66 61 49]\n",
      " [69 66 61 47]\n",
      " [69 66 61 47]\n",
      " [68 65 61 49]\n",
      " [68 65 61 49]\n",
      " [68 65 59 49]\n",
      " [68 65 59 49]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [66 61 57 42]\n",
      " [73 66 57 54]]\n",
      "d after preprocess: [31 26 22 19 31 26 22 19 33 26 24 19 33 26 24 19 34 31 26 19 34 31 26 21\n",
      " 34 31 26 22 34 31 26 24 33 30 26 26 33 30 26 26 33 30 24 14 33 30 24 14\n",
      " 31 31 22 15 31 31 22 15 31 31 22 15 31 31 22 15 31 31 24 15 31 31 24 15\n",
      " 33 31 24 15 33 31 24 15 34 31 26 14 34 31 26 14 34 31 26 12 34 31 26 12\n",
      " 33 30 26 14 33 30 26 14 33 30 24 14 33 30 24 14 31 26 22  7 31 26 22  7\n",
      " 31 26 22  7 31 26 22  7 38 31 22 19]\n",
      "Tensor(\"args_0:0\", shape=(None, None), dtype=int32)\n",
      "final dataset: (<tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n",
      "array([[31, 26, 22, ..., 38, 31, 22],\n",
      "       [31, 31, 24, ..., 33, 30, 26],\n",
      "       [38, 31, 22, ..., 36, 33, 29],\n",
      "       ...,\n",
      "       [29, 26, 22, ..., 27, 22, 18],\n",
      "       [32, 26, 23, ..., 30, 27, 22],\n",
      "       [27, 22, 18, ..., 27, 22, 19]], dtype=int32)>, <tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n",
      "array([[26, 22, 19, ..., 31, 22, 19],\n",
      "       [31, 24, 15, ..., 30, 26, 14],\n",
      "       [31, 22, 19, ..., 33, 29, 17],\n",
      "       ...,\n",
      "       [26, 22, 10, ..., 22, 18, 15],\n",
      "       [26, 23, 17, ..., 27, 22,  6],\n",
      "       [22, 18, 15, ..., 22, 19, 15]], dtype=int32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:49:43.022477: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data/bach-next-note/'\n",
    "args.train_folder = args.data_folder + 'train/'\n",
    "args.val_folder = args.data_folder + 'valid/'\n",
    "args.test_folder = args.data_folder + 'test/'\n",
    "# args.train_fraction = 0.8\n",
    "args.seed = 101\n",
    "args.batch_size = 32\n",
    "args.epochs = 7\n",
    "\n",
    "paths = Path(args.train_folder).glob('**/chorale_*.csv')\n",
    "train_np_list = [pd.read_csv(p).values.tolist() for p in paths]\n",
    "# print(len(train_np_list[0]))\n",
    "# print(train_np_list[0])\n",
    "\n",
    "paths = Path(args.val_folder).glob('**/chorale_*.csv')\n",
    "val_np_list = [pd.read_csv(p).values.tolist() for p in paths]\n",
    "\n",
    "paths = Path(args.test_folder).glob('**/chorale_*.csv')\n",
    "test_np_list = [pd.read_csv(p).values.tolist() for p in paths]\n",
    "\n",
    "def calc_const():\n",
    "    notes = set()\n",
    "    for chorales in (train_np_list, val_np_list, test_np_list):\n",
    "        for chorale in chorales:\n",
    "            for chord in chorale:\n",
    "                notes |= set(chord)\n",
    "\n",
    "    n_notes = len(notes)\n",
    "    min_note = min(notes - {0})\n",
    "    max_note = max(notes)\n",
    "    return n_notes, min_note, max_note\n",
    "\n",
    "n_notes, min_note, max_note = calc_const()\n",
    "\n",
    "\n",
    "def create_target(batch):\n",
    "    print(batch)\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        dataset = dataset.flat_map(batch_window)\n",
    "        return dataset\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "#     print(\"chorale[0] {}\".format(chorales[0]))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows)\n",
    "    for d in dataset:\n",
    "        tf.print(f\"d after flat_map: {d}\")\n",
    "        break\n",
    "    dataset = dataset.map(preprocess)\n",
    "    for d in dataset:\n",
    "        tf.print(f\"d after preprocess: {d}\")\n",
    "        break\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(create_target)\n",
    "    for d in dataset:\n",
    "        print(f\"final dataset: {d}\")\n",
    "        break\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "train_chorales = bach_dataset(np_list)\n",
    "val_chorales = bach_dataset(np_list)\n",
    "test_chorales = bach_dataset(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445ad7e-5e06-4e8c-ae7f-4ac4999875c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 21:53:54,351 : INFO : None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 5)           235       \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 32)          352       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, None, 48)          3120      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, None, 48)          192       \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, None, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, None, 96)          12384     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, None, 96)          384       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 256)         361472    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 47)          12079     \n",
      "=================================================================\n",
      "Total params: 396,810\n",
      "Trainable params: 396,330\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - 20s 166ms/step - loss: 2.0962 - accuracy: 0.4691 - val_loss: 3.8063 - val_accuracy: 0.0418\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 15s 157ms/step - loss: 1.0533 - accuracy: 0.7389 - val_loss: 3.9637 - val_accuracy: 0.0670\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 15s 157ms/step - loss: 0.8631 - accuracy: 0.7742 - val_loss: 3.3148 - val_accuracy: 0.0761\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 17s 171ms/step - loss: 0.7765 - accuracy: 0.7890 - val_loss: 2.3086 - val_accuracy: 0.2796\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 16s 160ms/step - loss: 0.7199 - accuracy: 0.7995 - val_loss: 1.3404 - val_accuracy: 0.6025\n",
      "Epoch 6/20\n",
      "91/98 [==========================>...] - ETA: 0s - loss: 0.6749 - accuracy: 0.8084"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    n_embedding_dims = 5\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(input_dim=n_notes, output_dim=n_embedding_dims,\n",
    "                               input_shape=[None]),\n",
    "        keras.layers.Conv1D(32, kernel_size=2, padding=\"causal\", activation=\"relu\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv1D(48, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=2),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv1D(64, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=4),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv1D(96, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=8),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.LSTM(256, return_sequences=True),\n",
    "        keras.layers.Dense(n_notes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    log.info(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_model(model):  \n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=1e-3)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    model.fit(train_chorales, epochs=20, validation_data=val_chorales)\n",
    "model = build_model()\n",
    "train_model(model)\n",
    "model.save(\"model-ignored//my_bach_model.h5\")\n",
    "model.evaluate(test_chorales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a9f370-d005-490b-b0ab-312d2cd25e29",
   "metadata": {},
   "source": [
    "### Tests\n",
    "#### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77bbed0c-efbf-46c1-ba86-e0cdf68520de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[1, 2], [3, 2], [4, 5]], [[1, 2]]]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ragged.constant([[1, 2], [3], [4, 5, 6]], ragged_rank=1)\n",
    "tf.ragged.constant([[[1, 2], [3, 2], [4, 5]], [[1, 2]]], ragged_rank=2)\n",
    "# tf.ragged.constant([[[1], [2]], [[3], [4]], [[5], [6]]], ragged_rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55fef7c8-959b-4624-b174-13d369ce7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "[0, 1, 2]\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "[2, 3, 4]\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "[4, 5, 6]\n",
      "<_VariantDataset shapes: (), types: tf.int64>\n",
      "[6]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3, 2)\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    print([item.numpy() for item in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1b60fa0-ba6d-4734-9a79-80b941237b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_VariantDataset shapes: (2,), types: tf.int32>\n",
      "[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
      "<_VariantDataset shapes: (2,), types: tf.int32>\n",
      "[array([3, 4], dtype=int32), array([5, 6], dtype=int32)]\n",
      "<_VariantDataset shapes: (2,), types: tf.int32>\n",
      "[array([5, 6], dtype=int32), array([7, 8], dtype=int32)]\n",
      "flat_map\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
      "tf.Tensor(\n",
      "[[3 4]\n",
      " [5 6]], shape=(2, 2), dtype=int32)\n",
      "[array([3, 4], dtype=int32), array([5, 6], dtype=int32)]\n",
      "tf.Tensor(\n",
      "[[5 6]\n",
      " [7 8]], shape=(2, 2), dtype=int32)\n",
      "[array([5, 6], dtype=int32), array([7, 8], dtype=int32)]\n",
      "map1\n",
      "[[1 1]\n",
      " [2 3]]\n",
      "tf.Tensor(\n",
      "[[1 1]\n",
      " [2 3]], shape=(2, 2), dtype=int32)\n",
      "[array([1, 1], dtype=int32), array([2, 3], dtype=int32)]\n",
      "[[2 3]\n",
      " [4 5]]\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n",
      "[array([2, 3], dtype=int32), array([4, 5], dtype=int32)]\n",
      "[[4 5]\n",
      " [6 7]]\n",
      "tf.Tensor(\n",
      "[[4 5]\n",
      " [6 7]], shape=(2, 2), dtype=int32)\n",
      "[array([4, 5], dtype=int32), array([6, 7], dtype=int32)]\n",
      "map2\n",
      "[[1 1]\n",
      " [2 3]]\n",
      "[1 1 2 3]\n",
      "tf.Tensor([1 1 2 3], shape=(4,), dtype=int32)\n",
      "[1, 1, 2, 3]\n",
      "[[2 3]\n",
      " [4 5]]\n",
      "[2 3 4 5]\n",
      "tf.Tensor([2 3 4 5], shape=(4,), dtype=int32)\n",
      "[2, 3, 4, 5]\n",
      "[[4 5]\n",
      " [6 7]]\n",
      "[4 5 6 7]\n",
      "tf.Tensor([4 5 6 7], shape=(4,), dtype=int32)\n",
      "[4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4], [5, 6], [7, 8]]).window(2, 1, drop_remainder=True)\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    print([item.numpy() for item in d])\n",
    "    \n",
    "def b(window):\n",
    "    return window.batch(3)\n",
    "\n",
    "dataset = dataset.flat_map(b)\n",
    "print(\"flat_map\")\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    print([item.numpy() for item in d])\n",
    "\n",
    "def b1(window):\n",
    "    window = tf.where(window == 1, window, window - 1) # shift values\n",
    "    tf.print(window)\n",
    "    return window\n",
    "\n",
    "print(\"map1\")\n",
    "dataset = dataset.map(b1)\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    print([item.numpy() for item in d])\n",
    "    \n",
    "def b2(window):\n",
    "    window = tf.reshape(window, [-1])\n",
    "    tf.print(window)\n",
    "    return window\n",
    "\n",
    "print(\"map2\")\n",
    "dataset = dataset.map(b2)\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    print([item.numpy() for item in d])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
