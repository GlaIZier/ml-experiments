{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767687b5-10fd-435a-97e1-82cb0d9f97ed",
   "metadata": {},
   "source": [
    "8. Reinf learning. Exercise: Use policy gradients to solve OpenAI Gym's LunarLander-v2 environment. You will need to install the Box2D dependencies (%pip install -U gym[box2d]). hands-on-ml book exerices page 623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbee982d-cf3a-4dfb-bddf-4fbc605513f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00422506,  1.4077823 ,  0.4279313 , -0.13946356, -0.00488894,\n",
       "       -0.09693269,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0320204f-372a-4e58-8271-4fce3bdb0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(-inf, inf, (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# img = env.render(mode=\"rgb_array\")\n",
    "# env.close()\n",
    "# plt.imshow(img)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4543afbd-bb27-4e8a-a317-f8ced879c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-160.12920791414706 99.56875747347738 -478.625099914632 113.81167088199918\n"
     ]
    }
   ],
   "source": [
    "# random policy\n",
    "# print(np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards))\n",
    "# -179.8682271598708 113.57826569586982 -536.6091716341286 11.068357820341546\n",
    "\n",
    "total_rewards = []\n",
    "for i_episode in range(200):\n",
    "    rewards = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(200):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(rewards)\n",
    "print(np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f3ab7d47-4308-492f-a705-2403c6f1ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00437126  1.4179332  -0.4427755   0.3116899   0.00507198  0.1002952\n",
      "   0.          0.        ]]\n",
      "4\n",
      "[<tf.Variable 'dense_36/kernel:0' shape=(8, 16) dtype=float32, numpy=\n",
      "array([[ 0.301795  , -0.38039255, -0.24801195,  0.04388976, -0.34565997,\n",
      "         0.1735723 , -0.47314346,  0.41481686,  0.05990446,  0.10238111,\n",
      "        -0.48438263,  0.2602874 , -0.09898937, -0.31536007, -0.1656872 ,\n",
      "         0.27430248],\n",
      "       [ 0.41590798,  0.32985508, -0.39172673,  0.02755785, -0.49637842,\n",
      "         0.4681188 ,  0.08902979,  0.34461904, -0.21435642, -0.07684267,\n",
      "        -0.4240328 , -0.07903743,  0.430732  ,  0.45916307,  0.38133073,\n",
      "         0.41708183],\n",
      "       [-0.18825614, -0.12405837, -0.45531416,  0.17599726,  0.06348991,\n",
      "        -0.03635478, -0.43160188, -0.15807903,  0.12305021,  0.3191123 ,\n",
      "        -0.40355122,  0.25191665, -0.07294977,  0.37522745, -0.16398835,\n",
      "        -0.28805494],\n",
      "       [-0.15455902, -0.3789966 , -0.35286403, -0.22206593, -0.36340916,\n",
      "        -0.09669185, -0.34490955, -0.43130815, -0.39260924,  0.33719277,\n",
      "        -0.10975492,  0.11328268, -0.27289534, -0.32261896,  0.2613356 ,\n",
      "        -0.06836903],\n",
      "       [ 0.05163264, -0.469424  , -0.20727289,  0.05763137, -0.09989309,\n",
      "         0.49782658, -0.3448981 , -0.47811234,  0.42369926,  0.31580877,\n",
      "         0.01945722, -0.1757822 , -0.14378345, -0.42951858, -0.01227546,\n",
      "         0.24962354],\n",
      "       [ 0.15717196,  0.17011762,  0.40369117, -0.3867073 , -0.36679578,\n",
      "        -0.23429203,  0.380769  , -0.03551114, -0.07210577,  0.08599997,\n",
      "        -0.23912013,  0.16166079,  0.20891774, -0.22563183,  0.49337065,\n",
      "        -0.12029803],\n",
      "       [-0.05371225,  0.18573952,  0.1870476 , -0.41057515, -0.03828228,\n",
      "         0.25838137, -0.19734848, -0.40277338,  0.1096431 , -0.4492004 ,\n",
      "        -0.00905788, -0.22053432,  0.26017225, -0.11380327, -0.14217758,\n",
      "         0.09240806],\n",
      "       [ 0.19246817,  0.47654247, -0.10125399, -0.14576983,  0.38624012,\n",
      "         0.04672801,  0.45464063, -0.43811572, -0.19108427, -0.18490994,\n",
      "        -0.0657686 , -0.16675079, -0.49005795, -0.07646704, -0.24863863,\n",
      "        -0.08828759]], dtype=float32)>, <tf.Variable 'dense_36/bias:0' shape=(16,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'dense_37/kernel:0' shape=(16, 8) dtype=float32, numpy=\n",
      "array([[ 0.34896028, -0.22524846,  0.26894188,  0.28185105, -0.00206316,\n",
      "         0.10937083,  0.09824002, -0.410676  ],\n",
      "       [ 0.24582648, -0.34190142, -0.3949746 ,  0.34702444, -0.11835659,\n",
      "        -0.30399442, -0.16041481,  0.2827301 ],\n",
      "       [-0.30111885, -0.08902907, -0.24769461,  0.21873212,  0.4243573 ,\n",
      "         0.04235554,  0.03485298,  0.10524714],\n",
      "       [-0.11229014, -0.36745214,  0.21034336, -0.24161792, -0.0356015 ,\n",
      "         0.45566905, -0.2805003 , -0.25286758],\n",
      "       [ 0.39117098, -0.20112944, -0.3844601 ,  0.01196444, -0.150285  ,\n",
      "         0.23336267, -0.25040913,  0.49237728],\n",
      "       [-0.38553154, -0.48790598, -0.44366372,  0.14355087,  0.2839296 ,\n",
      "         0.00326371, -0.24616122,  0.07395172],\n",
      "       [ 0.0064342 , -0.43778193, -0.09732926, -0.48437572,  0.17312992,\n",
      "         0.20476758, -0.13063657,  0.09412313],\n",
      "       [-0.29027689, -0.07449687,  0.25453115,  0.07566333, -0.48618996,\n",
      "        -0.19465446,  0.44709575, -0.4132383 ],\n",
      "       [ 0.37524056, -0.2889986 ,  0.20881414, -0.23446381,  0.02217829,\n",
      "        -0.24340224,  0.28025007,  0.18990505],\n",
      "       [-0.46884727,  0.17072034,  0.01316559,  0.30046153, -0.42457795,\n",
      "         0.1723448 , -0.19221056, -0.29808033],\n",
      "       [ 0.06029665,  0.07524335,  0.22527122, -0.10465133, -0.36974967,\n",
      "         0.20648861, -0.37075996,  0.31856334],\n",
      "       [-0.0848968 , -0.3637632 , -0.3102169 , -0.40517664,  0.37628508,\n",
      "         0.008008  , -0.18433261, -0.22445238],\n",
      "       [ 0.16352904,  0.28486502, -0.30604553, -0.08632088,  0.23793507,\n",
      "         0.46226633,  0.28163922, -0.38245654],\n",
      "       [-0.10376155,  0.2998414 , -0.1874969 , -0.09437847, -0.07504153,\n",
      "        -0.22145712, -0.27541924, -0.3290255 ],\n",
      "       [-0.37976992,  0.1800797 , -0.0259918 ,  0.41090953,  0.3672892 ,\n",
      "        -0.12675107, -0.40845442,  0.4587779 ],\n",
      "       [ 0.21760166,  0.08795929,  0.16824937,  0.44623208, -0.47314942,\n",
      "        -0.31655645, -0.03951299,  0.41370142]], dtype=float32)>, <tf.Variable 'dense_37/bias:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_38/kernel:0' shape=(8, 4) dtype=float32, numpy=\n",
      "array([[ 0.51734334,  0.5995192 ,  0.6741368 , -0.70352614],\n",
      "       [-0.2925401 ,  0.13590354, -0.6948012 , -0.2797278 ],\n",
      "       [ 0.02297848, -0.06778711, -0.48251516, -0.22980654],\n",
      "       [-0.50680137, -0.04414767, -0.6698041 ,  0.18238223],\n",
      "       [ 0.27501422, -0.34688324,  0.16426635,  0.4438613 ],\n",
      "       [ 0.33309716,  0.43238372,  0.56051666, -0.42981777],\n",
      "       [-0.37901834,  0.07553202,  0.15902108,  0.08639228],\n",
      "       [-0.18381995,  0.19897693,  0.35261315,  0.6731227 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_38/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(env.reset()[np.newaxis, :])\n",
    "print(env.action_space.n)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=(env.observation_space.shape[0],)))\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(env.action_space.n, activation='softmax'))\n",
    "\n",
    "model(env.reset()[np.newaxis, :])\n",
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d6d27e72-a3bd-40dd-aa63-cee82b93dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01519747,  1.4324677 , -0.77315265,  0.46594155,  0.0188807 ,\n",
       "         0.20210364,  0.        ,  0.        ], dtype=float32),\n",
       " -1.2968667428616516,\n",
       " False,\n",
       " [<tf.Tensor: shape=(8, 16), dtype=float32, numpy=\n",
       "  array([[ 6.58586796e-04,  3.87097098e-04,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00, -1.05614099e-03,\n",
       "          -1.10561890e-03,  2.35164911e-03,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          -5.45751362e-04,  5.77779138e-04, -1.60321000e-03,\n",
       "           1.66092440e-03],\n",
       "         [-1.23741761e-01, -7.27316067e-02,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  1.98438153e-01,\n",
       "           2.07734540e-01, -4.41850990e-01,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.02541134e-01, -1.08558834e-01,  3.01226884e-01,\n",
       "          -3.12070817e-01],\n",
       "         [ 6.67092800e-02,  3.92096676e-02,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00, -1.06978171e-01,\n",
       "          -1.11989863e-01,  2.38202214e-01,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          -5.52800111e-02,  5.85241541e-02, -1.62391648e-01,\n",
       "           1.68237627e-01],\n",
       "         [-4.27800044e-02, -2.51447726e-02,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  6.86040372e-02,\n",
       "           7.18179867e-02, -1.52756721e-01,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           3.54505219e-02, -3.75309624e-02,  1.04140155e-01,\n",
       "          -1.07889123e-01],\n",
       "         [-7.63729971e-04, -4.48897015e-04,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  1.22475356e-03,\n",
       "           1.28213060e-03, -2.72708922e-03,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           6.32880430e-04, -6.70021458e-04,  1.85916212e-03,\n",
       "          -1.92609057e-03],\n",
       "         [-1.51106464e-02, -8.88157450e-03,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  2.42321491e-02,\n",
       "           2.53673717e-02, -5.39563522e-02,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           1.25217456e-02, -1.32565927e-02,  3.67841274e-02,\n",
       "          -3.81083302e-02],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
       "  array([-0.08702063, -0.05114805,  0.        , -0.        , -0.        ,\n",
       "          0.1395504 ,  0.14608803, -0.31072897,  0.        , -0.        ,\n",
       "         -0.        ,  0.        ,  0.07211142, -0.07634333,  0.21183594,\n",
       "         -0.21946187], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(16, 8), dtype=float32, numpy=\n",
       "  array([[ 0.        ,  0.        ,  0.        , -0.06457921,  0.34360448,\n",
       "           0.        ,  0.        ,  0.09914531],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.03827691,  0.20365871,\n",
       "           0.        ,  0.        ,  0.05876468],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.05733678,  0.30506992,\n",
       "           0.        ,  0.        ,  0.08802636],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.03341768,  0.17780434,\n",
       "           0.        ,  0.        ,  0.05130453],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.03635016,  0.1934071 ,\n",
       "           0.        ,  0.        ,  0.05580663],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.05372332,  0.28584388,\n",
       "           0.        ,  0.        ,  0.08247878],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.01565555,  0.08329795,\n",
       "           0.        ,  0.        ,  0.0240352 ],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.08324414,  0.44291437,\n",
       "           0.        ,  0.        ,  0.12780066],\n",
       "         [ 0.        ,  0.        ,  0.        , -0.07158279,  0.38086817,\n",
       "           0.        ,  0.        ,  0.10989755]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       "  array([-0.        , -0.        , -0.        , -0.09424761,  0.50146013,\n",
       "         -0.        , -0.        ,  0.14469375], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(8, 4), dtype=float32, numpy=\n",
       "  array([[ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "         [ 0.15858497, -0.66786975,  0.15056664,  0.35871816],\n",
       "         [ 0.01477535, -0.06222538,  0.01402828,  0.03342175],\n",
       "         [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        , -0.        ,  0.        ,  0.        ],\n",
       "         [ 0.03448994, -0.14525202,  0.03274607,  0.07801601]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.1731644 , -0.72927004,  0.1644089 ,  0.39169675], dtype=float32)>])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_proba = model(obs[np.newaxis, :])[0]\n",
    "        rand = tf.random.uniform([env.action_space.n])\n",
    "        actions = action_proba - rand\n",
    "        max_action_value = tf.reduce_max(actions)\n",
    "        action = int(tf.argmax(actions))\n",
    "        y_target = tf.cast(actions == max_action_value, tf.float32)\n",
    "        # print(action_proba, y_target)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, action_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    return obs, reward, done, grads\n",
    "play_one_step(env, env.reset(), model, keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5b1ce77f-b611-4c02-ad5d-d80b6711d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22 -40 -50]\n",
      "[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
     ]
    }
   ],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor=0.97):\n",
    "    discounted = np.array(rewards)\n",
    "    for n_reward in range(len(discounted) - 2, -1, -1):\n",
    "        discounted[n_reward] += discount_factor * discounted[n_reward + 1]\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor=0.97):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_all_discounted_rewards = np.concatenate(all_discounted_rewards)\n",
    "    flat_mean = np.mean(flat_all_discounted_rewards)\n",
    "    flat_std = np.std(flat_all_discounted_rewards)\n",
    "    return [(discounted_rewards - flat_mean) / flat_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "print(discount_rewards([10, 0, -50], 0.8)) # -22 -40 -50\n",
    "print(discount_and_normalize_rewards([[10, 0, -50], [10, 20]], 0.8)) # -0.28 .... 1.26, 1.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53d6be02-f3f5-4a4c-ab89-9e917aea0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [12:22<00:00,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# training algorithm: play several episodes without applying grads, then apply mean grad according to a reward\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.97\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.01)\n",
    "loss_fn = keras.losses.categorical_crossentropy\n",
    "\n",
    "def train():\n",
    "    for n_iteration in tqdm(range(n_iterations)):\n",
    "        all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "        all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "        \n",
    "        mean_grads = []\n",
    "        for trainable_var_index in range(len(model.trainable_variables)):\n",
    "            weighted_grads = []\n",
    "            for episode, final_rewards in enumerate(all_final_rewards):\n",
    "                for step, final_reward in enumerate(final_rewards):\n",
    "                    grad = all_grads[episode][step][trainable_var_index]\n",
    "                    weighted_grads.append(grad * final_reward)\n",
    "            mean_grad = tf.reduce_mean(weighted_grads, axis=0)\n",
    "            mean_grads.append(mean_grad)\n",
    "        optimizer.apply_gradients(zip(mean_grads, model.trainable_variables))    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e647c2a5-f919-45a9-93c1-29a0a92f1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:41<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-39.04493488942172 19.198360777169057 -81.51906253760649 3.046460930017937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# play with the trained model\n",
    "\n",
    "total_rewards = []\n",
    "for i_episode in tqdm(range(200)):\n",
    "    rewards = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(200):\n",
    "        action_proba = model(obs[np.newaxis, :])[0]\n",
    "        action = int(tf.argmax(action_proba))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(rewards)\n",
    "# -39.04493488942172 19.198360777169057 -81.51906253760649 3.046460930017937\n",
    "print(np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
