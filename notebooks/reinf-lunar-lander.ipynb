{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767687b5-10fd-435a-97e1-82cb0d9f97ed",
   "metadata": {},
   "source": [
    "8. Reinf learning. Exercise: Use policy gradients to solve OpenAI Gym's LunarLander-v2 environment. You will need to install the Box2D dependencies (%pip install -U gym[box2d]). hands-on-ml book exerices page 623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbee982d-cf3a-4dfb-bddf-4fbc605513f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00629091,  1.4026109 , -0.63721627, -0.36931112,  0.00729639,\n",
       "        0.14433911,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0320204f-372a-4e58-8271-4fce3bdb0249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(-inf, inf, (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# img = env.render(mode=\"rgb_array\")\n",
    "# env.close()\n",
    "# plt.imshow(img)\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4543afbd-bb27-4e8a-a317-f8ced879c8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-176.62070072053183 105.6778123045778 -556.5619316341206 48.6481593929104\n"
     ]
    }
   ],
   "source": [
    "# random policy\n",
    "# print(np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards))\n",
    "# -179.8682271598708 113.57826569586982 -536.6091716341286 11.068357820341546\n",
    "\n",
    "total_rewards = []\n",
    "for i_episode in range(200):\n",
    "    rewards = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(200):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(rewards)\n",
    "print(np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ab7d47-4308-492f-a705-2403c6f1ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/Users/mkhokhlush/github/ml-experiments/.venv/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n",
      "2022-04-27 19:25:03.986660: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00632381  1.4050009  -0.64054596 -0.26309383  0.00733448  0.14509319\n",
      "   0.          0.        ]]\n",
      "4\n",
      "[<tf.Variable 'dense/kernel:0' shape=(8, 32) dtype=float32, numpy=\n",
      "array([[-0.16506653, -0.10286504,  0.3485502 ,  0.21087742, -0.09666863,\n",
      "         0.30440378,  0.20872843, -0.01611751, -0.29571205,  0.18467975,\n",
      "         0.37976182,  0.3422675 ,  0.29855305, -0.23479927,  0.38491297,\n",
      "         0.27555   ,  0.03468424, -0.33295113, -0.37943953,  0.09510672,\n",
      "         0.2147519 , -0.33260486,  0.3708518 , -0.2820009 ,  0.25632656,\n",
      "        -0.08792716,  0.07044172, -0.33130547,  0.19710422, -0.32593948,\n",
      "        -0.32950035, -0.19407587],\n",
      "       [-0.24595226, -0.17293982, -0.14953186,  0.02069542,  0.00083077,\n",
      "         0.24914914,  0.3541183 , -0.18796727, -0.13476849, -0.24621682,\n",
      "        -0.03331041, -0.14625455, -0.33777365,  0.08114955, -0.30508697,\n",
      "         0.15389633, -0.29010826,  0.27378392, -0.21701051,  0.2444629 ,\n",
      "        -0.08513954,  0.2243836 , -0.19367751,  0.1701566 ,  0.06590971,\n",
      "         0.24937546,  0.08908612,  0.04138261,  0.01469061, -0.18989521,\n",
      "        -0.38113618,  0.19625407],\n",
      "       [ 0.01075539, -0.04412359, -0.3066112 ,  0.00821126,  0.17623723,\n",
      "        -0.34496915,  0.28957963,  0.16568553,  0.24301988,  0.2730741 ,\n",
      "        -0.35664886,  0.05534956, -0.28448945, -0.08996564,  0.35852808,\n",
      "         0.06268746, -0.11794791,  0.35417086,  0.21611363, -0.2722724 ,\n",
      "         0.11320019, -0.37273943,  0.05011272,  0.05689439, -0.16714212,\n",
      "        -0.23633727,  0.04656541,  0.2654661 , -0.267145  , -0.32548237,\n",
      "        -0.29492256, -0.33990982],\n",
      "       [-0.05753317,  0.02700791,  0.37602568,  0.30875248, -0.19888748,\n",
      "        -0.36354068, -0.15494265, -0.0325042 ,  0.17029876, -0.34060872,\n",
      "         0.00242519,  0.2576126 , -0.00278679, -0.14627431, -0.18662873,\n",
      "         0.15720332,  0.02915782, -0.35399854,  0.10048786,  0.30832273,\n",
      "        -0.16117914, -0.00606668,  0.21692103,  0.01619831,  0.13257301,\n",
      "        -0.14502506, -0.00407779, -0.1261945 , -0.12601131,  0.05368698,\n",
      "        -0.24618597,  0.06869346],\n",
      "       [-0.1372078 , -0.08177322, -0.02635378, -0.1952291 , -0.3238816 ,\n",
      "         0.00704512,  0.37905586, -0.21208367,  0.10772318, -0.32302654,\n",
      "         0.14721441, -0.16250937,  0.14589995, -0.02375868,  0.31201905,\n",
      "        -0.31803375, -0.03502849, -0.27482414,  0.0895395 ,  0.26444823,\n",
      "        -0.11879522,  0.3424304 ,  0.0795832 , -0.3536832 , -0.11239231,\n",
      "        -0.19729823, -0.3199584 , -0.31287643,  0.08671576,  0.3443768 ,\n",
      "        -0.0561612 , -0.01002803],\n",
      "       [-0.18068135, -0.24145304, -0.06500518,  0.00856879,  0.16580135,\n",
      "         0.340802  , -0.15339504, -0.28262335,  0.26558226, -0.15473072,\n",
      "         0.1422652 ,  0.22843748, -0.1997032 ,  0.1489507 ,  0.1588183 ,\n",
      "         0.23952335, -0.20261253,  0.10973424, -0.26131767,  0.11898863,\n",
      "        -0.1609493 , -0.07199892, -0.35400465,  0.24947184, -0.19125518,\n",
      "         0.21470875, -0.24271089,  0.13776672, -0.07782146, -0.15908471,\n",
      "        -0.0078288 ,  0.28591305],\n",
      "       [ 0.15046293, -0.15255328, -0.06107375, -0.35396373,  0.3515061 ,\n",
      "        -0.3076418 , -0.2196347 ,  0.37410724, -0.16782275, -0.02519694,\n",
      "        -0.24320398, -0.30237997,  0.17278975, -0.24882041,  0.08311111,\n",
      "        -0.14293015, -0.36285895,  0.34965187,  0.36669886,  0.05614385,\n",
      "         0.27191228, -0.19925693, -0.18184224, -0.05644682, -0.34187108,\n",
      "         0.1907171 , -0.13592485,  0.27437204,  0.21949959, -0.11727697,\n",
      "        -0.05713373, -0.02213398],\n",
      "       [ 0.22405994,  0.30808598, -0.08893272, -0.22733845, -0.3034246 ,\n",
      "         0.1630634 ,  0.33454168, -0.2842226 ,  0.10947126,  0.31315058,\n",
      "         0.16566044,  0.36634016,  0.3794886 , -0.17950061, -0.04879302,\n",
      "        -0.2303572 ,  0.09745711,  0.21506697, -0.06759658,  0.03146243,\n",
      "        -0.33447835, -0.24670373, -0.35938275,  0.18183088,  0.28460366,\n",
      "         0.34774786,  0.22072947,  0.3841412 ,  0.08992076, -0.14048511,\n",
      "         0.3040312 , -0.1072877 ]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(32,) dtype=float32, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)>, <tf.Variable 'dense_1/kernel:0' shape=(32, 12) dtype=float32, numpy=\n",
      "array([[-0.30013674,  0.25007132, -0.16128509,  0.02301678, -0.1097275 ,\n",
      "        -0.15223086, -0.14957896, -0.16664535,  0.2074658 ,  0.31963918,\n",
      "        -0.14822425,  0.0122034 ],\n",
      "       [ 0.30892542,  0.13752708, -0.04899612,  0.02363369,  0.004062  ,\n",
      "         0.13741145, -0.11537582, -0.2321589 ,  0.15042892,  0.13864449,\n",
      "        -0.22698326, -0.21837436],\n",
      "       [-0.30991688, -0.1800278 ,  0.14780304, -0.33241335, -0.1565568 ,\n",
      "         0.32601032, -0.15468019,  0.21694747,  0.10505784, -0.02816522,\n",
      "         0.27612564,  0.2583548 ],\n",
      "       [-0.18948297,  0.06748357, -0.0304088 ,  0.20259866, -0.34278575,\n",
      "         0.2938694 , -0.29785144,  0.30586174, -0.06224164,  0.34883   ,\n",
      "        -0.07103819,  0.05096745],\n",
      "       [-0.07529342,  0.18184271,  0.13155475, -0.17270517, -0.14324637,\n",
      "         0.09466025, -0.18597943,  0.05605865,  0.11930135,  0.30759045,\n",
      "        -0.24787359, -0.27827534],\n",
      "       [-0.15545197, -0.01606378, -0.0798879 , -0.11532599, -0.06866801,\n",
      "         0.12204763, -0.28374353,  0.1849921 ,  0.07829699, -0.19156806,\n",
      "        -0.12721719, -0.24647284],\n",
      "       [ 0.06940728,  0.09848103,  0.13925412,  0.15912095,  0.17729166,\n",
      "         0.17119911,  0.09976184, -0.0602107 , -0.07614234, -0.23505002,\n",
      "        -0.30584785, -0.01763269],\n",
      "       [ 0.25623122,  0.32820407, -0.28604335, -0.28613043,  0.21814105,\n",
      "        -0.04077846,  0.10430244, -0.18221608, -0.3287455 ,  0.34918383,\n",
      "         0.14506134, -0.29132006],\n",
      "       [ 0.0660935 ,  0.36096254,  0.2905799 , -0.11555886,  0.01754877,\n",
      "         0.29660758, -0.12691107, -0.2526459 ,  0.02783683, -0.34850046,\n",
      "        -0.24456462,  0.24352697],\n",
      "       [-0.16267861, -0.32063845,  0.06519556,  0.01001203,  0.01411444,\n",
      "         0.3562    ,  0.18562773,  0.22609034, -0.34222525,  0.13301829,\n",
      "        -0.31226462,  0.29029587],\n",
      "       [-0.02871946, -0.04250425,  0.22652748,  0.26794317, -0.13559648,\n",
      "         0.36501375,  0.2707055 ,  0.19723824, -0.21435392, -0.07544926,\n",
      "        -0.09373212, -0.1133028 ],\n",
      "       [-0.21302581, -0.2587279 , -0.00046769, -0.2875223 , -0.32509285,\n",
      "        -0.06110081, -0.02630225,  0.11456946,  0.02329826,  0.00457889,\n",
      "         0.06702939, -0.23630805],\n",
      "       [-0.20656477,  0.23600546, -0.25985327, -0.17742246, -0.3026243 ,\n",
      "         0.08358249, -0.35151863,  0.35328326,  0.08531851,  0.08330771,\n",
      "         0.32331213,  0.35796222],\n",
      "       [-0.02641934, -0.06458956,  0.00872725,  0.22931233, -0.3190125 ,\n",
      "        -0.347834  , -0.30103576,  0.11446363,  0.02315518, -0.25241673,\n",
      "        -0.05966169,  0.0122835 ],\n",
      "       [-0.11682808,  0.22721806,  0.13664427,  0.23275343,  0.19303021,\n",
      "        -0.1144307 , -0.14185143, -0.30630964, -0.03716072,  0.18543455,\n",
      "         0.21464166,  0.07531816],\n",
      "       [-0.18409762,  0.0854767 ,  0.33093944,  0.33244798, -0.18839099,\n",
      "         0.20573518, -0.14022574,  0.27401122,  0.06353897,  0.2618489 ,\n",
      "         0.26652   , -0.2427727 ],\n",
      "       [ 0.30189392,  0.31626663,  0.2440761 ,  0.19858351, -0.15297842,\n",
      "         0.3641425 , -0.35289532,  0.03237909,  0.07026288, -0.34391242,\n",
      "         0.00752187,  0.13883105],\n",
      "       [ 0.1953148 ,  0.23089412,  0.06425139,  0.32379273,  0.19622919,\n",
      "         0.01905403,  0.07009199,  0.16562548,  0.3108054 , -0.08944449,\n",
      "         0.1733006 , -0.21505174],\n",
      "       [ 0.1048269 , -0.2759969 , -0.30201116, -0.27263266,  0.20762464,\n",
      "        -0.33009425, -0.2998722 , -0.24242432, -0.16351607, -0.21497999,\n",
      "         0.09590712,  0.28434232],\n",
      "       [ 0.2432749 , -0.29516643, -0.07613793,  0.00126666, -0.16052458,\n",
      "         0.33976635,  0.11112252,  0.02383065,  0.1180456 , -0.2860335 ,\n",
      "         0.05634117,  0.10343665],\n",
      "       [ 0.17663702, -0.05811769, -0.27384374, -0.00554109, -0.10054806,\n",
      "         0.03845361,  0.22424236,  0.3543208 , -0.17991397,  0.09167901,\n",
      "         0.04810521,  0.36320272],\n",
      "       [ 0.04514208, -0.34232923, -0.25707564,  0.3348892 , -0.07653841,\n",
      "         0.10579291, -0.16932984,  0.29428926, -0.01090354,  0.33662996,\n",
      "        -0.00435457, -0.21209072],\n",
      "       [ 0.28108922,  0.17924169, -0.17329788, -0.16183615,  0.23972276,\n",
      "        -0.2907395 , -0.2407701 , -0.35623142, -0.21451522, -0.17693542,\n",
      "        -0.148028  ,  0.35993913],\n",
      "       [ 0.11403775,  0.13181296,  0.03211337,  0.18817565, -0.05653751,\n",
      "        -0.08491403,  0.2651963 , -0.21840835,  0.2788898 ,  0.15120754,\n",
      "        -0.11607778, -0.24989934],\n",
      "       [-0.30216762, -0.24896953, -0.34104428,  0.12312403, -0.20588756,\n",
      "         0.14617279, -0.17308465, -0.23329191, -0.10337245,  0.2877284 ,\n",
      "         0.3505359 , -0.29923573],\n",
      "       [ 0.0823175 , -0.17247169, -0.10778898,  0.2551568 , -0.21564083,\n",
      "        -0.07285607, -0.36355606, -0.03948212, -0.20129925,  0.35716853,\n",
      "        -0.25058782,  0.02658811],\n",
      "       [ 0.03292528, -0.21733212, -0.2684247 ,  0.2692025 , -0.17088245,\n",
      "         0.05489516, -0.10474679,  0.03014255,  0.0159612 , -0.29354358,\n",
      "         0.27472576,  0.27307293],\n",
      "       [-0.32823682, -0.02845779, -0.17919677,  0.31033906,  0.20984593,\n",
      "         0.12446201,  0.11707819, -0.03495836,  0.01922852,  0.24476895,\n",
      "        -0.21931015,  0.11347297],\n",
      "       [-0.29646462, -0.16298509, -0.03468058,  0.0828808 , -0.26786357,\n",
      "        -0.07022467, -0.08642834, -0.35687888,  0.28762117, -0.28460458,\n",
      "         0.23311213,  0.22469631],\n",
      "       [-0.301623  ,  0.33807108,  0.21250513,  0.03476378, -0.04109699,\n",
      "        -0.02490363,  0.11562708, -0.28831545,  0.15132788, -0.30450416,\n",
      "        -0.10987654, -0.11025804],\n",
      "       [ 0.3127506 , -0.03407979,  0.33173683,  0.12569204, -0.07635838,\n",
      "        -0.25922984,  0.04971057,  0.26033232, -0.06000653, -0.1590269 ,\n",
      "        -0.19330065, -0.35719618],\n",
      "       [ 0.28796437,  0.28189024,  0.04476419, -0.04319486,  0.13006896,\n",
      "         0.27970865, -0.01590124, -0.33049625,  0.14766899, -0.20287211,\n",
      "        -0.20779683,  0.17425498]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(12,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'dense_2/kernel:0' shape=(12, 4) dtype=float32, numpy=\n",
      "array([[-0.42456484, -0.42418247, -0.24881428, -0.47168255],\n",
      "       [ 0.567988  , -0.16962788,  0.16539401,  0.401447  ],\n",
      "       [ 0.15468907, -0.4541563 ,  0.23245108,  0.5433565 ],\n",
      "       [-0.13570133, -0.16268963,  0.5892237 ,  0.12265396],\n",
      "       [ 0.03185111,  0.23324692,  0.5898964 , -0.10596502],\n",
      "       [ 0.32523584, -0.16019899,  0.04368699, -0.44906512],\n",
      "       [-0.23467875,  0.31635082,  0.23659343,  0.18192232],\n",
      "       [-0.16315216,  0.5294661 , -0.31566066,  0.09011883],\n",
      "       [-0.413499  , -0.339872  ,  0.10996866, -0.5008283 ],\n",
      "       [ 0.18414998, -0.28373888,  0.31209344,  0.55664784],\n",
      "       [-0.38962886,  0.23470563, -0.1061008 , -0.08321226],\n",
      "       [-0.18674985, -0.5377541 , -0.07974148,  0.4665665 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(env.reset()[np.newaxis, :])\n",
    "print(env.action_space.n)\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=(env.observation_space.shape[0],)))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(12, activation='relu'))\n",
    "model.add(keras.layers.Dense(env.action_space.n, activation='softmax'))\n",
    "\n",
    "model(env.reset()[np.newaxis, :])\n",
    "print(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6d27e72-a3bd-40dd-aa63-cee82b93dbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00834646,  1.3870767 ,  0.427131  , -0.50835776, -0.0089152 ,\n",
       "        -0.08303298,  0.        ,  0.        ], dtype=float32),\n",
       " 3.1594633717526053,\n",
       " False,\n",
       " [<tf.Tensor: shape=(8, 32), dtype=float32, numpy=\n",
       "  array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           5.4464461e-08, -1.7146746e-08, -2.4070822e-08,  4.7074707e-08,\n",
       "           0.0000000e+00,  4.7438562e-08,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00, -3.1500615e-08,  0.0000000e+00,  2.7008006e-08,\n",
       "           0.0000000e+00, -3.4097862e-08,  5.1876242e-08,  0.0000000e+00,\n",
       "           5.7299456e-08,  7.7531555e-09,  0.0000000e+00, -2.8408389e-08,\n",
       "           9.0709316e-09, -2.1691479e-08, -8.7921661e-09,  0.0000000e+00,\n",
       "          -4.2007219e-08,  0.0000000e+00,  0.0000000e+00, -2.1507704e-08],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           1.8500690e-05, -5.8244705e-06, -8.1764665e-06,  1.5990512e-05,\n",
       "           0.0000000e+00,  1.6114107e-05,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00, -1.0700247e-05,  0.0000000e+00,  9.1741804e-06,\n",
       "           0.0000000e+00, -1.1582488e-05,  1.7621514e-05,  0.0000000e+00,\n",
       "           1.9463692e-05,  2.6336206e-06,  0.0000000e+00, -9.6498670e-06,\n",
       "           3.0812480e-06, -7.3682422e-06, -2.9865557e-06,  0.0000000e+00,\n",
       "          -1.4269168e-05,  0.0000000e+00,  0.0000000e+00, -7.3058172e-06],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           5.5164387e-06, -1.7367099e-06, -2.4380156e-06,  4.7679669e-06,\n",
       "           0.0000000e+00,  4.8048200e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00, -3.1905433e-06,  0.0000000e+00,  2.7355088e-06,\n",
       "           0.0000000e+00, -3.4536054e-06,  5.2542905e-06,  0.0000000e+00,\n",
       "           5.8035816e-06,  7.8527910e-07,  0.0000000e+00, -2.8773466e-06,\n",
       "           9.1875029e-07, -2.1970236e-06, -8.9051548e-07,  0.0000000e+00,\n",
       "          -4.2547053e-06,  0.0000000e+00,  0.0000000e+00, -2.1784101e-06],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          -7.2940393e-06,  2.2963422e-06,  3.2236346e-06, -6.3043822e-06,\n",
       "           0.0000000e+00, -6.3531106e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  4.2186543e-06,  0.0000000e+00, -3.6169911e-06,\n",
       "           0.0000000e+00,  4.5664847e-06, -6.9474172e-06,  0.0000000e+00,\n",
       "          -7.6737106e-06, -1.0383251e-06,  0.0000000e+00,  3.8045343e-06,\n",
       "          -1.2148056e-06,  2.9049861e-06,  1.1774725e-06,  0.0000000e+00,\n",
       "           5.6257290e-06,  0.0000000e+00,  0.0000000e+00,  2.8803745e-06],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          -6.3021020e-08,  1.9840559e-08,  2.7852431e-08, -5.4470309e-08,\n",
       "           0.0000000e+00, -5.4891327e-08,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  3.6449475e-08,  0.0000000e+00, -3.1251059e-08,\n",
       "           0.0000000e+00,  3.9454754e-08, -6.0026181e-08,  0.0000000e+00,\n",
       "          -6.6301403e-08, -8.9712024e-09,  0.0000000e+00,  3.2871444e-08,\n",
       "          -1.0496007e-08,  2.5099286e-08,  1.0173446e-08,  0.0000000e+00,\n",
       "           4.8606697e-08,  0.0000000e+00,  0.0000000e+00,  2.4886640e-08],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          -1.2495581e-06,  3.9339145e-07,  5.5224797e-07, -1.0800177e-06,\n",
       "           0.0000000e+00, -1.0883655e-06,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  7.2270700e-07,  0.0000000e+00, -6.1963470e-07,\n",
       "           0.0000000e+00,  7.8229459e-07, -1.1901775e-06,  0.0000000e+00,\n",
       "          -1.3146002e-06, -1.7787777e-07,  0.0000000e+00,  6.5176317e-07,\n",
       "          -2.0811105e-07,  4.9765958e-07,  2.0171542e-07,  0.0000000e+00,\n",
       "           9.6375607e-07,  0.0000000e+00,  0.0000000e+00,  4.9344334e-07],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "           0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
       "  array([ 0.0000000e+00, -0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "          1.3228817e-05, -4.1647554e-06, -5.8465371e-06,  1.1433927e-05,\n",
       "          0.0000000e+00,  1.1522304e-05,  0.0000000e+00,  0.0000000e+00,\n",
       "          0.0000000e+00, -7.6511524e-06,  0.0000000e+00,  6.5599470e-06,\n",
       "         -0.0000000e+00, -8.2819943e-06,  1.2600167e-05,  0.0000000e+00,\n",
       "          1.3917406e-05,  1.8831558e-06,  0.0000000e+00, -6.9000844e-06,\n",
       "          2.2032293e-06, -5.2686210e-06, -2.1355202e-06,  0.0000000e+00,\n",
       "         -1.0203090e-05,  0.0000000e+00,  0.0000000e+00, -5.2239843e-06],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(32, 12), dtype=float32, numpy=\n",
       "  array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-1.27013755e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.45411639e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.91934805e-06, -1.98408338e-06,\n",
       "           0.00000000e+00,  1.07023016e-06,  0.00000000e+00],\n",
       "         [-3.58543662e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -4.10478560e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  5.41807503e-06, -5.60081480e-06,\n",
       "           0.00000000e+00,  3.02112358e-06,  0.00000000e+00],\n",
       "         [-5.76247248e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -6.59716397e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  8.70786789e-06, -9.00156556e-06,\n",
       "           0.00000000e+00,  4.85551527e-06,  0.00000000e+00],\n",
       "         [-1.52599205e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.74703132e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  2.30597857e-06, -2.38375401e-06,\n",
       "           0.00000000e+00,  1.28581564e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-5.57590567e-07,  0.00000000e+00,  0.00000000e+00,\n",
       "          -6.38357335e-07,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  8.42594090e-07, -8.71012901e-07,\n",
       "           0.00000000e+00,  4.69831207e-07,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-6.69615611e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -7.66609082e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.01187889e-05, -1.04600731e-05,\n",
       "           0.00000000e+00,  5.64224592e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-4.99855378e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -5.72259205e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  7.55348447e-06, -7.80824666e-06,\n",
       "           0.00000000e+00,  4.21182995e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-1.00768812e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.15365128e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.52275170e-05, -1.57411068e-05,\n",
       "           0.00000000e+00,  8.49087701e-06,  0.00000000e+00],\n",
       "         [-1.09350640e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.25190024e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.65243466e-06, -1.70816759e-06,\n",
       "           0.00000000e+00,  9.21399078e-07,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-1.90170260e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -2.17716342e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  2.87372745e-06, -2.97065185e-06,\n",
       "           0.00000000e+00,  1.60239301e-06,  0.00000000e+00],\n",
       "         [-1.59428680e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.82521853e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  2.40918098e-06, -2.49043728e-06,\n",
       "           0.00000000e+00,  1.34336153e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-4.51954293e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -5.17419630e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  6.82963491e-06, -7.05998309e-06,\n",
       "           0.00000000e+00,  3.80821052e-06,  0.00000000e+00],\n",
       "         [-5.07622190e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -5.81151062e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  7.67085203e-06, -7.92957235e-06,\n",
       "           0.00000000e+00,  4.27727400e-06,  0.00000000e+00],\n",
       "         [-2.22640438e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -2.54889801e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  3.36439530e-06, -3.47786886e-06,\n",
       "           0.00000000e+00,  1.87598994e-06,  0.00000000e+00],\n",
       "         [-8.89181501e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.01797905e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.34367237e-05, -1.38899140e-05,\n",
       "           0.00000000e+00,  7.49232913e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-5.61830075e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -6.43210933e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  8.49000571e-06, -8.77635466e-06,\n",
       "           0.00000000e+00,  4.73403452e-06,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [-1.13597332e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          -1.30051865e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "           0.00000000e+00,  1.71660793e-06, -1.77450534e-06,\n",
       "           0.00000000e+00,  9.57182124e-07,  0.00000000e+00]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(12,), dtype=float32, numpy=\n",
       "  array([-4.7329254e-06, -0.0000000e+00, -0.0000000e+00, -5.4184875e-06,\n",
       "         -0.0000000e+00, -0.0000000e+00,  0.0000000e+00,  7.1520844e-06,\n",
       "         -7.3933084e-06, -0.0000000e+00,  3.9880088e-06, -0.0000000e+00],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(12, 4), dtype=float32, numpy=\n",
       "  array([[ 4.7638181e-16,  1.4321468e-05, -1.4321722e-05,  2.5328478e-10],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "         [ 7.8739442e-16,  2.3671442e-05, -2.3671862e-05,  4.1864534e-10],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "         [ 2.4579160e-16,  7.3892338e-06, -7.3893648e-06,  1.3068356e-10],\n",
       "         [ 4.5908715e-16,  1.3801538e-05, -1.3801783e-05,  2.4408947e-10],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "         [ 7.8157684e-16,  2.3496548e-05, -2.3496965e-05,  4.1555223e-10],\n",
       "         [ 0.0000000e+00,  0.0000000e+00, -0.0000000e+00,  0.0000000e+00]],\n",
       "        dtype=float32)>,\n",
       "  <tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
       "  array([ 1.1731569e-16,  3.5268622e-06, -3.5269247e-06,  6.2374925e-11],\n",
       "        dtype=float32)>])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "# approach from the book\n",
    "# logits = tf.math.log(probas + keras.backend.epsilon())\n",
    "# action = tf.random.categorical(logits, num_samples=1)\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_proba = model(obs[np.newaxis, :])[0]\n",
    "        rand = tf.random.uniform([env.action_space.n]) # choose random action according to actions' probability\n",
    "        actions = action_proba - rand\n",
    "        max_action_value = tf.reduce_max(actions)\n",
    "        action = int(tf.argmax(actions))\n",
    "        y_target = tf.cast(actions == max_action_value, tf.float32) # right action is equal to 1 (action above) and 0s (not action) \n",
    "        # print(action_proba, y_target)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, action_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    return obs, reward, done, grads\n",
    "play_one_step(env, env.reset(), model, keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1ce77f-b611-4c02-ad5d-d80b6711d7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-22 -40 -50]\n",
      "[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\n"
     ]
    }
   ],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "def discount_rewards(rewards, discount_factor=0.97):\n",
    "    discounted = np.array(rewards)\n",
    "    for n_reward in range(len(discounted) - 2, -1, -1):\n",
    "        discounted[n_reward] += discount_factor * discounted[n_reward + 1]\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor=0.97):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_all_discounted_rewards = np.concatenate(all_discounted_rewards)\n",
    "    flat_mean = np.mean(flat_all_discounted_rewards)\n",
    "    flat_std = np.std(flat_all_discounted_rewards)\n",
    "    return [(discounted_rewards - flat_mean) / flat_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "print(discount_rewards([10, 0, -50], 0.8)) # -22 -40 -50\n",
    "print(discount_and_normalize_rewards([[10, 0, -50], [10, 20]], 0.8)) # -0.28 .... 1.26, 1.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d6be02-f3f5-4a4c-ab89-9e917aea0c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [41:16<00:00,  8.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# training algorithm: play several episodes without applying grads, then apply mean grad according to a reward\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_iterations = 300\n",
    "n_episodes_per_update = 12\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = keras.optimizers.Adam(0.01)\n",
    "loss_fn = keras.losses.categorical_crossentropy\n",
    "\n",
    "def train():\n",
    "    for n_iteration in tqdm(range(n_iterations)):\n",
    "        all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "        all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "        \n",
    "        mean_grads = []\n",
    "        for trainable_var_index in range(len(model.trainable_variables)):\n",
    "            weighted_grads = []\n",
    "            for episode, final_rewards in enumerate(all_final_rewards):\n",
    "                for step, final_reward in enumerate(final_rewards):\n",
    "                    grad = all_grads[episode][step][trainable_var_index]\n",
    "                    weighted_grads.append(grad * final_reward) # the more reward the more grad is important\n",
    "            mean_grad = tf.reduce_mean(weighted_grads, axis=0) # reduce mean to get a mean grad between all episodes and steps\n",
    "            mean_grads.append(mean_grad)\n",
    "        optimizer.apply_gradients(zip(mean_grads, model.trainable_variables))    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e647c2a5-f919-45a9-93c1-29a0a92f1212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:48<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.660429310869976 24.329520575340702 -25.517524436082713 98.75970244131753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# play with the trained model\n",
    "\n",
    "total_rewards = []\n",
    "for i_episode in tqdm(range(200)):\n",
    "    rewards = 0\n",
    "    obs = env.reset()\n",
    "    for t in range(200):\n",
    "        action_proba = model(obs[np.newaxis, :])[0]\n",
    "        action = int(tf.argmax(action_proba))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(rewards)\n",
    "# 29.660429310869976 24.329520575340702 -25.517524436082713 98.75970244131753\n",
    "print(np.mean(total_rewards), np.std(total_rewards), np.min(total_rewards), np.max(total_rewards))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccab51c2-3690-4fdc-b58c-6f8009e4bed7",
   "metadata": {},
   "source": [
    "# check the result\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def lander_render_policy_net(model, n_max_steps=500, seed=42):\n",
    "    frames = []\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    env.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        probas = model(obs[np.newaxis])\n",
    "        logits = tf.math.log(probas + keras.backend.epsilon())\n",
    "        action = tf.random.categorical(logits, num_samples=1)\n",
    "        obs, reward, done, info = env.step(action[0, 0].numpy())\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    return frames\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "frames = lander_render_policy_net(model, seed=42)\n",
    "plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
