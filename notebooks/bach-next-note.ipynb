{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c943105-b4dc-4a54-a726-e94cbad742d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 149\n"
     ]
    }
   ],
   "source": [
    "# Todo read about tf print, print and graphs. Check the differences. How tf2 creates graphs\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8b7d3d-78c7-444a-a80d-627a8f8e6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data/bach-next-note/'\n",
    "args.train_folder = args.data_folder + 'train/'\n",
    "args.val_folder = args.data_folder + 'valid/'\n",
    "args.test_folder = args.data_folder + 'test/'\n",
    "# args.train_fraction = 0.8\n",
    "args.seed = 101\n",
    "args.batch_size = 32\n",
    "args.epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372468fe-a3ee-4714-a528-8a0a48140176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(None,), dtype=int32)\n",
      "<MapDataset shapes: (None,), types: tf.int32>\n",
      "tf.Tensor(\n",
      "[66 61 57 54 66 61 57 54 68 61 59 54 68 61 59 54 69 66 61 54 69 66 61 56\n",
      " 69 66 61 57 69 66 61 59], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_history(history):\n",
    "    log.info(\"History keys: %s\", history.history.keys())\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(history.history['accuracy'], label='Train')\n",
    "    ax.plot(history.history['val_accuracy'], label='Test')\n",
    "    ax.set_title('Model accuracy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True)\n",
    "    ax.legend(['Train', 'Val'], loc='lower right')\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# couldn't make it work\n",
    "# def build_dataset(data_folder):\n",
    "#     def preprocess(line):\n",
    "#         csv_records = tf.io.decode_csv(line, record_defaults=[tf.constant(0)]*4)\n",
    "#         print(csv_records)\n",
    "#         x = tf.stack(csv_records)\n",
    "#         return x\n",
    "#     dataset = tf.data.Dataset.list_files(data_folder + 'chorale_*.csv', seed=args.seed)\n",
    "#     dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1))\n",
    "#     dataset = dataset.map(preprocess)\n",
    "# #     dataset = dataset.shuffle(10000)\n",
    "#     dataset = dataset.batch(args.batch_size).prefetch(1)\n",
    "#     for d in dataset:\n",
    "#         print(d)\n",
    "#         break\n",
    "#     return dataset\n",
    "\n",
    "def build_dataset(data_folder):\n",
    "    paths = Path(data_folder).glob('**/chorale_*.csv')\n",
    "    np_list = [pd.read_csv(p).to_numpy() for p in paths]\n",
    "    ragged = tf.ragged.constant(np_list)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(ragged)\n",
    "    def create_target(batch):\n",
    "        print(batch)\n",
    "        return batch\n",
    "#     def batch_window(window):\n",
    "#         return window.batch(32 + 1)\n",
    "#     def to_windows(chorale):\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "#         dataset = dataset.window(32 + 1, 16, drop_remainder=True)\n",
    "#         return dataset.flat_map(batch_window)\n",
    "    dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices).flat_map(tf.data.Dataset.from_tensor_slices) # to a seq of notes\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.map(create_target)\n",
    "    print(dataset)\n",
    "    for d in dataset:\n",
    "        print(d)\n",
    "        break\n",
    "#     def preprocess(line):\n",
    "#         csv_records = tf.io.decode_csv(line, record_defaults=[tf.constant(0)]*4)\n",
    "#         print(csv_records)\n",
    "#         x = tf.stack(csv_records)\n",
    "#         return x\n",
    "#     dataset = tf.data.Dataset.list_files(data_folder + 'chorale_*.csv', seed=args.seed)\n",
    "#     dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1))\n",
    "#     dataset = dataset.map(preprocess)\n",
    "# #     dataset = dataset.shuffle(10000)\n",
    "#     dataset = dataset.batch(args.batch_size).prefetch(1)\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 4]))\n",
    "    model.add(keras.layers.SimpleRNN(20))\n",
    "    model.add(keras.layers.Dense(4))\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Nadam(learning_rate=0.01), metrics=[keras.metrics.mean_squared_error])\n",
    "    return model\n",
    "\n",
    "train_dataset = build_dataset(data_folder=args.train_folder)\n",
    "# val_dataset = build_dataset(data_folder=args.val_folder)\n",
    "# model = baseline_model()\n",
    "# model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0ad426-43d4-460d-8106-948121374cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside to_windows chorale:\n",
      "[[66 61 57 54]\n",
      " [66 61 57 54]\n",
      " [68 61 59 54]\n",
      " ...\n",
      " [66 61 57 54]\n",
      " [66 61 57 54]\n",
      " [66 61 57 54]]\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "to_windows d after windows:\n",
      "<_VariantDataset shapes: (4,), types: tf.int32>\n",
      "final dataset:\n",
      "tf.Tensor(\n",
      "[31 26 22 19 31 26 22 19 33 26 24 19 33 26 24 19 34 31 26 19 34 31 26 21\n",
      " 34 31 26 22 34 31 26 24 33 30 26 26 33 30 26 26 33 30 24 14 33 30 24 14\n",
      " 31 31 22 15 31 31 22 15 31 31 22 15 31 31 22 15 31 31 24 15 31 31 24 15\n",
      " 33 31 24 15 33 31 24 15 34 31 26 14 34 31 26 14 34 31 26 12 34 31 26 12\n",
      " 33 30 26 14 33 30 26 14 33 30 24 14 33 30 24 14 31 26 22  7 31 26 22  7\n",
      " 31 26 22  7 31 26 22  7 38 31 22 19], shape=(132,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (None,), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "import argparse \n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data/bach-next-note/'\n",
    "args.train_folder = args.data_folder + 'train/'\n",
    "args.val_folder = args.data_folder + 'valid/'\n",
    "args.test_folder = args.data_folder + 'test/'\n",
    "# args.train_fraction = 0.8\n",
    "args.seed = 101\n",
    "args.batch_size = 32\n",
    "args.epochs = 7\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def create_target(batch):\n",
    "    print(batch)\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    min_note = 36\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        tf.print(\"inside to_windows chorale:\")\n",
    "        tf.print(chorale)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        for d in dataset:\n",
    "            tf.print(\"to_windows d after windows:\")\n",
    "            tf.print(d)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales, ragged_rank=1)\n",
    "#     print(\"chorale[0] {}\".format(chorales[0]))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "    dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "    \n",
    "#     dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "#     if cache:\n",
    "#         dataset = dataset.cache()\n",
    "#     if shuffle_buffer_size:\n",
    "#         dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.map(create_target)\n",
    "    for d in dataset:\n",
    "        print(\"final dataset:\")\n",
    "        print(d)\n",
    "        break\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "paths = Path(args.train_folder).glob('**/chorale_*.csv')\n",
    "np_list = [pd.read_csv(p).values.tolist() for p in paths]\n",
    "# print(np_list[0])\n",
    "bach_dataset(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fa0d7-b388-4365-9ea1-45c9fbd28f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    break\n",
    "\n",
    "dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77bbed0c-efbf-46c1-ba86-e0cdf68520de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[[1, 2], [3, 2], [4, 5]], [[1, 2]]]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ragged.constant([[1, 2], [3], [4, 5, 6]], ragged_rank=1)\n",
    "tf.ragged.constant([[[1, 2], [3, 2], [4, 5]], [[1, 2]]], ragged_rank=2)\n",
    "# tf.ragged.constant([[[1], [2]], [[3], [4]], [[5], [6]]], ragged_rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55fef7c8-959b-4624-b174-13d369ce7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(7).window(3, 4)\n",
    "for d in dataset:\n",
    "    print([item.numpy() for item in d])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
