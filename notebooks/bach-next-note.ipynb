{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c943105-b4dc-4a54-a726-e94cbad742d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8b7d3d-78c7-444a-a80d-627a8f8e6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data/bach-next-note/'\n",
    "args.train_folder = args.data_folder + 'train/'\n",
    "args.val_folder = args.data_folder + 'valid/'\n",
    "args.test_folder = args.data_folder + 'test/'\n",
    "# args.train_fraction = 0.8\n",
    "args.seed = 101\n",
    "args.batch_size = 32\n",
    "args.epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372468fe-a3ee-4714-a528-8a0a48140176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"args_0:0\", shape=(None,), dtype=int32)\n",
      "<MapDataset shapes: (None,), types: tf.int32>\n",
      "tf.Tensor(\n",
      "[66 61 57 54 66 61 57 54 68 61 59 54 68 61 59 54 69 66 61 54 69 66 61 56\n",
      " 69 66 61 57 69 66 61 59], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_history(history):\n",
    "    log.info(\"History keys: %s\", history.history.keys())\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(history.history['accuracy'], label='Train')\n",
    "    ax.plot(history.history['val_accuracy'], label='Test')\n",
    "    ax.set_title('Model accuracy')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.grid(True)\n",
    "    ax.legend(['Train', 'Val'], loc='lower right')\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# couldn't make it work\n",
    "# def build_dataset(data_folder):\n",
    "#     def preprocess(line):\n",
    "#         csv_records = tf.io.decode_csv(line, record_defaults=[tf.constant(0)]*4)\n",
    "#         print(csv_records)\n",
    "#         x = tf.stack(csv_records)\n",
    "#         return x\n",
    "#     dataset = tf.data.Dataset.list_files(data_folder + 'chorale_*.csv', seed=args.seed)\n",
    "#     dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1))\n",
    "#     dataset = dataset.map(preprocess)\n",
    "# #     dataset = dataset.shuffle(10000)\n",
    "#     dataset = dataset.batch(args.batch_size).prefetch(1)\n",
    "#     for d in dataset:\n",
    "#         print(d)\n",
    "#         break\n",
    "#     return dataset\n",
    "\n",
    "def build_dataset(data_folder):\n",
    "    paths = Path(data_folder).glob('**/chorale_*.csv')\n",
    "    np_list = [pd.read_csv(p).to_numpy() for p in paths]\n",
    "    ragged = tf.ragged.constant(np_list)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(ragged)\n",
    "    def create_target(batch):\n",
    "        print(batch)\n",
    "        return batch\n",
    "#     def batch_window(window):\n",
    "#         return window.batch(32 + 1)\n",
    "#     def to_windows(chorale):\n",
    "#         dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "#         dataset = dataset.window(32 + 1, 16, drop_remainder=True)\n",
    "#         return dataset.flat_map(batch_window)\n",
    "    dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices).flat_map(tf.data.Dataset.from_tensor_slices) # to a seq of notes\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.map(create_target)\n",
    "    print(dataset)\n",
    "    for d in dataset:\n",
    "        print(d)\n",
    "        break\n",
    "#     def preprocess(line):\n",
    "#         csv_records = tf.io.decode_csv(line, record_defaults=[tf.constant(0)]*4)\n",
    "#         print(csv_records)\n",
    "#         x = tf.stack(csv_records)\n",
    "#         return x\n",
    "#     dataset = tf.data.Dataset.list_files(data_folder + 'chorale_*.csv', seed=args.seed)\n",
    "#     dataset = dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1))\n",
    "#     dataset = dataset.map(preprocess)\n",
    "# #     dataset = dataset.shuffle(10000)\n",
    "#     dataset = dataset.batch(args.batch_size).prefetch(1)\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 4]))\n",
    "    model.add(keras.layers.SimpleRNN(20))\n",
    "    model.add(keras.layers.Dense(4))\n",
    "    model.compile(loss='mse',optimizer=keras.optimizers.Nadam(learning_rate=0.01), metrics=[keras.metrics.mean_squared_error])\n",
    "    return model\n",
    "\n",
    "train_dataset = build_dataset(data_folder=args.train_folder)\n",
    "# val_dataset = build_dataset(data_folder=args.val_folder)\n",
    "# model = baseline_model()\n",
    "# model.fit(train_dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0ad426-43d4-460d-8106-948121374cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3' b'note0' b'note1' b'note2' b'note3'\n",
      " b'note0' b'note1' b'note2' b'note3'], shape=(180,), dtype=string)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (None,), types: tf.string>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "import argparse \n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data/bach-next-note/'\n",
    "args.train_folder = args.data_folder + 'train/'\n",
    "args.val_folder = args.data_folder + 'valid/'\n",
    "args.test_folder = args.data_folder + 'test/'\n",
    "# args.train_fraction = 0.8\n",
    "args.seed = 101\n",
    "args.batch_size = 32\n",
    "args.epochs = 7\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def create_target(batch):\n",
    "    print(batch)\n",
    "    X = batch[:, :-1]\n",
    "    Y = batch[:, 1:] # predict next note in each arpegio, at each step\n",
    "    return X, Y\n",
    "\n",
    "def preprocess(window):\n",
    "    window = tf.where(window == 0, window, window - min_note + 1) # shift values\n",
    "    return tf.reshape(window, [-1]) # convert to arpegio\n",
    "\n",
    "def bach_dataset(chorales, batch_size=32, shuffle_buffer_size=None,\n",
    "                 window_size=32, window_shift=16, cache=True):\n",
    "    def batch_window(window):\n",
    "        return window.batch(window_size + 1)\n",
    "\n",
    "    def to_windows(chorale):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(chorale)\n",
    "        dataset = dataset.window(window_size + 1, window_shift, drop_remainder=True)\n",
    "        return dataset.flat_map(batch_window)\n",
    "\n",
    "    chorales = tf.ragged.constant(chorales)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(chorales)\n",
    "#     dataset = dataset.flat_map(to_windows).map(preprocess)\n",
    "#     if cache:\n",
    "#         dataset = dataset.cache()\n",
    "#     if shuffle_buffer_size:\n",
    "#         dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#     dataset = dataset.map(create_target)\n",
    "    for d in dataset:\n",
    "        print(d)\n",
    "        break\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "paths = Path(args.train_folder).glob('**/chorale_*.csv')\n",
    "np_list = [pd.read_csv(p) for p in paths]\n",
    "# print(np_list)\n",
    "bach_dataset(np_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01fa0d7-b388-4365-9ea1-45c9fbd28f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    break\n",
    "\n",
    "dataset = dataset.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\n",
    "\n",
    "for d in dataset:\n",
    "    print(d)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
