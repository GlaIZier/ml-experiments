{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24030bef-84b3-4bb0-9386-2e10e3794d2b",
   "metadata": {},
   "source": [
    "# Bag of words implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb8c66-6545-4e74-ac5a-883cd31c690b",
   "metadata": {},
   "source": [
    "Try to implement a simple sentiment classifier the way that you would do it in pytorch and see how tf is compared to pytorch. \n",
    "\n",
    "inspuired by\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification_with_hub\n",
    "https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "https://developers.google.com/machine-learning/guides/text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c536807-d1c3-46bb-96a1-a0d19b29c7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:10,822 : INFO : No config specified, defaulting to config: imdb_reviews/plain_text\n",
      "2023-12-11 17:13:10,824 : INFO : Load dataset info from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n",
      "2023-12-11 17:13:10,826 : INFO : Reusing dataset imdb_reviews (./data-ignored/imdb/imdb_reviews/plain_text/1.0.0)\n",
      "2023-12-11 17:13:10,875 : INFO : Constructing tf.data.Dataset imdb_reviews for split None, from ./data-ignored/imdb/imdb_reviews/plain_text/1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.13.0\n",
      "Eager mode:  True\n",
      "GPU is NOT AVAILABLE\n",
      "25000\n",
      "25000\n",
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:10.896311: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Todo control logging with debug\n",
    "import logging\n",
    "import argparse \n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.data_folder = './data-ignored/imdb/'\n",
    "args.val_fraction = 0.25\n",
    "args.vocab_size = 2500\n",
    "args.small_vocab_size = 250\n",
    "args.epochs = 50\n",
    "args.batch_size = 32\n",
    "\n",
    "Path(args.data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
    "\n",
    "ds, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True, data_dir=args.data_folder)\n",
    "train_ds_len= tf.data.experimental.cardinality(ds['train']).numpy()\n",
    "test_ds_len= tf.data.experimental.cardinality(ds['test']).numpy() \n",
    "print(train_ds_len)\n",
    "print(test_ds_len)\n",
    "for d in ds['train'].take(1):\n",
    "    print(d)\n",
    "    \n",
    "# train_dataset = ds['train'].batch(args.batch_size)\n",
    "train_dataset = ds['train']\n",
    "val_dataset = ds['test'].take(int(args.val_fraction * (train_ds_len + test_ds_len)))\n",
    "test_dataset = ds['test'].skip(int(args.val_fraction * (train_ds_len + test_ds_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291fb79-b188-4b19-826f-87c97060bfec",
   "metadata": {},
   "source": [
    "## 1. Bag of words from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a29783a-77e4-42c1-9869-190d1645e908",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_processed layer: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all']\n",
      "one_processed model: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:10.943827: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:13:10.967843: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class OneTextPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OneTextPreprocessing, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        # print(f\"Type inputs: {type(inputs)}\")\n",
    "        # print(f\"Inputs shape: {inputs.shape}\")\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            byte_string = string_tensor.numpy()\n",
    "            # print(f\"byte_string: {byte_string}\")\n",
    "            processed = ''.join(c for c in byte_string.decode('UTF-8').lower() if c.isalnum() or c == \" \")\n",
    "            # print(f\"type(processed): {type(processed)}\")\n",
    "            outputs = outputs.write(i, processed)\n",
    "        stacked_output = outputs.stack()\n",
    "        # print(f\"type(stacked_output): {type(stacked_output)}\")\n",
    "        # print(f\"tf.shape(stacked_output): {tf.shape(stacked_output)}\")\n",
    "        # outputs shape: B, Ragged texts\n",
    "        return stacked_output\n",
    "\n",
    "one_text_preprocessing = OneTextPreprocessing()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    one_processed = one_text_preprocessing(d[0])\n",
    "    print(f\"one_processed layer: {one_processed}\")\n",
    "\n",
    "one_model = keras.models.Sequential()\n",
    "one_model.add(one_text_preprocessing)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    one_processed = one_model(d[0])\n",
    "    print(f\"one_processed model: {one_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9ce39f-ab02-4c0c-8368-b2951f0d34e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:11.046598: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two_processed layer: <tf.RaggedTensor [[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie.', b\"Don't\",\n",
      "  b'be', b'lured', b'in', b'by', b'Christopher', b'Walken', b'or',\n",
      "  b'Michael', b'Ironside.', b'Both', b'are', b'great', b'actors,', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history.', b'Even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b\"movie's\", b'ridiculous', b'storyline.', b'This',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'US', b'propaganda',\n",
      "  b'piece.', b'The', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'Columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions.', b'Maria', b'Conchita', b'Alonso',\n",
      "  b'appeared', b'phony,', b'and', b'her', b'pseudo-love', b'affair',\n",
      "  b'with', b'Walken', b'was', b'nothing', b'but', b'a', b'pathetic',\n",
      "  b'emotional', b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid',\n",
      "  b'of', b'any', b'real', b'meaning.', b'I', b'am', b'disappointed',\n",
      "  b'that', b'there', b'are', b'movies', b'like', b'this,', b'ruining',\n",
      "  b\"actor's\", b'like', b'Christopher', b\"Walken's\", b'good', b'name.', b'I',\n",
      "  b'could', b'barely', b'sit', b'through', b'it.']                          ,\n",
      " [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films,', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including,', b'really', b'tired,',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot.', b'However', b'on',\n",
      "  b'this', b'occasion', b'I', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish.', b'The', b'plot', b'development', b'was',\n",
      "  b'constant.', b'Constantly', b'slow', b'and', b'boring.', b'Things',\n",
      "  b'seemed', b'to', b'happen,', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why.', b'I',\n",
      "  b'admit,', b'I', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film,', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else.', b'I', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all.']                                               ]>\n",
      "two_processed model: <tf.RaggedTensor [[b'this', b'was', b'an', b'absolutely', b'terrible', b'movie', b'dont',\n",
      "  b'be', b'lured', b'in', b'by', b'christopher', b'walken', b'or',\n",
      "  b'michael', b'ironside', b'both', b'are', b'great', b'actors', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history', b'even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b'movies', b'ridiculous', b'storyline', b'this',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'us', b'propaganda',\n",
      "  b'piece', b'the', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions', b'maria', b'conchita', b'alonso',\n",
      "  b'appeared', b'phony', b'and', b'her', b'pseudolove', b'affair', b'with',\n",
      "  b'walken', b'was', b'nothing', b'but', b'a', b'pathetic', b'emotional',\n",
      "  b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid', b'of', b'any',\n",
      "  b'real', b'meaning', b'i', b'am', b'disappointed', b'that', b'there',\n",
      "  b'are', b'movies', b'like', b'this', b'ruining', b'actors', b'like',\n",
      "  b'christopher', b'walkens', b'good', b'name', b'i', b'could', b'barely',\n",
      "  b'sit', b'through', b'it']                                                ,\n",
      " [b'i', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including', b'really', b'tired',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot', b'however', b'on',\n",
      "  b'this', b'occasion', b'i', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish', b'the', b'plot', b'development', b'was',\n",
      "  b'constant', b'constantly', b'slow', b'and', b'boring', b'things',\n",
      "  b'seemed', b'to', b'happen', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why', b'i',\n",
      "  b'admit', b'i', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else', b'i', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all']                                                ]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:11.102266: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class TwoTokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(TwoTokenizer, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        # print(f\"Type inputs: {type(inputs)}\")\n",
    "        # print(f\"Inputs shape: {inputs.shape}\")\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        tokens_flat_tensor_array = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        str_len_tensor_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        start_len = 0\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            byte_string = string_tensor.numpy()\n",
    "            # print(f\"byte_string: {byte_string}\")\n",
    "            string = byte_string.decode('UTF-8')\n",
    "            tokens = string.split()\n",
    "            for iw in tf.range(len(tokens)):\n",
    "                tokens_flat_tensor_array = tokens_flat_tensor_array.write(start_len + iw, tokens[iw])\n",
    "            str_len_tensor_array = str_len_tensor_array.write(i, len(tokens))\n",
    "            start_len += len(tokens)\n",
    "        \n",
    "        ragged_tensor = tf.RaggedTensor.from_row_lengths(\n",
    "                values=tokens_flat_tensor_array.stack(),\n",
    "                row_lengths=str_len_tensor_array.stack())\n",
    "        # print(f\"type(ragged_tensor): {type(ragged_tensor)}\")\n",
    "        # print(f\"tf.shape(ragged_tensor): {tf.shape(ragged_tensor)}\")\n",
    "        # outputs shape: Ragged tensor: B, Tokens\n",
    "        return ragged_tensor\n",
    "        \n",
    "two_tokenizer = TwoTokenizer()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    two_processed = two_tokenizer(d[0])\n",
    "    print(f\"two_processed layer: {two_processed}\")\n",
    "\n",
    "two_model = keras.models.Sequential()\n",
    "two_model.add(one_text_preprocessing)\n",
    "two_model.add(two_tokenizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    two_processed = two_model(d[0])\n",
    "    print(f\"two_processed model: {two_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f851e838-d5fa-4b3f-832d-11dee20c73e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:14.823473: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:13:15.794995: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:13:15.885284: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three_processed model: [[43.  2.  1.  2.  1.  0.  1.  3.  5.  1.  2.  2.  0.  1.  0.  1.  3.  0.\n",
      "   3.  2.  0.  0.  2.  0.  0.  2.  0.  1.  0.  2.  0.  0.  1.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "   3.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.\n",
      "   0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  2.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38.  5.  4.  2.  5.  4.  1.  0.  3.  1.  6.  0.  0.  1.  0.  1.  3.  3.\n",
      "   0.  3.  0.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  1.  0.  0.  2.  0.\n",
      "   0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "three_processed model with batch normalization: [[ 0.9999199  -0.9997778  -0.9997779   0.         -0.99987507 -0.99987507\n",
      "   0.          0.99977785  0.9995005   0.         -0.99987507  0.99950033\n",
      "   0.          0.          0.          0.          0.         -0.99977785\n",
      "   0.99977785 -0.99800587  0.          0.          0.99950033 -0.99950033\n",
      "   0.          0.99950033 -0.99950033  0.998006   -0.998006    0.99950033\n",
      "   0.         -0.998006    0.998006    0.         -0.99950033  0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.99950033  0.          0.          0.         -0.998006    0.998006\n",
      "  -0.998006    0.          0.998006    0.998006    0.          0.\n",
      "   0.99977785  0.          0.          0.         -0.998006    0.\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.99950033  0.          0.\n",
      "   0.          0.998006    0.          0.          0.99950033  0.99950033\n",
      "   0.          0.          0.          0.          0.998006    0.\n",
      "   0.         -0.998006    0.         -0.998006    0.          0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.99950033  0.         -0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.          0.         -0.998006   -0.998006    0.          0.\n",
      "   0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.998006    0.998006    0.\n",
      "   0.          0.          0.          0.         -0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.998006    0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.99950033  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.9999199   0.99977803  0.9997778   0.          0.99987507  0.99987507\n",
      "   0.         -0.99977785 -0.9995003   0.          0.99987507 -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.99977785\n",
      "  -0.99977785  0.99800634  0.          0.         -0.99950033  0.99950033\n",
      "   0.         -0.99950033  0.99950033 -0.998006    0.998006   -0.99950033\n",
      "   0.          0.998006   -0.998006    0.          0.99950033  0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "  -0.99950033  0.          0.          0.          0.998006   -0.998006\n",
      "   0.998006    0.         -0.998006   -0.998006    0.          0.\n",
      "  -0.99977785  0.          0.          0.          0.998006    0.\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.99950033  0.          0.\n",
      "   0.         -0.998006    0.          0.         -0.99950033 -0.99950033\n",
      "   0.          0.          0.          0.         -0.998006    0.\n",
      "   0.          0.998006    0.          0.998006    0.         -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.998006    0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.99950033  0.          0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.          0.          0.998006    0.998006    0.          0.\n",
      "  -0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.998006   -0.998006    0.\n",
      "   0.          0.          0.          0.          0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.998006   -0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.99950033  0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "three_processed model with layer normalization: [[15.267825    0.55133015  0.19239126  0.55133015  0.19239126 -0.16654766\n",
      "   0.19239126  0.9102691   1.6281468   0.19239126  0.55133015  0.55133015\n",
      "  -0.16654766  0.19239126 -0.16654766  0.19239126  0.9102691  -0.16654766\n",
      "   0.9102691   0.55133015 -0.16654766 -0.16654766  0.55133015 -0.16654766\n",
      "  -0.16654766  0.55133015 -0.16654766  0.19239126 -0.16654766  0.55133015\n",
      "  -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "   0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766  0.19239126  0.19239126 -0.16654766 -0.16654766\n",
      "   0.9102691  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766  0.55133015 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766  0.55133015  0.55133015\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.55133015\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766  0.19239126  0.19239126 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766]\n",
      " [14.861998    1.8015503   1.4057791   0.61423683  1.8015503   1.4057791\n",
      "   0.21846567 -0.17730547  1.010008    0.21846567  2.1973214  -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547  0.21846567  1.010008    1.010008\n",
      "  -0.17730547  1.010008   -0.17730547 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547  0.61423683 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547  0.61423683 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567  0.21846567  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547  0.21846567  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547  0.21846567  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.61423683 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:16.026205: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import functools\n",
    "\n",
    "class ThreeVectorizer(tf.keras.layers.Layer):\n",
    "    def __init__(self, token_to_int: dict):\n",
    "        super(ThreeVectorizer, self).__init__()\n",
    "        self._token_to_int = token_to_int\n",
    "        self._vocab_size = len(self._token_to_int) + 1\n",
    "    \n",
    "    def call(self, inputs):  \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            tokens = inputs[i]\n",
    "            array_string = tf.TensorArray(dtype=tf.float32, size=self._vocab_size)\n",
    "            array_string.unstack(tf.zeros(self._vocab_size))\n",
    "            for token in tokens:\n",
    "                idx = self._token_to_int.get(token.numpy(), tf.constant(0))\n",
    "                array_string = array_string.write(idx, array_string.read(idx) + 1.0)\n",
    "            outputs = outputs.write(i, array_string.stack())\n",
    "        return outputs.stack()\n",
    "\n",
    "    @classmethod\n",
    "    @functools.lru_cache(maxsize=10)\n",
    "    def from_train_data(cls, train_ds, vocab_size=args.small_vocab_size, batch_size=args.batch_size, take=5):\n",
    "        _one_text_preprocessing = OneTextPreprocessing()\n",
    "        _two_tokenizer = TwoTokenizer()\n",
    "        _preproc_mdl = keras.models.Sequential()\n",
    "        _preproc_mdl.add(_one_text_preprocessing)\n",
    "        _preproc_mdl.add(_two_tokenizer)\n",
    "\n",
    "        _counter = Counter()\n",
    "        for d in train_ds.batch(batch_size).take(take):\n",
    "            _processed = _preproc_mdl(d[0])\n",
    "            for b in _processed:\n",
    "                _counter.update(b.numpy().tolist())\n",
    "\n",
    "        # +-1 cause we need space for unknown tokens with 0 index\n",
    "        _token_dict = {k: tf.cast(i + 1, tf.int32) for i, (k, _) in enumerate(_counter.most_common(vocab_size - 1))}\n",
    "        return cls(_token_dict)\n",
    "\n",
    "three_vectorizer = ThreeVectorizer.from_train_data(train_dataset)\n",
    "\n",
    "three_model = keras.models.Sequential()\n",
    "three_model.add(one_text_preprocessing)\n",
    "three_model.add(two_tokenizer)\n",
    "three_model.add(three_vectorizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    three_processed = three_model(d[0])\n",
    "    print(f\"three_processed model: {three_processed}\")\n",
    "\n",
    "three_model.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    three_processed = three_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"three_processed model with batch normalization: {three_processed}\")\n",
    "\n",
    "three_model.pop()\n",
    "three_model.add(tf.keras.layers.LayerNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    three_processed = three_model(d[0])\n",
    "    print(f\"three_processed model with layer normalization: {three_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed165be-e50d-40c6-b81e-ff5747883ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four_processed model with batch normalization: [[0.21752222]\n",
      " [0.7006044 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:16.113415: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "four_model = keras.models.Sequential()\n",
    "four_model.add(one_text_preprocessing)\n",
    "four_model.add(two_tokenizer)\n",
    "four_model.add(three_vectorizer)\n",
    "four_model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "four_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "four_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    four_processed = four_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"four_processed model with batch normalization: {four_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25bf7c32-d26c-48bf-afc5-003417e9ea31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:13:16.214471: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:13:17,829 : WARNING : At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "2023-12-11 17:13:17,830 : WARNING : There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " one_text_preprocessing (On  multiple                  0         \n",
      " eTextPreprocessing)                                             \n",
      "                                                                 \n",
      " two_tokenizer (TwoTokenize  multiple                  0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " three_vectorizer (ThreeVec  multiple                  0         \n",
      " torizer)                                                        \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  multiple                  1000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  16064     \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17129 (66.91 KB)\n",
      "Trainable params: 16629 (64.96 KB)\n",
      "Non-trainable params: 500 (1.95 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      " 15/782 [..............................] - ETA: 21:38 - loss: 0.7966 - accuracy: 0.5063"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39mmonitor, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m reduce_lr_on_plateau \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39mmonitor, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfive_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr_on_plateau\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal_accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal_loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1338\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_function\u001b[39m(iterator):\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1322\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1319\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1322\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1324\u001b[0m     outputs,\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1326\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1327\u001b[0m )\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1669\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1672\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3250\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3250\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4048\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4046\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   4047\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 4048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1303\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1080\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;66;03m# Run forward pass.\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m-> 1080\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:569\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    567\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1150\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1149\u001b[0m ):\n\u001b[0;32m-> 1150\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/sequential.py:420\u001b[0m, in \u001b[0;36mSequential.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m argspec:\n\u001b[1;32m    418\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[0;32m--> 420\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m inputs \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_mask_from_keras_tensor\u001b[39m(kt):\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py:1150\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1149\u001b[0m ):\n\u001b[0;32m-> 1150\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mTwoTokenizer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m tokens \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iw \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;28mlen\u001b[39m(tokens)):\n\u001b[0;32m---> 20\u001b[0m     tokens_flat_tensor_array \u001b[38;5;241m=\u001b[39m tokens_flat_tensor_array\u001b[38;5;241m.\u001b[39mwrite(\u001b[43mstart_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43miw\u001b[49m, tokens[iw])\n\u001b[1;32m     21\u001b[0m str_len_tensor_array \u001b[38;5;241m=\u001b[39m str_len_tensor_array\u001b[38;5;241m.\u001b[39mwrite(i, \u001b[38;5;28mlen\u001b[39m(tokens))\n\u001b[1;32m     22\u001b[0m start_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1500\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.r_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mr_binary_op_wrapper\u001b[39m(y, x):\n\u001b[1;32m   1497\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28;01mNone\u001b[39;00m, op_name, [x, y]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m-> 1500\u001b[0m     y, x \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_promote_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_same_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:1433\u001b[0m, in \u001b[0;36mmaybe_promote_tensors\u001b[0;34m(force_same_dtype, *tensors)\u001b[0m\n\u001b[1;32m   1430\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\n\u001b[1;32m   1431\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m   1432\u001b[0m     promoted_tensors\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m-> 1433\u001b[0m         \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1434\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m promoted_tensors\n\u001b[1;32m   1435\u001b[0m result_type \u001b[38;5;241m=\u001b[39m np_dtypes\u001b[38;5;241m.\u001b[39m_result_type(\n\u001b[1;32m   1436\u001b[0m     \u001b[38;5;241m*\u001b[39m[_maybe_get_dtype(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(tensors)])\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1443\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/ml-experiments/.venv/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:245\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, accepted_result_types):\n\u001b[1;32m    240\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    241\u001b[0m       _add_error_prefix(\n\u001b[1;32m    242\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m returned non-Tensor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    244\u001b[0m           name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m--> 245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    246\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m       _add_error_prefix(\n\u001b[1;32m    248\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturned incompatible dtype: requested = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    251\u001b[0m           name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "five_model = keras.models.Sequential()\n",
    "five_model.add(one_text_preprocessing)\n",
    "five_model.add(two_tokenizer)\n",
    "five_model.add(three_vectorizer)\n",
    "five_model.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
    "five_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "five_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# instead of model.build() we can just call the model on some data\n",
    "for d in train_dataset.batch(args.batch_size).take(1):\n",
    "    five_model(d[0])\n",
    "five_model.compile(optimizer=keras.optimizers.Nadam(learning_rate=1e-3), loss='binary_crossentropy', \n",
    "                   metrics=['accuracy'], run_eagerly=True)\n",
    "five_model.summary()\n",
    "\n",
    "ds_train = train_dataset.shuffle(args.batch_size * 10).batch(args.batch_size).prefetch(1)\n",
    "ds_val = val_dataset.batch(args.batch_size).prefetch(1)\n",
    "monitor='val_loss'\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=monitor, patience=3, mode='auto', restore_best_weights=True, verbose=1)\n",
    "reduce_lr_on_plateau = keras.callbacks.ReduceLROnPlateau(monitor=monitor, factor=0.1, patience=2, min_delta=1e-4, mode='auto', verbose=1)\n",
    "\n",
    "history = five_model.fit(ds_train, validation_data=ds_val, epochs=args.epochs, callbacks=[early_stopping, reduce_lr_on_plateau])\n",
    "\n",
    "print('Val_accuracy:', max(history.history['val_accuracy']))\n",
    "print('Val_loss:', min(history.history['val_loss']))\n",
    "print('Accuracy:', max(history.history['accuracy']))\n",
    "\n",
    "# 782/782 [==============================] - 2417s 3s/step - loss: 0.4725 - accuracy: 0.7740 - val_loss: 0.4740 - val_accuracy: 0.7734 - lr: 0.0010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa8cba-84fd-4986-9da9-9d5782a8767a",
   "metadata": {},
   "source": [
    "### Problems occurred with implementing it\n",
    "- Using TensorArray and other special TF types to support different sizes of tensors\n",
    "- When trying to train (model.fit()):\n",
    "  AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "    \n",
    "    \n",
    "    Call arguments received by layer 'one_text_preprocessing' (type OneTextPreprocessing):\n",
    "       inputs=tf.Tensor(shape=(None,), dtype=string)\n",
    "    on calling numpy() on a Tensor when model.fit().  During just __call__() on the layers and the model, this didn't happen\n",
    "\n",
    "  Solution from https://stackoverflow.com/questions/52357542/attributeerror-tensor-object-has-no-attribute-numpy: This can also happen in TF2.0 if your code is wrapped in a @tf.function or inside a Keras layer (my case). Both of those run in graph mode. There's a lot of secretly broken code out of there because behavior differs between eager and graph modes and people are not aware that they're switching contexts, so be careful!\n",
    "  The issue seems to be that for certain functions during the fitting model.fit() the @tf.function decorator prohibits the execution of functions like tensor.numpy() for performance reasons.\n",
    "  \n",
    "    The solution for me was to pass the flag run_eagerly=True to the model.compile() like this model.compile(..., run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd1434e2-948e-4aef-b5d4-c41ed30653fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.998006  , -0.99800634],\n",
       "       [ 0.9980061 ,  0.99800587]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl = tf.keras.layers.BatchNormalization(axis=1)\n",
    "\n",
    "tl(tf.constant([[1, 3], [2, 4]]), training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe01b8-622f-46ea-b4f6-98ce1eaaa2ba",
   "metadata": {},
   "source": [
    "## 2. Bag of words from scratch in graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd430113-0666-4822-b02a-38d8e37cffa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dadb695-edae-4a00-a495-af5dff22291b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "six_processed layer: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all'];\n",
      "six_processed model: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all'];\n",
      "six_processed model graph inside tf function: Tensor(\"sequential_6/six_text_preprocessing/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string);\n",
      "six_processed model graph outside of tf function: [b'this was an absolutely terrible movie dont be lured in by christopher walken or michael ironside both are great actors but this must simply be their worst role in history even their great acting could not redeem this movies ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the columbian rebels were making their cases for revolutions maria conchita alonso appeared phony and her pseudolove affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actors like christopher walkens good name i could barely sit through it'\n",
      " b'i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the sette and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else i cant recommend this film at all'];\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:14:05.828507: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:14:05.855769: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:14:05.921972: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class SixTextPreprocessing(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SixTextPreprocessing, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        # print(f\"Type inputs: {type(inputs)}\")\n",
    "        # print(f\"Inputs shape: {inputs.shape}\")\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            lower_string = tf.strings.lower(string_tensor)\n",
    "            # print(lower_string)\n",
    "            processed = tf.strings.regex_replace(lower_string, \"[^\\w\\s]\", \"\", replace_global=True, name=None)\n",
    "            # print(processed)\n",
    "            outputs = outputs.write(i, processed)\n",
    "        stacked_output = outputs.stack()\n",
    "        # print(f\"type(stacked_output): {type(stacked_output)}\")\n",
    "        # print(f\"tf.shape(stacked_output): {tf.shape(stacked_output)}\")\n",
    "        # outputs shape: B, Ragged texts\n",
    "        return stacked_output\n",
    "\n",
    "six_text_preprocessing = SixTextPreprocessing()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    six_processed = six_text_preprocessing(d[0])\n",
    "    print(f\"six_processed layer: {six_processed};\")\n",
    "\n",
    "six_model = keras.models.Sequential()\n",
    "six_model.add(six_text_preprocessing)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    six_processed = six_model(d[0])\n",
    "    print(f\"six_processed model: {six_processed};\")\n",
    "\n",
    "@tf.function\n",
    "def run_six_graph(inp):\n",
    "    six_processed_graph = six_model(inp)\n",
    "    tf.print(f\"six_processed model graph inside tf function: {six_processed_graph};\")\n",
    "    return six_processed_graph\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    six_processed_graph = run_six_graph(d[0])\n",
    "    print(f\"six_processed model graph outside of tf function: {six_processed_graph};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0584615d-2d59-48c5-bd47-fec7d34a3fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:14:08.952405: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:14:09.037886: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven_processed layer: <tf.RaggedTensor [[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie.', b\"Don't\",\n",
      "  b'be', b'lured', b'in', b'by', b'Christopher', b'Walken', b'or',\n",
      "  b'Michael', b'Ironside.', b'Both', b'are', b'great', b'actors,', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history.', b'Even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b\"movie's\", b'ridiculous', b'storyline.', b'This',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'US', b'propaganda',\n",
      "  b'piece.', b'The', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'Columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions.', b'Maria', b'Conchita', b'Alonso',\n",
      "  b'appeared', b'phony,', b'and', b'her', b'pseudo-love', b'affair',\n",
      "  b'with', b'Walken', b'was', b'nothing', b'but', b'a', b'pathetic',\n",
      "  b'emotional', b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid',\n",
      "  b'of', b'any', b'real', b'meaning.', b'I', b'am', b'disappointed',\n",
      "  b'that', b'there', b'are', b'movies', b'like', b'this,', b'ruining',\n",
      "  b\"actor's\", b'like', b'Christopher', b\"Walken's\", b'good', b'name.', b'I',\n",
      "  b'could', b'barely', b'sit', b'through', b'it.']                          ,\n",
      " [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films,', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including,', b'really', b'tired,',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot.', b'However', b'on',\n",
      "  b'this', b'occasion', b'I', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish.', b'The', b'plot', b'development', b'was',\n",
      "  b'constant.', b'Constantly', b'slow', b'and', b'boring.', b'Things',\n",
      "  b'seemed', b'to', b'happen,', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why.', b'I',\n",
      "  b'admit,', b'I', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film,', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else.', b'I', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all.']                                               ]>;\n",
      "seven_processed model: <tf.RaggedTensor [[b'this', b'was', b'an', b'absolutely', b'terrible', b'movie', b'dont',\n",
      "  b'be', b'lured', b'in', b'by', b'christopher', b'walken', b'or',\n",
      "  b'michael', b'ironside', b'both', b'are', b'great', b'actors', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history', b'even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b'movies', b'ridiculous', b'storyline', b'this',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'us', b'propaganda',\n",
      "  b'piece', b'the', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions', b'maria', b'conchita', b'alonso',\n",
      "  b'appeared', b'phony', b'and', b'her', b'pseudolove', b'affair', b'with',\n",
      "  b'walken', b'was', b'nothing', b'but', b'a', b'pathetic', b'emotional',\n",
      "  b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid', b'of', b'any',\n",
      "  b'real', b'meaning', b'i', b'am', b'disappointed', b'that', b'there',\n",
      "  b'are', b'movies', b'like', b'this', b'ruining', b'actors', b'like',\n",
      "  b'christopher', b'walkens', b'good', b'name', b'i', b'could', b'barely',\n",
      "  b'sit', b'through', b'it']                                                ,\n",
      " [b'i', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including', b'really', b'tired',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot', b'however', b'on',\n",
      "  b'this', b'occasion', b'i', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish', b'the', b'plot', b'development', b'was',\n",
      "  b'constant', b'constantly', b'slow', b'and', b'boring', b'things',\n",
      "  b'seemed', b'to', b'happen', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why', b'i',\n",
      "  b'admit', b'i', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else', b'i', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all']                                                ]>;\n",
      "seven_processed model graph inside tf function: tf.RaggedTensor(values=Tensor(\"sequential_7/seven_tokenizer/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string), row_splits=Tensor(\"sequential_7/seven_tokenizer/RaggedFromRowLengths/control_dependency:0\", shape=(None,), dtype=int32));\n",
      "seven_processed model graph outside of tf function: <tf.RaggedTensor [[b'this', b'was', b'an', b'absolutely', b'terrible', b'movie', b'dont',\n",
      "  b'be', b'lured', b'in', b'by', b'christopher', b'walken', b'or',\n",
      "  b'michael', b'ironside', b'both', b'are', b'great', b'actors', b'but',\n",
      "  b'this', b'must', b'simply', b'be', b'their', b'worst', b'role', b'in',\n",
      "  b'history', b'even', b'their', b'great', b'acting', b'could', b'not',\n",
      "  b'redeem', b'this', b'movies', b'ridiculous', b'storyline', b'this',\n",
      "  b'movie', b'is', b'an', b'early', b'nineties', b'us', b'propaganda',\n",
      "  b'piece', b'the', b'most', b'pathetic', b'scenes', b'were', b'those',\n",
      "  b'when', b'the', b'columbian', b'rebels', b'were', b'making', b'their',\n",
      "  b'cases', b'for', b'revolutions', b'maria', b'conchita', b'alonso',\n",
      "  b'appeared', b'phony', b'and', b'her', b'pseudolove', b'affair', b'with',\n",
      "  b'walken', b'was', b'nothing', b'but', b'a', b'pathetic', b'emotional',\n",
      "  b'plug', b'in', b'a', b'movie', b'that', b'was', b'devoid', b'of', b'any',\n",
      "  b'real', b'meaning', b'i', b'am', b'disappointed', b'that', b'there',\n",
      "  b'are', b'movies', b'like', b'this', b'ruining', b'actors', b'like',\n",
      "  b'christopher', b'walkens', b'good', b'name', b'i', b'could', b'barely',\n",
      "  b'sit', b'through', b'it']                                                ,\n",
      " [b'i', b'have', b'been', b'known', b'to', b'fall', b'asleep', b'during',\n",
      "  b'films', b'but', b'this', b'is', b'usually', b'due', b'to', b'a',\n",
      "  b'combination', b'of', b'things', b'including', b'really', b'tired',\n",
      "  b'being', b'warm', b'and', b'comfortable', b'on', b'the', b'sette',\n",
      "  b'and', b'having', b'just', b'eaten', b'a', b'lot', b'however', b'on',\n",
      "  b'this', b'occasion', b'i', b'fell', b'asleep', b'because', b'the',\n",
      "  b'film', b'was', b'rubbish', b'the', b'plot', b'development', b'was',\n",
      "  b'constant', b'constantly', b'slow', b'and', b'boring', b'things',\n",
      "  b'seemed', b'to', b'happen', b'but', b'with', b'no', b'explanation',\n",
      "  b'of', b'what', b'was', b'causing', b'them', b'or', b'why', b'i',\n",
      "  b'admit', b'i', b'may', b'have', b'missed', b'part', b'of', b'the',\n",
      "  b'film', b'but', b'i', b'watched', b'the', b'majority', b'of', b'it',\n",
      "  b'and', b'everything', b'just', b'seemed', b'to', b'happen', b'of',\n",
      "  b'its', b'own', b'accord', b'without', b'any', b'real', b'concern',\n",
      "  b'for', b'anything', b'else', b'i', b'cant', b'recommend', b'this',\n",
      "  b'film', b'at', b'all']                                                ]>;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:14:09.287497: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "class SevenTokenizer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(SevenTokenizer, self).__init__()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # inputs shape: B, Ragged texts\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        tokens_flat_tensor_array = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\n",
    "        str_len_tensor_array = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
    "        start_len = 0\n",
    "        for i in tf.range(batch_size):\n",
    "            string_tensor = inputs[i]\n",
    "            tokens_ragged_tensor = tf.strings.split(string_tensor, \" \")\n",
    "            for iw in tf.range(tf.size(tokens_ragged_tensor)):\n",
    "                tokens_flat_tensor_array = tokens_flat_tensor_array.write(start_len + iw, tokens_ragged_tensor[iw])\n",
    "            str_len_tensor_array = str_len_tensor_array.write(i, tf.size(tokens_ragged_tensor))\n",
    "            start_len += tf.size(tokens_ragged_tensor)\n",
    "        \n",
    "        ragged_tensor = tf.RaggedTensor.from_row_lengths(\n",
    "                values=tokens_flat_tensor_array.stack(),\n",
    "                row_lengths=str_len_tensor_array.stack())\n",
    "        # print(f\"type(ragged_tensor): {type(ragged_tensor)}\")\n",
    "        # print(f\"tf.shape(ragged_tensor): {tf.shape(ragged_tensor)}\")\n",
    "        # outputs shape: Ragged tensor: B, Tokens\n",
    "        return ragged_tensor\n",
    "        \n",
    "seven_tokenizer = SevenTokenizer()\n",
    "\n",
    "# for d in train_dataset.batch(args.batch_size).take(1):\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    seven_processed = seven_tokenizer(d[0])\n",
    "    print(f\"seven_processed layer: {seven_processed};\")\n",
    "\n",
    "seven_model = keras.models.Sequential()\n",
    "seven_model.add(six_text_preprocessing)\n",
    "seven_model.add(seven_tokenizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    seven_processed = seven_model(d[0])\n",
    "    print(f\"seven_processed model: {seven_processed};\")\n",
    "\n",
    "@tf.function\n",
    "def run_seven_graph(inp):\n",
    "    seven_processed_graph = seven_model(inp)\n",
    "    tf.print(f\"seven_processed model graph inside tf function: {seven_processed_graph};\")\n",
    "    return seven_processed_graph\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    seven_processed_graph = run_seven_graph(d[0])\n",
    "    print(f\"seven_processed model graph outside of tf function: {seven_processed_graph};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "897fa267-ccaa-43fc-9a99-27aada4adf75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:14:30.154661: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(250, shape=(), dtype=int32)\n",
      "eight_processed model: [[43.  2.  1.  2.  1.  0.  1.  3.  5.  1.  2.  2.  0.  1.  0.  1.  3.  0.\n",
      "   3.  2.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  2.  0.  0.  1.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  3.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "   0.  0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38.  5.  4.  2.  5.  4.  1.  0.  3.  1.  6.  0.  0.  1.  0.  1.  3.  3.\n",
      "   0.  3.  0.  0.  0.  2.  0.  0.  2.  0.  0.  1.  0.  0.  1.  0.  0.  2.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]];\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:14:31.670292: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:14:31.870528: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight_processed model with batch normalization: [[ 0.9999199  -0.9997778  -0.9997779   0.         -0.99987507 -0.99987507\n",
      "   0.          0.99977785  0.9995005   0.         -0.99987507  0.99950033\n",
      "   0.          0.          0.          0.          0.         -0.99977785\n",
      "   0.99977785 -0.99800587  0.          0.          0.99950033 -0.99950033\n",
      "   0.          0.99950033 -0.99950033  0.998006    0.         -0.998006\n",
      "   0.99950033  0.         -0.998006    0.998006    0.         -0.99950033\n",
      "   0.          0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.99950033  0.          0.          0.         -0.998006\n",
      "   0.998006   -0.998006    0.          0.998006    0.998006    0.\n",
      "   0.          0.99977785  0.          0.          0.         -0.998006\n",
      "   0.          0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.99950033  0.\n",
      "   0.          0.          0.998006    0.          0.          0.99950033\n",
      "   0.99950033  0.          0.          0.          0.          0.998006\n",
      "   0.          0.         -0.998006    0.         -0.998006    0.\n",
      "   0.99950033  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.998006   -0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.99950033  0.         -0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.          0.         -0.998006   -0.998006    0.\n",
      "   0.          0.998006    0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.998006    0.998006\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006   -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.9999199   0.99977803  0.9997778   0.          0.99987507  0.99987507\n",
      "   0.         -0.99977785 -0.9995003   0.          0.99987507 -0.99950033\n",
      "   0.          0.          0.          0.          0.          0.99977785\n",
      "  -0.99977785  0.99800634  0.          0.         -0.99950033  0.99950033\n",
      "   0.         -0.99950033  0.99950033 -0.998006    0.          0.998006\n",
      "  -0.99950033  0.          0.998006   -0.998006    0.          0.99950033\n",
      "   0.         -0.998006    0.998006    0.          0.          0.\n",
      "   0.         -0.99950033  0.          0.          0.          0.998006\n",
      "  -0.998006    0.998006    0.         -0.998006   -0.998006    0.\n",
      "   0.         -0.99977785  0.          0.          0.          0.998006\n",
      "   0.         -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.99950033  0.\n",
      "   0.          0.         -0.998006    0.          0.         -0.99950033\n",
      "  -0.99950033  0.          0.          0.          0.         -0.998006\n",
      "   0.          0.          0.998006    0.          0.998006    0.\n",
      "  -0.99950033  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.998006    0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.99950033  0.          0.998006    0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006    0.          0.          0.998006    0.998006    0.\n",
      "   0.         -0.998006   -0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.         -0.998006   -0.998006\n",
      "   0.          0.          0.          0.          0.          0.998006\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.         -0.998006\n",
      "  -0.998006    0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.99950033\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.998006    0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.998006    0.998006    0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "  -0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.998006    0.998006    0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]];\n",
      "eight_processed model with layer normalization: [[15.267825    0.55133015  0.19239126  0.55133015  0.19239126 -0.16654766\n",
      "   0.19239126  0.9102691   1.6281468   0.19239126  0.55133015  0.55133015\n",
      "  -0.16654766  0.19239126 -0.16654766  0.19239126  0.9102691  -0.16654766\n",
      "   0.9102691   0.55133015 -0.16654766 -0.16654766  0.55133015 -0.16654766\n",
      "  -0.16654766  0.55133015 -0.16654766  0.19239126 -0.16654766 -0.16654766\n",
      "   0.55133015 -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126  0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766  0.19239126  0.19239126 -0.16654766\n",
      "  -0.16654766  0.9102691  -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.55133015 -0.16654766\n",
      "  -0.16654766 -0.16654766  0.19239126 -0.16654766 -0.16654766  0.55133015\n",
      "   0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.55133015 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766  0.55133015 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126  0.19239126 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126  0.19239126\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766  0.19239126\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766  0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "   0.19239126 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766 -0.16654766\n",
      "  -0.16654766 -0.16654766 -0.16654766 -0.16654766]\n",
      " [14.861998    1.8015503   1.4057791   0.61423683  1.8015503   1.4057791\n",
      "   0.21846567 -0.17730547  1.010008    0.21846567  2.1973214  -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547  0.21846567  1.010008    1.010008\n",
      "  -0.17730547  1.010008   -0.17730547 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547  0.61423683 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567  0.21846567  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567 -0.17730547 -0.17730547  0.21846567  0.21846567 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.21846567  0.21846567\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547  0.61423683\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "   0.21846567  0.21846567 -0.17730547 -0.17730547 -0.17730547 -0.17730547\n",
      "  -0.17730547 -0.17730547 -0.17730547 -0.17730547]];\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 17:14:31.969658: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-12-11 17:14:32.059611: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight_processed model graph inside tf function: Tensor(\"sequential_9/eight_vectorizer/TensorArrayV2Stack/TensorListStack:0\", shape=(None, 250), dtype=float32);\n",
      "eight_processed model graph outside of tf function: [[43.  2.  1.  2.  1.  0.  1.  3.  5.  1.  2.  2.  0.  1.  0.  1.  3.  0.\n",
      "   3.  2.  0.  0.  2.  0.  0.  2.  0.  1.  0.  0.  2.  0.  0.  1.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  3.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "   0.  0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [38.  5.  4.  2.  5.  4.  1.  0.  3.  1.  6.  0.  0.  1.  0.  1.  3.  3.\n",
      "   0.  3.  0.  0.  0.  2.  0.  0.  2.  0.  0.  1.  0.  0.  1.  0.  0.  2.\n",
      "   0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.]];\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import functools\n",
    "from tensorflow.lookup import KeyValueTensorInitializer, StaticHashTable\n",
    "\n",
    "class EightVectorizer(tf.keras.layers.Layer):\n",
    "    # +-1 cause we need space for unknown tokens with 0 index\n",
    "    def __init__(self, token_to_int: StaticHashTable):\n",
    "        super(EightVectorizer, self).__init__()\n",
    "        self._token_to_int = token_to_int\n",
    "        self._vocab_size = tf.cast(self._token_to_int.size() + 1, tf.int32)\n",
    "    \n",
    "    def call(self, inputs):  \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        for i in tf.range(batch_size):\n",
    "            tokens = inputs[i]\n",
    "            array_string = tf.TensorArray(dtype=tf.float32, size=self._vocab_size)\n",
    "            # array_string = tf.TensorArray(dtype=tf.float32, size=250)\n",
    "            array_string.unstack(tf.zeros(self._vocab_size))\n",
    "            # array_string.unstack(tf.zeros(250))\n",
    "            # for itoken in tf.range(tf.size(\n",
    "            indexes = self._token_to_int.lookup(tokens)\n",
    "            for idx in indexes:\n",
    "                array_string = array_string.write(idx, array_string.read(idx) + 1.0)\n",
    "            outputs = outputs.write(i, array_string.stack())\n",
    "        return outputs.stack()\n",
    "\n",
    "    @classmethod\n",
    "    @functools.lru_cache(maxsize=10)\n",
    "    def from_train_data(cls, train_ds, vocab_size=args.small_vocab_size, batch_size=args.batch_size, take=5):\n",
    "        _six_text_preprocessing = SixTextPreprocessing()\n",
    "        _seven_tokenizer = SevenTokenizer()\n",
    "        _preproc_mdl = keras.models.Sequential()\n",
    "        _preproc_mdl.add(_six_text_preprocessing)\n",
    "        _preproc_mdl.add(_seven_tokenizer)\n",
    "\n",
    "        _counter = Counter()\n",
    "        for d in train_ds.batch(batch_size).take(take):\n",
    "            _processed = _preproc_mdl(d[0])\n",
    "            for ragged_tensor in _processed:\n",
    "                _counter.update(ragged_tensor.numpy().tolist())\n",
    "        keys_tensor = tf.constant([t for t, _ in _counter.most_common(vocab_size - 1)])\n",
    "        # print(f\"keys_tensor: {keys_tensor}\")\n",
    "        vals_tensor = tf.constant([i for i in range(1, vocab_size)])\n",
    "        # print(f\"vals_tensor: {vals_tensor}\")\n",
    "        _token_table = StaticHashTable(KeyValueTensorInitializer(keys_tensor, vals_tensor), default_value=0)\n",
    "        return cls(_token_table)\n",
    "\n",
    "eight_vectorizer = EightVectorizer.from_train_data(train_dataset)\n",
    "print(eight_vectorizer._vocab_size)\n",
    "\n",
    "eight_model = keras.models.Sequential()\n",
    "eight_model.add(six_text_preprocessing)\n",
    "eight_model.add(seven_tokenizer)\n",
    "eight_model.add(eight_vectorizer)\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed = eight_model(d[0])\n",
    "    print(f\"eight_processed model: {eight_processed};\")\n",
    "\n",
    "eight_model.add(tf.keras.layers.BatchNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed = eight_model(d[0], training=True) # batch normalization works while training only\n",
    "    print(f\"eight_processed model with batch normalization: {eight_processed};\")\n",
    "\n",
    "eight_model.pop()\n",
    "eight_model.add(tf.keras.layers.LayerNormalization(axis=1))\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed = eight_model(d[0])\n",
    "    print(f\"eight_processed model with layer normalization: {eight_processed};\")\n",
    "\n",
    "eight_model.pop()\n",
    "@tf.function\n",
    "def run_eight_graph(inp):\n",
    "    eight_processed_graph = eight_model(inp)\n",
    "    tf.print(f\"eight_processed model graph inside tf function: {eight_processed_graph};\")\n",
    "    return eight_processed_graph\n",
    "\n",
    "for d in train_dataset.batch(2).take(1):\n",
    "    eight_processed_graph = run_eight_graph(d[0])\n",
    "    print(f\"eight_processed model graph outside of tf function: {eight_processed_graph};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b1d4d2-07e2-40bc-bd5b-b628c47401ed",
   "metadata": {},
   "source": [
    "## Problems faced when implementing the  in graph mode\n",
    "1. Can use only tf methods and functions, no numpy or python func. If you lack some function in tf, it might be difficult to implement something.\n",
    "2. To run it graph mode, you need to wrap a model in a tf fucntion and run a tf.function. Can't just enable graph mode to be usef everywhere.\n",
    "3. Couldn't make it print a tensor value inside tf.function neither with print() nor with tf.print() it returns something like Tensor(\"sequential_4/six_text_preprocessing_4/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string) instead of a real value\n",
    "4. AttributeError: 'Tensor' object has no attribute 'numpy'\n",
    "5. When self._vocab_size = self._token_to_int.size() + 1 (Tensor int64) got an error AssertionError: Unreachable\n",
    "    \n",
    "    \n",
    "    Call arguments received by layer 'eight_vectorizer_16' (type EightVectorizer):\n",
    "       inputs=tf.RaggedTensor(values=Tensor(\"sequential_39/seven_tokenizer_2/TensorArrayV2Stack/TensorListStack:0\", shape=(None,), dtype=string), row_splits=Tensor(\"sequential_39/seven_tokenizer_2/RaggedFromRowLengths/control_dependency:0\", shape=(None,), dtype=int32))\n",
    "   needed to cast:\n",
    "   self._vocab_size = tf.cast(self._token_to_int.size() + 1, tf.int32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
