{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e3f8156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "log = logging.getLogger()\n",
    "\n",
    "%config Completer.use_jedi = False # make autocompletion works in jupyter\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "378c7e00-c3f7-4ece-8849-cc08354be6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-19 14:24:24,368 : INFO : Load dataset info from /Users/mkhokhlush/tensorflow_datasets/fashion_mnist/3.0.1\n",
      "2021-04-19 14:24:24,370 : INFO : Reusing dataset fashion_mnist (/Users/mkhokhlush/tensorflow_datasets/fashion_mnist/3.0.1)\n",
      "2021-04-19 14:24:24,371 : INFO : Constructing tf.data.Dataset for split None, from /Users/mkhokhlush/tensorflow_datasets/fashion_mnist/3.0.1\n",
      "2021-04-19 14:24:24,423 : INFO : Loaded dataset: {'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>, 'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}\n",
      "2021-04-19 14:24:24,425 : INFO : Train dataset len: 48000\n",
      "2021-04-19 14:24:24,426 : INFO : Val dataset len: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1500/1500 [==============================] - 9s 3ms/step - loss: 16.5441 - sparse_categorical_accuracy: 0.7081 - val_loss: 0.8003 - val_sparse_categorical_accuracy: 0.7648\n",
      "Epoch 2/3\n",
      "1500/1500 [==============================] - 8s 3ms/step - loss: 0.6367 - sparse_categorical_accuracy: 0.7900 - val_loss: 0.5747 - val_sparse_categorical_accuracy: 0.8123\n",
      "Epoch 3/3\n",
      "1500/1500 [==============================] - 8s 3ms/step - loss: 0.5457 - sparse_categorical_accuracy: 0.8130 - val_loss: 0.5703 - val_sparse_categorical_accuracy: 0.8223\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.train import Feature, Example, Features, BytesList, Int64List\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "\n",
    "def ch13_ex_9():\n",
    "    train_fraction = 0.8\n",
    "    seed = 101\n",
    "    train_dir = 'data-ignored/tf/train'\n",
    "    val_dir = 'data-ignored/tf/val'\n",
    "    batch_size = 32\n",
    "    epochs = 3\n",
    "\n",
    "    def serialize_dataset(dataset, data_dir):\n",
    "        filepaths = []\n",
    "        Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "        for i, d in enumerate(dataset):\n",
    "            ser_image = tf.io.serialize_tensor(d['image']).numpy()\n",
    "            image_example = Example(\n",
    "                features=Features(\n",
    "                    feature={\n",
    "                        'image': Feature(bytes_list=BytesList(value=[ser_image])),\n",
    "                        'label': Feature(int64_list=Int64List(value=[d['label']]))\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "            filepath = f'{data_dir}/image{i}.tfrecord'\n",
    "            with tf.io.TFRecordWriter(filepath) as f:\n",
    "                f.write(image_example.SerializeToString())\n",
    "            filepaths.append(filepath)\n",
    "        return filepaths\n",
    "                \n",
    "    def create_dataset(filepaths):\n",
    "        image_descr = {\n",
    "            'image': tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "            'label': tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "        }\n",
    "        def parse_image_example(ser):\n",
    "            parsed_image_example = tf.io.parse_single_example(ser, image_descr)\n",
    "            image = tf.io.parse_tensor(parsed_image_example[\"image\"], out_type=tf.uint8)\n",
    "            label = parsed_image_example[\"label\"]\n",
    "            return image, label\n",
    "       \n",
    "        dataset = tf.data.TFRecordDataset(filepaths).map(parse_image_example).shuffle(buffer_size=len(filepaths), seed=seed).batch(batch_size).prefetch(1)\n",
    "        return dataset\n",
    "    \n",
    "    def create_model():\n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.InputLayer(input_shape=(28, 28)))\n",
    "        model.add(keras.layers.Flatten())\n",
    "        model.add(keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'))\n",
    "        model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                     optimizer=keras.optimizers.Nadam(learning_rate=0.001),\n",
    "                     metrics=[keras.metrics.sparse_categorical_accuracy])\n",
    "        return model\n",
    "    \n",
    "    dataset = tfds.load(\"fashion_mnist\")\n",
    "    log.info(f'Loaded dataset: {dataset}')\n",
    "    train_idx = int(len(dataset['train']) * train_fraction)\n",
    "    train = dataset['train'].take(train_idx).shuffle(101, seed=seed, reshuffle_each_iteration=True)\n",
    "    log.info(f'Train dataset len: {len(train)}')\n",
    "    val = dataset['train'].skip(train_idx)\n",
    "    log.info(f'Val dataset len: {len(val)}')\n",
    "    \n",
    "    train_filepaths = serialize_dataset(dataset=train, data_dir=train_dir)\n",
    "    val_filepaths = serialize_dataset(dataset=val, data_dir=val_dir)\n",
    "    \n",
    "    train_dataset = create_dataset(train_filepaths)\n",
    "    val_dataset = create_dataset(val_filepaths)\n",
    "    \n",
    "    model = create_model()\n",
    "    model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)\n",
    "\n",
    "             \n",
    "ch13_ex_9()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
